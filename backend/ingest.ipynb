{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.20.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.9/site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.9/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.9/site-packages (from openai) (2.6.4)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.9/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./venv/lib/python3.9/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Downloading openai-1.20.0-py3-none-any.whl (292 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai\n",
      "Successfully installed openai-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install pinecone-client\n",
    "# %pip install python-dotenv\n",
    "# %pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI, AzureOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ogb/projects/pdfai/backend/venv/lib/python3.9/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "# index = pc.Index(\"test-pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0038017325568944216, -0.010467467829585075, -0.003118067281320691, 0.045824043452739716, 0.01688837632536888, -0.010218022391200066, -0.033647410571575165, 0.04719137027859688, -0.013303753919899464, 0.015003678388893604, 0.018274184316396713, 0.024870628491044044, -0.02950846590101719, -0.009271053597331047, -0.01054137758910656, 5.4530089983018115e-05, -0.021396871656179428, -0.004078893922269344, -0.02819656766951084, -0.024926060810685158, 0.03139316290616989, 0.020546909421682358, -0.0726163238286972, 0.03813742846250534, -0.004531591199338436, 0.016112323850393295, -0.036345116794109344, 0.012001094408333302, 0.011437533423304558, -0.012102720327675343, 0.03597556799650192, 0.022043582051992416, 0.033462636172771454, 0.004459991119801998, -0.002283118199557066, -0.01551180798560381, 0.018846984952688217, 0.038913480937480927, 0.0026284153573215008, 0.02010345086455345, 0.004092752002179623, 0.006674973759800196, -0.04774569347500801, 0.002265795599669218, 0.00916018895804882, 0.0032220028806477785, 0.022172924131155014, -0.018671449273824692, -0.021544691175222397, -0.03285288065671921, 0.026533598080277443, 0.017479654401540756, -0.0296932402998209, 0.0008505394798703492, -0.005677192471921444, 0.005455463193356991, -0.0013165173586457968, 0.016361769288778305, -0.0009371524793095887, 0.020011063665151596, -0.03531038016080856, 0.005751102231442928, 0.01823722943663597, 0.015946026891469955, -0.04504799097776413, -0.024057621136307716, 0.006356237921863794, 0.03189205378293991, -0.004277526400983334, 0.05258678272366524, 0.012490746565163136, -0.03638207167387009, -0.004277526400983334, 0.03250180929899216, 0.0073309228755533695, 0.005820392165333033, 0.019179578870534897, 0.02106427773833275, -0.022135969251394272, -0.008019207045435905, 0.031448595225811005, -0.005035101436078548, -0.006938277278095484, 0.013460812158882618, 0.027069443836808205, 0.02174794301390648, -0.020528431981801987, -0.10066507011651993, -0.012888011522591114, 0.027217263355851173, -0.0166851244866848, 0.010014770552515984, 0.03152250498533249, -0.011585352011024952, -0.004298313520848751, 0.008481143042445183, 0.03301917761564255, -0.017673667520284653, -0.026718372479081154, 0.007113812956959009, 0.010947881266474724, 0.008240936324000359, -0.003600790398195386, -0.0411677286028862, 0.00985771231353283, 0.014911291189491749, 0.021489258855581284, -0.008568910881876945, 0.005935876164585352, -0.02025127038359642, -0.01600145921111107, -0.024334782734513283, 0.017174776643514633, 0.01341461855918169, -0.013590154238045216, -0.011585352011024952, 0.005524753592908382, -0.03285288065671921, 0.002141072880476713, -0.004524662159383297, -0.0010982526000589132, -0.030432337895035744, -0.011723932810127735, -0.0021549309603869915, -0.009977815672755241, 0.0015370916808024049, -0.039763443171978, -0.010578332468867302, 0.0011334752198308706, -0.05125640705227852, -0.04342197626829147, 0.013719496317207813, -0.014366206713020802, 0.04778264835476875, 0.050184715539216995, 0.025240177288651466, -0.019826289266347885, 0.021895762532949448, 0.03440498560667038, 0.04035472124814987, -0.07694004476070404, -0.01790463551878929, 0.005811153445392847, 0.022856589406728745, -0.05580185726284981, -0.0349038764834404, 0.024907583370804787, -0.0165557824075222, -0.01873612031340599, 0.011067984625697136, -0.015724297612905502, -0.06640790402889252, 0.018976327031850815, 0.019678469747304916, 0.026237959042191505, -0.0024367119185626507, -0.001311897998675704, 0.012712475843727589, -0.0031619512010365725, 0.03351806849241257, -0.06422756612300873, -0.0165557824075222, 0.006527154240757227, -0.013543960638344288, -0.017775293439626694, -0.026274913921952248, 0.07294891774654388, -0.025776023045182228, -0.02363264001905918, 0.0364375039935112, 0.00014132351498119533, -0.00696599343791604, 0.030838841572403908, -0.007076858077198267, -0.033481113612651825, -0.012241301126778126, 0.011927184648811817, 0.012259778566658497, 0.009996293112635612, 0.03758310526609421, -0.031245345249772072, 0.01181632000952959, -0.004501565359532833, -0.026718372479081154, -0.005760340951383114, 0.02006649598479271, 0.019992586225271225, 0.012435314245522022, 0.019881721585989, -0.04368066042661667, 0.009728370234370232, -0.013488528318703175, -0.0042475005611777306, -0.019364353269338608, 0.052734602242708206, -0.010781584307551384, 0.0014758851611986756, 0.016657408326864243, -0.01786768063902855, -0.035328857600688934, 0.027531379833817482, -0.01693456992506981, -0.019863244146108627, -0.01672207936644554, -0.025591248646378517, 0.034312598407268524, -0.019807811826467514, 0.029637807980179787, 0.0023270021192729473, 0.014975962229073048, 0.03004431165754795, -0.009783802554011345, 0.014661845751106739, 0.013183650560677052, -0.025757545605301857, 0.015779729932546616, -0.037028782069683075, -0.006730406079441309, -0.02950846590101719, -0.03813742846250534, 0.016546543687582016, -0.0182187519967556, -0.02245008572936058, 0.017913874238729477, -0.08314846456050873, -0.03141164034605026, 0.018274184316396713, 0.05668877437710762, -0.004654004238545895, 0.01256465632468462, -0.02431630529463291, 0.015724297612905502, 0.0074140713550150394, 0.016315575689077377, -0.010522900149226189, -0.011991855688393116, 0.015456375665962696, -0.016740556806325912, 0.01407980639487505, -0.0008014587801881135, 0.016481872648000717, 0.017110105603933334, -0.04467844218015671, -0.01331299263983965, 0.029249781742691994, 0.014772710390388966, 0.006711928639560938, -0.012721714563667774, 0.04567622393369675, -0.0012148914393037558, -0.024353260174393654, 0.0025429571978747845, 0.040391676127910614, -0.015299317426979542, 0.0234293881803751, 0.0174519382417202, 0.015825923532247543, -0.024667376652359962, -0.012426075525581837, -0.02429782785475254, 0.010190306231379509, 0.01908719167113304, -0.002623795997351408, 0.031300775706768036, 0.03983735293149948, 0.008933840319514275, 0.033980004489421844, -0.03283440321683884, -0.02261638268828392, -0.03983735293149948, 0.026866191998124123, -0.002062543760985136, 0.03545819967985153, 0.02091645821928978, -0.016426440328359604, -0.008319465443491936, -0.016648169606924057, 0.026773804798722267, 0.011326668784022331, -0.00809311680495739, -0.006725786719471216, 0.038913480937480927, 0.04641531780362129, 0.006092934403568506, -0.006938277278095484, -7.997265493031591e-05, 0.0023697311989963055, 0.0044022491201758385, -0.0023512537591159344, -0.032649628818035126, -0.006185321602970362, 0.0008788330596871674, 0.023521775379776955, -0.010328887030482292, 0.02261638268828392, 0.04408716410398483, 0.0324094220995903, -0.03529190272092819, -0.008573530241847038, 0.018495913594961166, 0.012056526727974415, 0.0038132809568196535, -0.062195051461458206, -0.023743504658341408, 0.013192889280617237, 0.0641167014837265, 0.0324094220995903, 0.03682553023099899, -0.05099772289395332, -0.015216168947517872, 0.047671783715486526, -0.04460453242063522, -0.015715058892965317, -0.0053215017542243, -0.049334753304719925, -0.007751284632831812, 0.022357698529958725, 0.016731318086385727, 0.02701401151716709, -0.019881721585989, 0.024704331532120705, 0.0075757489539682865, -0.023078318685293198, 0.017424222081899643, 0.028473729267716408, 0.0065363929606974125, -0.03423868864774704, 0.01517921406775713, 0.040909044444561005, 0.02226531133055687, -0.05957125499844551, -0.009506640955805779, -0.010744629427790642, -0.0026399637572467327, -0.007552652154117823, -0.0553583987057209, 0.016740556806325912, -0.01890241727232933, 0.008989272639155388, -0.034663669764995575, -0.01844971999526024, 0.002868622075766325, -0.03543972223997116, -0.026348823681473732, 0.038913480937480927, -0.022025104612112045, 0.022893544286489487, -0.011419055983424187, 0.029489988461136818, -0.013183650560677052, 0.001012794440612197, -0.01940130814909935, 0.06056903675198555, 0.014698800630867481, -0.006554870400577784, -0.01795082911849022, -0.03632663935422897, -0.003409086959436536, 0.0017934661591425538, 0.010356603190302849, 0.019456740468740463, 0.022875066846609116, 0.03876566141843796, -0.011261997744441032, 0.033998481929302216, -0.030062789097428322, 0.0015463304007425904, 0.02394675649702549, 0.018024738878011703, -0.007386355195194483, -0.011917945928871632, -0.003901048796251416, -0.05583881214261055, 0.019678469747304916, -0.0363081619143486, -0.0007489135605283082, 0.020029541105031967, -0.016657408326864243, 0.008970795199275017, 0.015105304308235645, -0.010227261111140251, 0.01104026846587658, 0.031097525730729103, -0.028584593906998634, 0.031300775706768036, 0.007016806397587061, -0.02006649598479271, 0.00666573503986001, 0.006379334721714258, 0.012324449606239796, -0.016278620809316635, 0.056614864617586136, -0.0074140713550150394, -0.012426075525581837, 0.014957484789192677, 0.01875459775328636, -0.008652059361338615, -0.010005531832575798, 0.05971907451748848, 0.01925348863005638, 0.00653177360072732, -0.024501079693436623, 0.007326303515583277, -0.00891074351966381, -0.00045211976976133883, -0.0006784683791920543, -0.0032081448007375, -0.010347364470362663, 0.039393894374370575, 0.022523995488882065, -0.02073168382048607, -0.030801886692643166, -0.013045069761574268, -0.016629692167043686, 0.0234293881803751, 0.004376842640340328, -0.0001623993448447436, 0.033240906894207, -0.020140405744314194, -0.03861784189939499, 0.04811524227261543, -0.006166844163089991, 0.00759884575381875, 0.025240177288651466, -0.008388755843043327, -0.003944932483136654, 0.014671084471046925, 0.005072056315839291, -0.03832220286130905, -0.011613068170845509, -0.025572771206498146, 0.03252028673887253, 0.03242789953947067, 0.0296932402998209, 0.005792676005512476, -0.015271601267158985, 0.0182834230363369, -0.042608968913555145, 0.017830725759267807, 0.011603829450905323, -0.027438992634415627, -0.009289531037211418, -0.022727247327566147, 0.007192342076450586, 4.622967753675766e-05, 0.03664075583219528, 0.010181067511439323, -0.021212097257375717, -0.007557271514087915, -0.03203987330198288, -0.009423492476344109, -0.018264945596456528, -0.06740568578243256, 0.011188087984919548, -0.06733177602291107, -0.027402037754654884, 0.01072615198791027, 0.025184744969010353, 0.029748672619462013, -0.003637745277956128, 0.019530650228261948, -0.025849932804703712, -0.0011294332798570395, 0.006504057440906763, 0.02900957502424717, -0.020491477102041245, 0.008106974884867668, 0.0074741230346262455, 0.0014400852378457785, 0.03847002238035202, -0.01239835936576128, -0.022376175969839096, -0.006111411843448877, 0.03351806849241257, -0.01770138368010521, 0.01374721247702837, -0.010569093748927116, -0.0030372284818440676, 0.01062452606856823, 0.0032474093604832888, 0.0008609330398030579, 0.029083484783768654, -0.013276037760078907, 0.0046055009588599205, 0.030820364132523537, -0.036197297275066376, 0.004919617436826229, 0.0021618600003421307, 0.02668141759932041, 0.047154415398836136, 0.03763853758573532, 0.03828524798154831, -0.0021560858003795147, -0.009820757433772087, -0.02161860093474388, 0.0009411944192834198, -0.02010345086455345, -0.008522717282176018, 0.015567240305244923, -0.01079082302749157, 0.0030418478418141603, 0.05583881214261055, 0.032963745296001434, 0.017997022718191147, -0.020528431981801987, 0.0388026162981987, -0.01595526561141014, -0.0021306793205440044, -0.013737973757088184, -0.0032474093604832888, 0.026515120640397072, -0.010430512949824333, -0.025868410244584084, 5.424137998488732e-05, -0.02900957502424717, 0.005367695353925228, -0.011169610545039177, 0.0051367273554205894, -0.011160371825098991, -0.021526213735342026, -0.005658715032041073, 0.021008845418691635, 0.014384684152901173, 0.005182920955121517, -0.01298039872199297, 0.01840352639555931, 0.03983735293149948, -0.016176994889974594, -0.049519527703523636, -0.025221699848771095, -0.028640026226639748, 0.034183256328105927, -0.0008372588199563324, -0.006762741599231958, -0.0357353612780571, 0.01079082302749157, -0.009996293112635612, -0.0144955487921834, -0.007390974555164576, -0.005630998872220516, -0.019641514867544174, -0.0025914604775607586, -0.06818173825740814, -0.011243520304560661, -0.007973013445734978, 0.06659267842769623, -0.004353745840489864, 0.003887190716341138, -0.014320013113319874, 0.004709436558187008, -0.043089382350444794, 0.0421285554766655, 0.003205835120752454, 0.015299317426979542, 0.008074639365077019, -0.00039264551014639437, -0.010532138869166374, -0.024685854092240334, -0.011668500490486622, -0.014615652151405811, 0.015243885107338428, 0.027198785915970802, 0.008264033123850822, 0.0044738491997122765, 0.0037647776771336794, 0.0017160918796434999, -0.004173590801656246, -0.018893178552389145, -0.003261267440393567, 0.018357332795858383, 0.04164814203977585, -0.013322231359779835, -0.02764224447309971, -0.03069102205336094, 0.006550251040607691, 0.02481519617140293, 0.05975602939724922, -0.007802097592502832, -0.0032705061603337526, -0.02376198209822178, -0.015105304308235645, -0.0357353612780571, 0.002517550718039274, -0.008037684485316277, 0.009428111836314201, -0.001788846799172461, 0.00419668760150671, -0.012204346247017384, -0.036031000316143036, -0.005580185912549496, 0.016463395208120346, 0.004919617436826229, 0.04438280314207077, 0.039061300456523895, -0.022727247327566147, 0.012416836805641651, 0.03712116926908493, -0.001627169200219214, 0.005460082553327084, -0.004942714236676693, -0.0024482603184878826, 0.05247591808438301, 0.01958608254790306, 0.016195472329854965, -0.013045069761574268, -0.037693969905376434, 0.03980039805173874, 0.0017380338395014405, -0.00979304127395153, 0.0027208025567233562, -0.03876566141843796, -0.018542107194662094, 0.0363081619143486, 0.005524753592908382, -0.0159183107316494, -0.027420515194535255, 0.004125087521970272, 0.011520681902766228, 0.00527068879455328, 0.024390215054154396, 0.014744994230568409, -0.044641487300395966, -0.0189208947122097, -0.028233522549271584, 0.0010930558200925589, 0.01736878976225853, -0.025535816326737404, 0.0027462090365588665, -0.006674973759800196, -0.02145230397582054, -0.004531591199338436, -0.0157889686524868, -0.02263486012816429, 0.0025314087979495525, -0.0070260451175272465, 0.02967476285994053, 0.007940677925944328, 0.009284911677241325, -0.015123781748116016, -0.021821852773427963, 0.03069102205336094, 0.019161101430654526, -0.006855128798633814, 0.04153727740049362, -0.008702872321009636, -0.014144477434456348, -0.01037508063018322, 0.035698406398296356, 0.03385066241025925, 0.009178666397929192, 0.01273095328360796, 0.01273095328360796, 0.05365847423672676, -0.007991490885615349, 0.02160012349486351, -0.02431630529463291, 0.010448990389704704, 0.0221174918115139, -0.021396871656179428, 0.02801179327070713, 0.004517733119428158, -0.007885245606303215, -0.01089244894683361, -0.0037232034374028444, 0.01104026846587658, 0.007566510234028101, 0.020491477102041245, 0.005575566552579403, -0.006310044322162867, 0.018311139196157455, 0.0013257560785859823, -0.020861025899648666, 0.0010532138403505087, -0.00779747823253274, -0.01703619584441185, -0.025609726086258888, -0.004767178557813168, 0.02701401151716709, -0.006120650563389063, 0.020990367978811264, 0.00939115695655346, 0.021008845418691635, -0.00919252447783947, 0.039541713893413544, 0.026348823681473732, -0.004566236399114132, -1.5473047824343666e-05, 0.015206930227577686, -0.007645039353519678, -0.01534551102668047, 0.008873788639903069, -0.018098648637533188, 0.0005999392597004771, 0.01940130814909935, 0.006961374077945948, 0.024168485775589943, 0.03847002238035202, -0.018828507512807846, -0.042756788432598114, 0.003868713276460767, -0.02023279294371605, -0.01536398846656084, -0.019992586225271225, -0.013137456960976124, -0.0009533202392049134, 0.028603071346879005, -0.01020878367125988, 0.009178666397929192, -0.032612673938274384, -0.01316517312079668, 0.02261638268828392, 0.016916092485189438, 0.013470050878822803, 0.03527342528104782, -0.0041158488020300865, 0.01079082302749157, 0.0026145572774112225, -0.01568734273314476, -0.04759787395596504, -0.016223188489675522, -0.023152226582169533, 0.036844007670879364, -0.018957849591970444, -0.02564668096601963, -0.012869534082710743, -0.03104209341108799, 0.024223918095231056, 0.008273271843791008, 0.005072056315839291, -0.002171098720282316, -0.020824071019887924, 0.00518754031509161, 0.04501103609800339, 0.0058019147254526615, -0.012934205122292042, 0.014902052469551563, -0.020787116140127182, -0.008822975680232048, -0.018874701112508774, -0.003524570958688855, 0.019493695348501205, -0.026699895039200783, -0.015003678388893604, -0.02359568513929844, -0.00995933823287487, 0.013109740801155567, -0.0064763412810862064, -0.0036169581580907106, 0.02108275517821312, -0.0064948187209665775, -0.014744994230568409, -0.011594590730965137, 0.0449371263384819, -0.028880232945084572, -0.007349400315433741, 0.039578668773174286, 0.022727247327566147, -0.00754803279414773, -3.5529366869013757e-05, 0.009257195517420769, -0.013553199358284473, 0.004727913998067379, -0.01553028542548418, -0.008106974884867668, -0.017091628164052963, -0.002104118000715971, -0.010245738551020622, 0.0013915819581598043, 0.030062789097428322, 0.014310774393379688, 0.002032517921179533, 0.03858088701963425, -0.011723932810127735, 0.0032104544807225466, 0.013876554556190968, 0.00035569065948948264, 0.012786385603249073, 0.009081659838557243, 0.009118614718317986, 0.007404832635074854, 0.017414983361959457, -0.02546190656721592, -0.006037502083927393, 0.028917187824845314, -0.013192889280617237, 0.0004766601196024567, 0.034645192325115204, 0.007742045912891626, 0.003951861523091793, 0.008966175839304924, 0.014089045114815235, 0.03577231615781784, 0.013534721918404102, 0.003510712878778577, 0.039726488292217255, 0.004108919762074947, 0.005861966405063868, 0.009950099512934685, -0.019105669111013412, 0.017775293439626694, 0.01029193215072155, -0.00518754031509161, 0.0174519382417202, 0.006541012320667505, 0.0035684548784047365, -0.005266069434583187, -0.03928302973508835, -0.005695669911801815, -0.007053761277347803, -0.01718401536345482, -0.0011132655199617147, -0.007012187037616968, -0.013359186239540577, 0.031818144023418427, 0.01637100800871849, 0.004619359038770199, 0.004328339360654354, -0.0057464828714728355, 0.009016988798975945, 0.012259778566658497, -0.025184744969010353, 0.010079441592097282, 0.005912779364734888, 0.0023223827593028545, 0.0010191460605710745, 0.041943781077861786, 0.03218769282102585, -0.0373983308672905, -0.007483361754566431, -0.00653177360072732, -0.013987419195473194, 0.0016525756800547242, -0.004162042401731014, -0.010781584307551384, -0.004781036637723446, 0.06019948795437813, -0.001264549558982253, 0.03883957117795944, -0.004785655997693539, 0.040576450526714325, -0.04578708857297897, 0.019198056310415268, -0.04268287867307663, 0.008855311200022697, -0.014911291189491749, -0.029970401898026466, -0.009977815672755241, -0.011603829450905323, -0.014874336309731007, -0.029933447018265724, 0.05048035457730293, 0.011603829450905323, 0.005395411513745785, 0.04079817980527878, 0.00839337520301342, -0.005630998872220516, -0.012185868807137012, -0.0005661601899191737, 0.006042121443897486, -0.017331834882497787, -0.013045069761574268, 0.015216168947517872, 0.006222276482731104, -0.007363258395344019, 0.001597143360413611, -0.00038744873018004, 0.02749442495405674, -0.023669594898819923, 0.02668141759932041, -0.000979881500825286, 0.036197297275066376, -0.0033536546397954226, 0.01198261696845293, -0.02259790524840355, 0.02544342912733555, 0.002529099117964506, -0.004076584242284298, 0.012878772802650928, 0.006610302720218897, 0.0031018995214253664, -0.012841817922890186, 0.0106430035084486, -0.017091628164052963, 0.014320013113319874, 0.014504787512123585, -0.020787116140127182, -0.002857073675841093, -0.016528066247701645, 0.012499985285103321, -0.018440481275320053, 0.027069443836808205, 0.03828524798154831, -0.014375445432960987, -0.032649628818035126, 0.002820118796080351, 0.03712116926908493, -0.013349947519600391, -0.024390215054154396, 0.01492976862937212, 0.00403039064258337, 0.003630816238000989, -0.021692510694265366, -0.005085914395749569, 0.013239082880318165, -0.0076773748733103275, -0.028603071346879005, -0.018412765115499496, 0.003857164876535535, -0.010199544951319695, 0.03200291842222214, 0.0015567239606752992, 0.021674033254384995, 0.011382101103663445, 0.007335542235523462, 0.007178483996540308, -0.0061899409629404545, -0.013543960638344288, 0.02041756734251976, 0.013654825277626514, 0.01055985502898693, -0.03102361597120762, 0.022191401571035385, 0.010901687666773796, 0.017128583043813705, 0.014107522554695606, -0.0015902143204584718, 0.01840352639555931, 0.013488528318703175, -0.0034598999191075563, -0.013451573438942432, 0.03259419649839401, -0.00469557847827673, -0.008952317759394646, 0.024667376652359962, 0.011021791025996208, -0.011483727023005486, 0.0212860070168972, 0.024556512013077736, -0.020953413099050522, -0.0174519382417202, 0.012721714563667774, 0.025406474247574806, -0.015927549451589584, -0.035014741122722626, -0.015373227186501026, -0.028251999989151955, 0.0023997570388019085, 0.01163154561072588, 0.006716547999531031, 0.01179784256964922, -0.006647257599979639, 0.01407980639487505, -0.004180519841611385, -0.010005531832575798, 0.018375810235738754, -0.002062543760985136, -0.02111971005797386, 0.0042290231212973595, -0.012721714563667774, -0.013941225595772266, 0.00224847299978137, -0.05447148159146309, -0.027217263355851173, 0.016860660165548325, 0.006647257599979639, 0.0073309228755533695, -0.018375810235738754, -0.014939007349312305, 0.008767543360590935, -0.004021151922643185, -0.01956760510802269, -0.006180702243000269, 0.0037832551170140505, 0.0008592007798142731, 0.03385066241025925, 0.0019932533614337444, -0.00847652368247509, -0.00952511839568615, -0.00839337520301342, 0.009123234078288078, 0.008134691044688225, -0.016066130250692368, 0.015160736627876759, 0.031430117785930634, 0.016906853765249252, -0.027124876156449318, 0.03120839037001133, 0.021988149732351303, 0.024999970570206642, 0.010060964152216911, 0.02359568513929844, 0.00039899713010527194, -0.005062817595899105, -0.02444564737379551, -0.0013580915983766317, -0.004097371362149715, 0.032612673938274384, -0.009922383353114128, -0.026348823681473732, -0.04955648258328438, 0.018024738878011703, -0.00015821304987184703, 0.0026699895970523357, -0.012712475843727589, -0.018865462392568588, 0.038728706538677216, 0.0010653396602720022, -0.003277435200288892, -0.01705467328429222, 0.02749442495405674, -0.007806716952472925, -0.001091323560103774, 0.033314816653728485, 0.0069244191981852055, -0.004457681439816952, 0.016278620809316635, -0.009802279993891716, 0.0027161831967532635, -0.0010064428206533194, 0.09408710151910782, -0.018080171197652817, 0.02091645821928978, 0.03494083136320114, -0.007871387526392937, -0.005566327832639217, -0.007109193596988916, -0.005150585435330868, 0.0026492024771869183, 0.007792858872562647, 0.016463395208120346, 0.01583516225218773, 0.02060234174132347, -0.011049507185816765, -0.008273271843791008, 0.024186963215470314, 0.006836651358753443, -0.015982981771230698, -0.050702083855867386, -0.004450752399861813, 0.002171098720282316, 0.00922485999763012, -0.023540252819657326, 0.02060234174132347, -0.0006484425393864512, 0.01322984416037798, 0.015982981771230698, -0.01509606558829546, 0.025720590725541115, -0.005109011195600033, -0.015169975347816944, 0.032280080020427704, -0.012472269125282764, 0.03961562365293503, -0.006314663682132959, -0.029065007343888283, 0.004693268798291683, -0.016056891530752182, -0.006661115679889917, 0.02921282686293125, -0.001960917841643095, 0.0021433825604617596, -0.0013222915586084127, 0.000128908985061571, -0.020472999662160873, 0.005159824155271053, 0.010910926386713982, 0.013765689916908741, -0.0174519382417202, -0.018089409917593002, -0.001121349399909377, 0.04150032252073288, -0.01820027455687523, 0.018976327031850815, 0.004831849597394466, 0.009562073275446892, 0.01805245503783226, 0.01316517312079668, -0.0035384290385991335, -0.03592013567686081, -0.026367301121354103, 0.0043722232803702354, 0.009894667193293571, -0.024741286411881447, -0.005751102231442928, -0.012204346247017384, -0.011696216650307178, -0.015391704626381397, -0.020990367978811264, 0.027124876156449318, 0.008153168484568596, -0.047154415398836136, 0.004831849597394466, -0.003727822797372937, 0.0033998482394963503, 0.007007567677646875, -0.003332867519930005, 0.033296339213848114, 0.07365106046199799, 0.033148519694805145, 0.0031665705610066652, -0.020029541105031967, -0.026348823681473732, -0.012093481607735157, -0.0010520590003579855, -0.0015047561610117555, 0.03150402754545212, -0.001407749718055129, 0.0025360281579196453, 0.002067163120955229, -0.003021060721948743, -0.00290788640268147, 0.0004887859104201198, 0.009349582716822624, 0.007686613593250513, 0.034829966723918915, 0.015003678388893604, -0.03542124480009079, 0.014089045114815235, 0.0012333688791841269, 0.014283058233559132, -0.01923501119017601, -0.0015844401204958558, -0.022893544286489487, 0.005732624791562557, 0.009987054392695427, -0.023059841245412827, 0.02444564737379551, -0.022487040609121323, -0.008046923205256462, 0.012158152647316456, 0.007922200486063957, -0.007557271514087915, 0.022893544286489487, -0.0144308777526021, 0.020454522222280502, 0.01778453215956688, -0.008689014241099358, -0.008439568802714348, 0.03375827521085739, -0.007506458554416895, 0.0006929038790985942, 0.01188099104911089, -0.012370643205940723, -0.0013430786784738302, -0.025036925449967384, 0.02246856316924095, -0.012546178884804249, 0.020491477102041245, -0.00850885920226574, -0.03209530562162399, -0.006799696478992701, -0.00309958984144032, -0.02025127038359642, 0.004316790960729122, 0.010837016627192497, 0.0062315152026712894, -0.0048919012770056725, -0.014005896635353565, 0.00274389935657382, 0.00970989279448986, 0.01281410176306963, 0.008846072480082512, 0.013774928636848927, 0.0031273060012608767, 0.020657774060964584, -0.025757545605301857, 0.00905856303870678, -0.003852545516565442, -0.02749442495405674, -0.0007731652003712952, -0.009437350556254387, -0.019955631345510483, -0.01029193215072155, 0.018763836473226547, 0.010920165106654167, 0.004219784401357174, -0.029933447018265724, 0.012259778566658497, 0.015308556146919727, 0.003924145363271236, -0.027438992634415627, -0.032113783061504364, 0.012546178884804249, -0.017008479684591293, -0.018338855355978012, -0.019105669111013412, 0.002713873516768217, 0.007155387196689844, -0.015881355851888657, 0.005210637114942074, -0.003009512322023511, -0.0042174747213721275, -0.03943084925413132, 0.0035037838388234377, -0.02511083520948887, -0.014818903990089893, -0.0003582890494726598, -0.0026076282374560833, -0.022154446691274643, 0.0037116550374776125, -0.02562820352613926, 0.02681075967848301, 0.011511443182826042, 0.01620471104979515, -0.01601993665099144, -0.003332867519930005, -0.00822707824409008, 0.0114744883030653, 0.009931622073054314, 0.011428294703364372, 0.004914998076856136, -0.017341073602437973, -0.012185868807137012, -0.023983711376786232, -0.001513994880951941, 0.023983711376786232, 0.013470050878822803, -0.011077223345637321, 0.013026592321693897, 0.03673314303159714, 0.0010890138801187277, -0.027050966396927834, -0.001019723480567336, 0.019696947187185287, 0.0035037838388234377, 0.017692144960165024, -0.006799696478992701, 0.01399665791541338, 0.019955631345510483, 0.009839234873652458, 0.016260143369436264, -0.007317064795643091, 0.0013904271181672812, 0.03072797693312168, -0.022062059491872787, -0.00880911760032177, -0.028455251827836037, -0.001365020638331771, -0.012185868807137012, 0.010162590071558952, 0.003834068076685071, -0.02984105981886387, -0.016832944005727768, -0.04526972025632858, 0.03209530562162399, -0.00944196991622448, -0.0007073393790051341, -0.006661115679889917, -0.01759975776076317, 0.005363075993955135, -0.018246468156576157, 0.004009603522717953, -0.0006825103191658854, 0.02228378877043724, -0.009234098717570305, -0.019013281911611557, 0.036160342395305634, 0.01941978558897972, 0.009700654074549675, 0.0018731501186266541, -0.03442346304655075, 0.00411353912204504, 0.012721714563667774, 0.003506093518808484, 0.00020657196000684053, -0.00433757808059454, 0.019807811826467514, -0.0197523795068264, 0.01651882752776146, 0.018505152314901352, -0.0076173231936991215, 0.0016190853202715516, 0.0028224284760653973, 0.0032104544807225466, 0.021230574697256088, 0.004386081360280514, -0.02836286462843418, 0.011178849264979362, 0.0007616168004460633, -0.013368424959480762, -0.0076773748733103275, -0.0009140556794591248, 0.015862878412008286, -0.03608643263578415, -0.011936423368752003, -0.0190502367913723, 0.027457470074295998, 0.00293329288251698, 0.01973390206694603, 0.0038733326364308596, 0.039763443171978, 0.029619330540299416, -0.007367877755314112, 0.008568910881876945, 0.015641149133443832, 3.926455246983096e-05, -0.0107076745480299, 0.004674791358411312, 0.007543413434177637, -0.014135238714516163, 0.020990367978811264, 0.0028524543158710003, 0.02448260225355625, -0.002616866957396269, -0.01476347167044878, 0.0025198603980243206, -0.004499255679547787, 0.027365082874894142, -0.011788603849709034, 0.005229114554822445, 0.019179578870534897, 0.008702872321009636, 0.00565409567207098, 0.03318547457456589, -0.019438263028860092, 0.0036631517577916384, 0.0056864311918616295, 0.004164352081716061, -0.0006513296393677592, 0.0014943626010790467, -0.010735390707850456, -0.019198056310415268, -0.005293785594403744, -0.002529099117964506, -0.023503297939896584, -0.01080930046737194, 0.012851056642830372, 0.014135238714516163, 0.01813560351729393, 0.013349947519600391, -0.02075016126036644, -0.025332564488053322, -0.007284729275852442, 0.012195107527077198, 0.0008083878201432526, -0.00485494639724493, 0.003986506722867489, -0.005954353604465723, -0.01029193215072155, 0.01432925183326006, 0.01298039872199297, -0.0011317429598420858, -0.0004223826399538666, 0.004714055918157101, -0.008915362879633904, 0.01568734273314476, -0.00792681984603405, -0.012490746565163136, 0.00581577280536294, 0.01993715390563011, -0.008162407204508781, -0.0014065948780626059, 0.024519557133316994, 0.0047487011179327965, -0.03139316290616989, -0.030081266537308693, -0.007621942553669214, -0.00754803279414773, -0.008596627041697502, 0.006753502879291773, 0.0008667072397656739, 0.005081295035779476, -0.0013407689984887838, -0.024704331532120705, -0.02309679612517357, -0.016759034246206284, -0.004845707677304745, -0.008462665602564812, -0.009086279198527336, -0.0003314390196464956, -0.01383036095649004, -0.0079591553658247, 0.009571311995387077, 0.020694728940725327, -0.015521046705543995, -0.022875066846609116, -0.0018338855588808656, -0.016195472329854965, -0.017812248319387436, 0.01856982335448265, -0.0032220028806477785, 0.0010589880403131247, 0.0025660539977252483, -0.00882759504020214, -0.01954912766814232, -0.03327786177396774, 0.0009498557192273438, -0.004046558402478695, 0.015659626573324203, 0.004194377921521664, 0.00477179791778326, 0.013941225595772266, 0.01349776703864336, -0.03037690557539463, 0.006517915520817041, -0.013008114881813526, -0.0009001975995488465, 0.011936423368752003, 0.012342927046120167, 0.01906871423125267, 0.011622306890785694, -0.005640237592160702, 0.007423310074955225, 0.010393558070063591, -0.014477071352303028, 0.0136640639975667, -0.008051542565226555, -0.015825923532247543, -0.003039538161829114, 0.0041874488815665245, 0.0005693359998986125, -0.023207658901810646, 0.00329129328019917, 0.004905759356915951, 0.006790457759052515, -0.027974838390946388, 0.002702325116842985, 0.004831849597394466, -0.01890241727232933, -0.038359157741069794, 0.007723568473011255, -0.0033166997600346804, -0.009400395676493645, -0.01139133982360363, -0.0008418781799264252, -0.010615287348628044, -0.0073309228755533695, -0.01796930655837059, -0.01693456992506981, -0.013340708799660206, 0.0165557824075222, 0.008171645924448967, 0.01765519008040428, 0.009455827996134758, -0.005917398724704981, -0.019955631345510483, -0.00619917968288064, -0.043606750667095184, 0.017442699521780014, -0.01441240031272173, 0.0018027048790827394, -0.026921624317765236, 0.004466920159757137, -0.014310774393379688, 0.020990367978811264, -0.0174519382417202, 0.02026974782347679, -0.012888011522591114, 0.008462665602564812, 0.0035268806386739016, -0.022172924131155014, -0.006296186242252588, -0.0067812190391123295, 0.002785473596304655, 0.02780854143202305, 0.010920165106654167, 0.03154098242521286, 0.02041756734251976, 0.014754232950508595, -0.00392183568328619, 0.010485945269465446, -0.020472999662160873, 0.003088041441515088, -0.019512172788381577, 3.0134100597933866e-05, -0.018865462392568588, 0.01122504286468029, -0.033462636172771454, -0.00696599343791604, -0.015798207372426987, -0.012426075525581837, -0.025092357769608498, 0.01710086688399315, -0.022413130849599838, -0.014532503671944141, 0.01990019902586937, -0.001651420840062201, -0.04460453242063522, -0.0004007293900940567, -0.04538058489561081, 0.004674791358411312, -0.0074140713550150394, 0.0040557971224188805, 0.0027231122367084026, 0.013386902399361134, 0.011188087984919548, -0.011095700785517693, 0.020325180143117905, 0.014440116472542286, -0.0233000461012125, -0.019881721585989, 0.01316517312079668, -0.038063518702983856, -0.008906124159693718, -0.012851056642830372, 0.01105874590575695, -0.014717278070747852, 0.002104118000715971, 0.008998511359095573, -0.004873423837125301, 0.0011710075195878744, 0.007183103356510401, 0.003582312958315015, 0.0008811427396722138, -0.0025360281579196453, 0.00825941376388073, -0.016574259847402573, -0.015641149133443832, 0.03290831297636032, -0.037693969905376434, -0.004494636319577694, -0.003231241600587964, 0.008088497444987297, 0.001377723878249526, 0.002755447756499052, 0.014421639032661915, 0.023743504658341408, 0.00930800847709179, -0.0008747911197133362, 0.018440481275320053, 0.02448260225355625, 0.012721714563667774, 0.006213037762790918, 0.008707491680979729, 0.0007598845404572785, -0.01954912766814232, 0.01755356416106224, 0.02612709440290928, -0.02010345086455345, -0.014449355192482471, -0.006328521762043238, 0.006647257599979639, -0.016999240964651108, 0.000982768600806594, 0.014273819513618946, -0.015733536332845688, -0.004711746238172054, -0.02647816576063633, 0.021008845418691635, 0.0037832551170140505, 0.005751102231442928, -0.014172193594276905, -0.010606048628687859, 0.011465249583125114, 0.0038895003963261843, -0.02751290239393711, -0.00661492208018899, -0.006795077119022608, -0.016916092485189438, -0.014153716154396534, -0.013340708799660206, 0.009127853438258171, -0.0025060023181140423, -0.010356603190302849, -0.00042988909990526736, 0.0046470751985907555, -0.008799878880381584, 0.010079441592097282, -0.0016202401602640748, 0.00666573503986001, 0.004365294240415096, 0.022560950368642807, 0.030450815334916115, -0.006776599679142237, -0.018394287675619125, 0.017692144960165024, -0.005418508313596249, 0.003039538161829114, -0.007986871525645256, 0.029101962223649025, -0.0034552805591374636, 0.0014527883613482118, 0.017063912004232407, -0.003933384083211422, -0.014116761274635792, -0.004395320080220699, -0.003933384083211422, 0.006808935198932886, -0.00867515616118908, -0.018976327031850815, 0.004081203602254391, -0.008116213604807854, 0.06141899898648262, 0.01778453215956688, 0.02697705663740635, 0.0021445374004542828, -0.00397957768291235, 0.000391779380152002, 0.0315779373049736, -0.02581297792494297, -0.006203799042850733, -0.012878772802650928, -0.013553199358284473, -0.0041458746418356895, 0.016380246728658676, -0.03695487231016159, 0.004725604318082333, 0.0023951376788318157, 0.03488539904356003, -0.004307552240788937, -0.025868410244584084, 0.005455463193356991, -0.00540465023368597, -0.00861048512160778, -0.003981887362897396, -0.001860446878708899, -0.011261997744441032, 0.0012668592389672995, -0.0034899257589131594, 0.006273089442402124, 0.022542472928762436, 0.012444552965462208, 0.014939007349312305, 0.010772345587611198, -0.002116821240633726, -0.009451208636164665, -0.020824071019887924, 0.011271236464381218, 0.022727247327566147, 0.014837381429970264, -0.015040633268654346, 0.00337444175966084, 0.015585717745125294, -0.01393198687583208, 0.017673667520284653, 0.0024967635981738567, 0.0030025832820683718, 0.008070020005106926, 0.007945297285914421, -0.0028432155959308147, -0.019198056310415268, 0.008407233282923698, -0.003088041441515088, 0.0158536396920681, 0.006490199360996485, 0.022838111966848373, -0.0043606748804450035, 0.008268652483820915, 0.016500350087881088, -0.025424951687455177, -0.011613068170845509, 0.007580368313938379, 0.015844400972127914, 0.002122595440596342, -0.010282693430781364, -0.010097919031977654, -0.007912961766123772, 0.020546909421682358, 0.016574259847402573, -0.03660380095243454, -0.0003424099995754659, 0.03470062464475632, 0.01685142144560814, -0.0024367119185626507, 0.017331834882497787, 0.0007044522790238261, 0.024852151051163673, 0.01291572768241167, 0.006120650563389063, 0.009848473593592644, -0.01771986111998558, 0.012490746565163136, 0.004767178557813168, 0.010522900149226189, -0.002706944476813078, -0.018689926713705063, -0.011354384943842888, 0.0034968547988682985, -0.015964504331350327, 0.024538034573197365, -0.0029956542421132326, -0.012269017286598682, -0.008434949442744255, 0.0018950920784845948, -0.017248686403036118, -0.0063516185618937016, 0.02126752957701683, -0.004598571918904781, -0.01680522784590721, -0.006134508643299341, -0.006300805602222681, -0.0050120046362280846, -0.013774928636848927, 0.02950846590101719, 0.00995933823287487, -0.02431630529463291, -0.007996110245585442, -0.012777146883308887, 0.0023812795989215374, -0.021858807653188705, -0.029249781742691994, -0.00619917968288064, -0.008287129923701286, 0.0024667377583682537, -0.006079076323658228, -0.01374721247702837, 0.021674033254384995, -0.02718030847609043, -0.0241315308958292, -0.014948246069252491, -0.011908707208931446, 0.030118221417069435, 0.0013627109583467245, -0.00016066708485595882, 0.02093493565917015, -0.03795265406370163, 0.0031319253612309694, 0.013895031996071339, -0.013590154238045216, 0.002646892797201872, 0.00842571072280407, -0.007603465113788843, -0.005515514872968197, 0.007589607033878565, 0.016657408326864243, -0.02448260225355625, -0.012287494726479053, -0.004882662557065487, 0.010153351351618767, 0.0006161070195958018, 0.006688831839710474, -0.00466324295848608, 0.01853286847472191, -0.006037502083927393, 0.01181632000952959, 0.004628597758710384, 0.005908160004764795, 0.007400213275104761, -0.018708404153585434, -0.03000735677778721, 0.006060598883777857, 0.014865097589790821, 0.0031573318410664797, -0.006938277278095484, 0.020787116140127182, 0.004092752002179623, 0.013645586557686329, -0.00022620423987973481, 0.011862513609230518, 0.009903905913233757, -0.011151133105158806, 0.0068412707187235355, -0.0010936332400888205, -0.01072615198791027, -0.021877285093069077, 0.018006261438131332, 0.0011825559195131063, 0.012139675207436085, 0.007160006556659937, -0.009164808318018913, -0.001890472718514502, -0.000852271739859134, -0.007072238717228174, -0.015327033586800098, -0.008550433441996574, -0.016832944005727768, -0.014421639032661915, 0.007520316634327173, -0.0011652333196252584, -0.0034552805591374636, -0.0026792283169925213, -0.002515241038054228, 0.0029171251226216555, -0.006286947522312403, -0.003332867519930005, 0.00924333743751049, -0.009811518713831902, 0.030062789097428322, 0.0079591553658247, -0.009007750079035759, -0.009571311995387077, 0.004762559197843075, 0.011779365129768848, 0.021544691175222397, -0.00819474272429943, 0.006855128798633814, 0.010171828791499138, 0.003914906643331051, -0.0014366207178682089, 0.0008932685595937073, -0.0027531380765140057, 0.017830725759267807, -0.007437168154865503, -0.004007293842732906, 0.006042121443897486, 0.02629339136183262, -0.026533598080277443, 0.0061483667232096195, -0.0030302994418889284, 0.0050120046362280846, -0.0022681052796542645, 0.011760887689888477, -0.04778264835476875, 0.005113630555570126, -0.0008926911395974457, 0.009765325114130974, 0.007589607033878565, 0.011289713904261589, 0.007742045912891626, 0.003469138639047742, 0.01923501119017601, 0.01079082302749157, 0.007880626246333122, -0.025923842564225197, -0.012167391367256641, -0.02431630529463291, -0.0006328521994873881, 0.01798778399825096, -0.0051552047953009605, 0.020325180143117905, 0.022671815007925034, -0.0053815534338355064, 0.021729465574026108, -0.012010333128273487, 0.0158536396920681, -0.008878407999873161, 0.00466324295848608, 0.0009313782793469727, 0.01873612031340599, -0.004566236399114132, -0.0012414527591317892, 0.0017599757993593812, 0.00469557847827673, -0.007885245606303215, -0.009719131514430046, -0.01155763678252697, 0.0048087527975440025, -0.00954359583556652, -0.01087397150695324, 0.031097525730729103, 0.003679319517686963, -3.919237497029826e-05, -0.010495183989405632, -0.0056032827123999596, -0.008938459679484367, 0.01139133982360363, -0.023337000980973244, 0.021341439336538315, 0.038396112620830536, 0.006929038558155298, -0.002861693035811186, 0.002480595838278532, -0.010522900149226189, -0.003845616476610303, -0.0005935876397415996, -0.01171469409018755, -0.00017380338977091014, 0.01374721247702837, -0.008439568802714348, -0.017895396798849106, -0.027217263355851173, 0.02025127038359642, -0.002529099117964506, -0.011465249583125114, 0.0007044522790238261, 0.012379881925880909, 0.0120657654479146, 0.012499985285103321, -0.002174563240259886, 0.00041574230999685824, -0.002529099117964506, 0.014735755510628223, 0.020214315503835678, -0.0040742745622992516, -0.001538246520794928, -0.007940677925944328, -0.00271849287673831, -0.010827777907252312, 0.013035831041634083, 0.019641514867544174, -0.013821122236549854, 0.01105874590575695, -0.020011063665151596, -0.022043582051992416, -0.0025729830376803875, 0.006545631680637598, 0.006693451199680567, 0.01735031232237816, -0.0229489766061306, 0.01037508063018322, 0.016565021127462387, 0.000982768600806594, -0.041093818843364716, -0.004942714236676693, -0.013155934400856495, -0.0175166092813015, 0.0014285368379205465, -0.014236864633858204, 0.010615287348628044, 0.0036747001577168703, 0.0004916730104014277, -0.003284364240244031, 0.006480960641056299, 0.00864282064139843, 0.014560219831764698, -0.010282693430781364, -0.0002098921249853447, -0.013377663679420948, -0.010800061747431755, -0.011261997744441032, -0.0099408607929945, -0.004272907041013241, -0.018957849591970444, -0.00018838323012460023, 0.013867315836250782, 0.02379893697798252, -0.014966723509132862, -0.008735207840800285, 0.01095711998641491, 0.005681811831891537, -0.016361769288778305, 0.0026214863173663616, 0.03453432768583298, 0.012333688326179981, 0.0058434889651834965, -0.005062817595899105, -0.0018373500788584352, 0.005732624791562557, 0.00939115695655346, -0.008130071684718132, -0.009344963356852531, 0.014504787512123585, -0.020454522222280502, 0.0016583498800173402, -0.010153351351618767, -0.017664428800344467, -0.024427169933915138, 0.0029425316024571657, 0.006462483201175928, -0.011095700785517693, -0.005852727685123682, 0.006023644004017115, -0.0062915668822824955, -0.020048018544912338, 0.0019147243583574891, -0.012989637441933155, -0.008652059361338615, 0.012093481607735157, 0.004510804079473019, -0.000846497539896518, 0.006402431521564722, 0.0009908524807542562, -0.005958972964435816, -0.009617505595088005, -0.016010697931051254, -0.020177360624074936, -0.0040257712826132774, -0.004510804079473019, -0.013941225595772266, -0.009811518713831902, -0.022838111966848373, 0.020509954541921616, 0.0012726334389299154, 0.003919526003301144, -0.017978545278310776, -0.019863244146108627, -0.015493330545723438, -0.015825923532247543, -0.002986415522173047, 0.01188099104911089, -0.0018373500788584352, -0.011936423368752003, 0.022671815007925034, -0.01374721247702837, -0.008120832964777946, -0.008804498240351677, -0.009127853438258171, -0.001221820479258895, -0.0018073242390528321, 0.012860295362770557, 0.020528431981801987, 0.008370278403162956, 0.0019043307984247804, -0.017359551042318344, -0.013091263361275196, -0.014440116472542286, -0.04253505915403366, 0.0044022491201758385, 0.0026214863173663616, 0.010134873911738396, 0.009146330878138542, -0.013701018877327442, 0.014948246069252491, -0.011086462065577507, -0.002473666798323393, 0.020472999662160873, 0.021526213735342026, -0.00018520742014516145, 0.003690867917612195, -0.016149278730154037, 0.00658258656039834, 0.011677739210426807, -0.004238261841237545, -0.002342015039175749, -0.002732350956648588, -0.0013384593185037374, 0.0019124146783724427, 0.009497402235865593, -0.008254794403910637, -0.0070860967971384525, -0.0004206503799650818, -0.005367695353925228, 0.0017599757993593812, 0.011945662088692188, -0.015539524145424366, -0.0057464828714728355, -0.003774016397073865, 0.013507005758583546, 0.02464889921247959, -0.003690867917612195, -0.006000547204166651, 0.02984105981886387, 0.035661451518535614, 0.0014562528813257813, 0.008111594244837761, 0.016860660165548325, -0.0011438687797635794, -0.003487616078928113, -0.006402431521564722, -0.028270477429032326, -0.013137456960976124, -0.006845890078693628, -0.004224403761327267, -0.0068412707187235355, -0.007520316634327173, 0.017710622400045395, -0.02195119485259056, 0.009381918236613274, 0.017322596162557602, 0.0070676193572580814, 0.0032566480804234743, -0.001508220680989325, -0.005788056645542383, 0.011456010863184929, 0.010319648310542107, 0.00011562833242351189, 0.005289166234433651, 0.004215165041387081, -0.018597539514303207, 0.018338855355978012, 0.005372314713895321, -0.011188087984919548, -0.006120650563389063, -0.005312263034284115, 0.004681720398366451, -0.009150950238108635, -0.004570855759084225, 0.005723386071622372, -0.004593952558934689, -0.01511454302817583, -0.015641149133443832, 0.002290047239512205, -0.004619359038770199, 0.013045069761574268, -0.0006126424996182323, 0.02494453825056553, 0.01813560351729393, -0.018412765115499496, -0.0003464519395492971, -0.0018096339190378785, 0.018994804471731186, 0.009839234873652458, 0.026422733440995216, -0.010827777907252312, -0.0144308777526021, -0.0009290685993619263, -0.010837016627192497, 0.017248686403036118, 0.009534357115626335, -0.0020729373209178448, -0.001489743241108954, 0.0015278529608622193, 0.007409451995044947, 0.015336272306740284, 0.001186020439490676, 0.0054000308737158775, 0.0026815379969775677, -0.00754803279414773, 0.0214153490960598, -0.03519951552152634, -0.004739462397992611, 0.01391350943595171, -0.01536398846656084, -0.011354384943842888, 0.008485762402415276, 0.00222306651994586, -0.014273819513618946, -0.0026145572774112225, -0.017211731523275375, -0.0020579244010150433, 0.007783620152622461, 0.0037416808772832155, -0.0035199515987187624, 0.01798778399825096, -0.0025960798375308514, -0.035181038081645966, -0.043976299464702606, 0.013405379839241505, -0.011409817263484001, 0.010716913267970085, 0.01542865950614214, 0.0022080536000430584, 0.0091093759983778, 0.017858441919088364, 0.004483087919652462, 0.008116213604807854, 0.004164352081716061, 0.007862148806452751, 0.003397538559511304, -0.011779365129768848, -0.003118067281320691, -0.0037578486371785402, -0.05314110592007637, -0.0034853063989430666, 0.006314663682132959, 0.017026957124471664, 0.014467832632362843, 0.00032220029970631003, -0.017562802881002426, -0.008841453120112419, -0.018061693757772446, 0.005811153445392847, -0.02261638268828392, -0.004882662557065487, 0.009150950238108635, -1.9127033738186583e-05, 0.0008719040197320282, -0.007312445435672998, 0.020694728940725327, 0.007257013116031885, -0.003552287118509412, -0.0049380948767066, -0.007783620152622461, 0.0015925240004435182, 0.02363264001905918, 0.01757204160094261, 0.0063516185618937016, 0.006323902402073145, 0.0005566327599808574, -0.016749795526266098, -0.0023085246793925762, 0.007889864966273308, 0.02699553407728672, -0.015105304308235645, -0.01635253056883812, 0.01830190047621727, 0.02394675649702549, -0.0011727397795766592, 0.029471511021256447, -0.0009446589392609894, -0.008347181603312492, -0.028547639027237892, -0.018172558397054672, 0.007377116475254297, 0.013183650560677052, -0.0006409360794350505, 0.006383954081684351, -0.012102720327675343, -0.002683847676962614, -0.02429782785475254, 0.00301644136197865, 0.00806078128516674, 0.004975049756467342, -0.015539524145424366, 0.011686977930366993, -0.011936423368752003, -0.0152623625472188, -0.005293785594403744, 0.007478742394596338, -0.020861025899648666, 0.025018448010087013, 0.012888011522591114, 0.010227261111140251, 0.0006449780194088817, -0.011770126409828663, -0.01670360192656517, 0.0069244191981852055, 0.009996293112635612, 0.01020878367125988, 0.0054416051134467125, -0.018043216317892075, 0.02280115708708763, 0.006485580001026392, 0.028584593906998634, 0.02917587198317051, 0.03037690557539463, 0.0008499620598740876, -0.007196961436420679, 0.0012195107992738485, 0.01324832160025835, 0.00905856303870678, -0.004293694160878658, -0.010800061747431755, -0.023725027218461037, 0.011908707208931446, -0.018874701112508774, -0.0001488299749325961, -0.0036285065580159426, -0.008264033123850822, -0.013137456960976124, 0.013645586557686329, -0.008199362084269524, -0.024223918095231056, -0.00809311680495739, 0.0008124297601170838, -0.004543139599263668, 0.0015359368408098817, -0.016260143369436264, -0.008407233282923698, 0.009811518713831902, 0.002605318557471037, -0.001365020638331771, 0.007451026234775782, -0.02025127038359642, 0.0006005166796967387, -0.012426075525581837, -0.011437533423304558, 0.004556997679173946, 0.014541742391884327, -0.008291749283671379, 0.0021018083207309246, -0.006360857281833887, 0.0037347518373280764, -0.01281410176306963, 0.007589607033878565, 0.002515241038054228, -0.0026214863173663616, 0.01432925183326006, -0.03035842813551426, 0.021304484456777573, -0.009215621277689934, -0.002271569799631834, 0.0013569367583841085, -0.007612703833729029, -0.028732413426041603, -0.0011456010397523642, 0.00364929367788136, 0.0057280054315924644, 0.03102361597120762, 0.006896703038364649, -0.015059110708534718, 0.007728187832981348, 0.011566875502467155, 0.005506276153028011, 0.02093493565917015, 0.01940130814909935, 0.015031394548714161, 0.0136640639975667, -0.001198723679408431, 0.016962286084890366, -0.008790640160441399, 0.0077790007926523685, -0.02226531133055687, -0.001990943681448698, -0.011502204462885857, -0.00137194967828691, 0.023706549778580666, 0.023558730259537697, 0.0035037838388234377, 0.006176082883030176, -0.003469138639047742, -0.005450843833386898, -0.010846255347132683, 0.023503297939896584, -0.00015604772488586605, -0.01139133982360363, -8.58009843796026e-06, 0.023410910740494728, -0.027032488957047462, 0.019161101430654526, 0.016140040010213852, 0.004522352479398251, 0.01873612031340599, -0.01846819743514061, 0.014153716154396534, 0.01095711998641491, 0.013303753919899464, -0.009506640955805779, 0.003171189920976758, 0.009987054392695427, -0.005626379512250423, -0.0018073242390528321, 0.0047186752781271935, 0.00573724415153265, -0.018477436155080795, -0.014726516790688038, -0.010513661429286003, 0.0181540809571743, 0.014135238714516163, 0.0020244340412318707, -0.003506093518808484, -0.018505152314901352, 0.006453244481235743, -0.005007385276257992, 0.0011253913398832083, 0.026552075520157814, 0.022339221090078354, 0.000161821924848482, 0.013433095999062061, -0.010587571188807487, 0.010855494067072868, -0.0015036013210192323, 0.0009440815192647278, 0.003755538957193494, -0.0053215017542243, 0.023484820500016212, -0.0031919770408421755, -0.0136640639975667, -0.0030487768817692995, 0.015779729932546616, -0.010190306231379509, -0.013386902399361134, -0.00485494639724493, 0.0029540800023823977, -0.01179784256964922, -0.01645415648818016, 0.01432925183326006, -0.02429782785475254, -0.007113812956959009, 0.014292296953499317, -0.0083425622433424, 0.002307369839400053, 0.010578332468867302, -0.009552834555506706, 0.002529099117964506, 0.02174794301390648, 0.026090139523148537, -0.015049871988594532, 0.013691780157387257, 0.005242972634732723, 0.00026994379004463553, -0.00020844857499469072, -0.0061483667232096195, -0.01796930655837059, 0.027623767033219337, 0.020861025899648666, 0.003076493041589856, 0.016112323850393295, 0.01713782176375389, 0.0029517703223973513, -0.0022681052796542645, -0.022339221090078354, 0.010837016627192497, -0.012278256006538868, 0.004166661761701107, 0.009044704958796501, 0.0035499774385243654, 0.01651882752776146, -0.01214891392737627, 0.009922383353114128, -0.01685142144560814, -0.002439021598547697, -0.011197326704859734, -0.004090442322194576, 0.018311139196157455, -0.03004431165754795, -0.010448990389704704, -0.005127488635480404, -0.013691780157387257, 0.03359197825193405, -0.007755903992801905, -0.009756086394190788, 0.004799514077603817, 0.0043191006407141685, -0.024334782734513283, -0.0040557971224188805, 0.007053761277347803, 0.006721167359501123, 0.013470050878822803, 0.0022380794398486614, -0.007737426552921534, 0.03118991293013096, -0.03235398977994919, 0.010763106867671013, -0.0053399791941046715, -0.020214315503835678, 0.006545631680637598, 0.01181632000952959, -0.019345875829458237, 0.00995933823287487, 0.002204589080065489, -0.002480595838278532, -0.024556512013077736, 0.012333688326179981, -0.014301535673439503, -0.007945297285914421, -0.003804042236879468, 0.005099772475659847, 0.026755327358841896, 0.014504787512123585, -0.02751290239393711, -0.0035915516782552004, -0.0018396597588434815, 0.0031734996009618044, -0.01925348863005638, 0.011031029745936394, 0.025757545605301857, -0.01856982335448265, -0.024242395535111427, 0.02075016126036644, 0.01112341694533825, 0.0030072026420384645, 0.02143382653594017, -0.009737608954310417, 0.012342927046120167, -0.006490199360996485, -0.008236316964030266, 0.0030372284818440676, -0.006989090237766504, -0.00413894560188055, 0.0062499926425516605, -0.013691780157387257, -0.0010890138801187277, -0.025258654728531837, 0.007936058565974236, 0.002325847279280424, 0.0030857317615300417, 0.005912779364734888, -0.0008540039998479187, 0.005825011525303125, 0.0315779373049736, 0.0022450084798038006, -0.011603829450905323, 0.020362135022878647, -0.01771986111998558, -0.0026076282374560833, 0.012250539846718311, -0.010485945269465446, 0.0068227932788431644, -0.012601611204445362, 0.015641149133443832, 0.012518462724983692, -0.013433095999062061, 0.014052090235054493, -0.00691518047824502, 0.0009082814794965088, 0.004293694160878658, 0.0159183107316494, 0.02714335359632969, 0.005760340951383114, -0.015641149133443832, 0.010162590071558952, -0.00817626528441906, -0.010190306231379509, 0.006721167359501123, 0.012823340483009815, 0.012389120645821095, 0.0026676799170672894, -0.0257945004850626, 0.004323720000684261, -0.005533992312848568, 0.017645951360464096, -5.222040999797173e-05, -0.011400578543543816, -0.010134873911738396, 0.008619723841547966, -0.01782148703932762, 0.006522534880787134, 0.015216168947517872, 0.009257195517420769, -0.01401513535529375, 0.00230621499940753, -0.020953413099050522, 0.012370643205940723, 0.02446412481367588, 0.018264945596456528, 0.024538034573197365, -0.0009741073590703309, -0.00891074351966381, -0.004822610877454281, 0.004785655997693539, 0.00916018895804882, 0.0087259691208601, -0.015188452787697315, -0.004510804079473019, -0.007839052006602287, -0.006956754717975855, -0.002540647517889738, 0.00939115695655346, -0.01072615198791027, -0.004536210559308529, -0.014403161592781544, 0.007224677596241236, 0.029563898220658302, 0.005307643674314022, -0.000688861939124763, 0.015484091825783253, 0.014061328954994678, -0.008356420323252678, -0.002068317960947752, -0.005977450404316187, -0.015678104013204575, -0.021008845418691635, 0.005150585435330868, -0.005575566552579403, 0.005469321273267269, -0.0006686522392556071, 0.008088497444987297, -0.0071276710368692875, -0.012841817922890186, -0.01778453215956688, -0.0036400549579411745, -0.0007183103589341044, 0.018495913594961166, 0.008356420323252678, -0.012971160002052784, 0.018061693757772446, 0.02243160828948021, 0.0021445374004542828, -0.016648169606924057, -0.013682541437447071, -0.011465249583125114, 0.002630725037306547, 0.007404832635074854, -0.00649943808093667, -0.012093481607735157, -0.008818356320261955, -0.005769579671323299, 0.017239447683095932, 0.01492976862937212, 0.012971160002052784, 0.006933657918125391, -0.0076311812736094, 0.014717278070747852, -0.010144112631678581, -0.0077790007926523685, 0.006753502879291773, -0.004395320080220699, 0.004395320080220699, 0.004249810241162777, 0.01711934432387352, -0.01838504895567894, -0.004757939837872982, 0.0009308008593507111, -0.009423492476344109, 0.0151976915076375, 0.016740556806325912, -0.02010345086455345, 0.004640146158635616, 0.006416289601475, -0.016232427209615707, 0.01938283070921898, -0.004654004238545895, 0.0013869625981897116, -0.00573724415153265, -0.003882571356371045, 0.014902052469551563, -0.006240753922611475, 0.020011063665151596, -0.020805593580007553, -0.01566886529326439, 0.026053184643387794, -0.00658258656039834, -0.005344598554074764, -0.009428111836314201, 0.003697796957567334, -0.010070202872157097, 0.01223206240683794, 0.021507736295461655, -0.0004726181796286255, 0.008605865761637688, 0.00792681984603405, 0.012527701444923878, 0.010245738551020622, 0.006564109120517969, 0.0005808843998238444, 0.0033166997600346804, 0.01736878976225853, 0.005372314713895321, -0.0023223827593028545, 0.006028263363987207, -0.00309958984144032, 0.034331075847148895, -0.004194377921521664, -0.0007933749002404511, -0.0020544598810374737, 0.004914998076856136, 0.007672755513340235, -0.004058106802403927, -0.01020878367125988, -0.007996110245585442, 0.0012541559990495443, -0.02006649598479271, 0.00946044735610485, 0.012075004167854786, -0.009257195517420769, -0.0016802918398752809, -0.01908719167113304, 0.023189181461930275, 0.008005348965525627, -0.0026907767169177532, -0.006411670241504908, -0.0107076745480299, -0.01391350943595171, 0.009067801758646965, 0.009635983034968376, 0.00035771162947639823, 0.018957849591970444, -0.005566327832639217, -0.0007546877604909241, 0.005099772475659847, -0.020343657582998276, -0.008531956002116203, 0.007363258395344019, 0.004905759356915951, -0.005085914395749569, -0.014421639032661915, -0.021138187497854233, -0.003046467201784253, -0.011677739210426807, 0.006310044322162867, -0.0022681052796542645, -0.004106610082089901, -0.012075004167854786, -0.0027092541567981243, -0.011742410250008106, -0.0073309228755533695, -0.0018027048790827394, -0.0027762348763644695, 0.00581577280536294, 0.011927184648811817, -0.007400213275104761, 0.006388573441654444, -0.021563168615102768, 0.044456712901592255, 0.00419668760150671, 0.01757204160094261, -0.012527701444923878, -0.019826289266347885, -0.03743528574705124, 0.004395320080220699, -0.0020879502408206463, -0.00891074351966381, -0.010338125750422478, -0.0029656284023076296, -0.0013280657585710287, -0.00036290838033892214, -0.030339950695633888, -0.010827777907252312, 0.005307643674314022, -0.0036516033578664064, -0.001425072317942977, -0.0034437321592122316, 0.029065007343888283, 0.005829630885273218, -0.008337942883372307, -0.008883027359843254, -0.004651694558560848, 0.020380612462759018, 0.01880079135298729, 0.007257013116031885, -0.030949706211686134, 0.010273454710841179, -0.0006998329190537333, 0.0025614346377551556, -0.01678675040602684, -0.0031504028011113405, 0.01637100800871849, -0.006480960641056299, 0.002559124957770109, -0.029896492138504982, 0.007326303515583277, -0.004129706881940365, -0.0013569367583841085, 0.013026592321693897, 0.006795077119022608, 0.019604559987783432, -0.012610849924385548, 0.027623767033219337, 0.015059110708534718, 0.0077790007926523685, 0.025055402889847755, -0.013396141119301319, -0.06426452100276947, -0.005880443844944239, 0.008222458884119987, 0.010384319350123405, 0.002355873119086027, 0.0026907767169177532, -7.824039494153112e-05, 0.014384684152901173, -0.012222823686897755, -0.019807811826467514, -0.0020163501612842083, -0.005778818391263485, -2.8798816856578924e-05, -0.002956389682367444, -0.011779365129768848, -0.01273095328360796, 0.007261632476001978, -0.00850885920226574, -0.0024066860787570477, 0.017895396798849106, 0.014606413431465626, -0.0005372891901060939, 0.022062059491872787, 0.021396871656179428, -0.000545661780051887, 0.006356237921863794, -0.007917581126093864, -0.0011721623595803976, -0.019161101430654526, -0.009321866557002068, -0.005048959515988827, 0.002487524878233671, -0.023854369297623634, -0.005460082553327084, 0.013608631677925587, 0.009765325114130974, 0.011492965742945671, 0.012361404486000538, 0.02629339136183262, 0.026921624317765236, -0.01643567904829979, 0.010679958388209343, 0.015086826868355274, -0.002313144039362669, -0.006180702243000269, -0.0106430035084486, 0.002997963922098279, 0.01971542462706566, -0.026755327358841896, 0.03224312514066696, 0.026884669438004494, 0.010495183989405632, 0.0005494150100275874, 0.0004928278503939509, -0.022579427808523178, -0.02208053693175316, 0.01670360192656517, 0.018773075193166733, -0.004032700322568417, 0.024205440655350685, 0.012250539846718311, 0.025406474247574806, 0.009289531037211418, 0.032963745296001434, 0.014754232950508595, -0.01938283070921898, 0.024205440655350685, -0.01871764287352562, -0.011594590730965137, -0.006328521762043238, 0.003697796957567334, 0.004406868480145931, 0.008273271843791008, -0.0010959429200738668, -0.025092357769608498, -0.027716154232621193, -0.0033190094400197268, -0.010439751669764519, 0.005811153445392847, 0.008134691044688225, -0.029323691502213478, -0.014310774393379688, 7.224966248031706e-05, -0.015733536332845688, -0.0044022491201758385, 0.009802279993891716, -0.012084242887794971, -0.001866221078671515, -0.0008667072397656739, -0.0016652789199724793, -0.005894301924854517, -0.008185504004359245, 0.014107522554695606, 0.0009406169992871583, -0.004469229839742184, 0.008347181603312492, -0.020029541105031967, -0.01358091551810503, 0.010060964152216911, 0.0028247381560504436, 0.016906853765249252, 0.013793406076729298, -0.0015867498004809022, 0.016666647046804428, -0.012509224005043507, -0.029138917103409767, -0.021729465574026108, -0.002937912242487073, 0.011169610545039177, 0.031965963542461395, 0.028067225590348244, 0.014310774393379688, 0.01855134591460228, 0.009478924795985222, 0.039209119975566864, -0.0037047259975224733, -0.009234098717570305, -0.005894301924854517, 0.02477824129164219, -0.0054416051134467125, -0.004515423439443111, -0.008661298081278801, -0.021027322858572006, -0.006619541440159082, 0.014403161592781544, 0.015456375665962696, 0.007390974555164576, -0.03909825533628464, 0.011095700785517693, -0.009030846878886223, 0.011003313586115837, -0.00255219591781497, -0.0009325331193394959, -8.004483242984861e-05, -0.010800061747431755, -0.0033166997600346804, -0.014052090235054493, -0.016010697931051254, -0.026108616963028908, -0.012463030405342579, 0.014800426550209522, 0.023725027218461037, -0.02039908990263939, 0.00017625742475502193, 0.0037070356775075197, 0.01196413952857256, 0.017775293439626694, -0.004725604318082333, 0.009820757433772087, -0.021729465574026108, -0.01239835936576128, -0.00864282064139843, -0.003427564399316907, -0.017941590398550034, -0.0029009573627263308, 0.014911291189491749, 0.0007858684402890503, -0.02936064638197422, -0.012121197767555714, 0.0024967635981738567, -0.01736878976225853, -0.001627169200219214, -0.008231697604060173, 0.0045038750395178795, 0.019179578870534897, -0.014560219831764698, -0.013719496317207813, 0.019844766706228256, -0.002503692638128996, 0.0010474396403878927, 0.008716730400919914, 0.013636347837746143, -0.02026974782347679, -0.0166204534471035, 0.01923501119017601, -0.013017353601753712, 0.00043104393989779055, -0.0222468338906765, 0.02427935041487217, -0.02008497342467308, -0.005709527991712093, -0.0024829055182635784, -0.015724297612905502, -0.010569093748927116, -0.01771986111998558, -0.002522170078009367, 0.0047487011179327965, 0.006314663682132959, -0.009169427677989006, 0.01273095328360796, 0.00019054855511058122, -0.007940677925944328, 0.009885428473353386, -0.008254794403910637, 0.0024367119185626507, 0.08433102071285248, 0.009428111836314201, -0.006721167359501123, 0.0018985565984621644, -0.006628780160099268, 3.668420686153695e-05, 0.0007131135789677501, -0.011419055983424187, -0.014273819513618946, -0.007862148806452751, 0.01625090464949608, 0.003524570958688855, 0.004739462397992611, 0.037546150386333466, -0.001579820760525763, -0.01678675040602684, -0.013848838396370411, 0.014384684152901173, -0.02766072191298008, -0.003385990159586072, -0.016232427209615707, 0.01861601695418358, -0.001884698518551886, -0.02446412481367588, 0.011206565424799919, 0.0024043763987720013, 0.01517921406775713, 0.023725027218461037, -0.004272907041013241, -0.00042873425991274416, 0.0066795931197702885, -0.014477071352303028, -0.013562438078224659, 0.008504239842295647, 0.0043191006407141685, 0.000545661780051887, -0.015151497907936573, 0.006541012320667505, -0.007146148476749659, -0.014643368311226368, -0.0019101049983873963, -0.0026076282374560833, -0.0072477743960917, -0.01358091551810503, -0.00905856303870678, -0.005640237592160702, -0.0034783773589879274, 0.02106427773833275, 0.0022865827195346355, -0.016260143369436264, -0.00565409567207098, -0.016897615045309067, 0.007746665272861719, 0.02293049916625023, -0.007012187037616968, 0.004757939837872982, -0.0033998482394963503, -0.018976327031850815, -0.019364353269338608, 0.010365841910243034, -0.013507005758583546, -0.02026974782347679, 0.004998146556317806, 0.004101990722119808, -0.03677009791135788, 0.02614557184278965, -0.016426440328359604, 0.011659261770546436, -0.0071276710368692875, -0.002134143840521574, -0.013673302717506886, 0.013590154238045216, -0.00581577280536294, -0.003894119756296277, 0.011659261770546436, -0.0034414224792271852, 0.011742410250008106, -0.021138187497854233, 0.012851056642830372, -0.004711746238172054, 0.0020128856413066387, 0.0003906245401594788, -0.026570552960038185, 0.004711746238172054, -0.0006016715196892619, -0.0009394621592946351, 0.004928856156766415, -0.027069443836808205, 0.006092934403568506, -0.016860660165548325, 0.014504787512123585, 0.016260143369436264, 0.013978180475533009, -0.006762741599231958, 0.007404832635074854, -0.011077223345637321, 0.01577049121260643, 0.0011040268000215292, -0.0051967790350317955, 0.006300805602222681, -0.001949369558133185, 0.016343291848897934, 0.0019701565615832806, -0.012416836805641651, -0.003554596798494458, -0.005589424632489681, -0.018255706876516342, 0.0019643823616206646, 0.023854369297623634, -0.0074140713550150394, 0.006171463523060083, 0.009344963356852531, -0.0009284911793656647, 0.01595526561141014, 0.004956572316586971, 0.007783620152622461, 0.009178666397929192, -0.018348094075918198, 0.005113630555570126, -0.001984014641493559, 0.0011635010596364737, 0.018874701112508774, -0.0046655526384711266, -0.0136640639975667, 0.004409178160130978, -0.0049103787168860435, -0.010818539187312126, 0.018431242555379868, 0.0042475005611777306, 0.025591248646378517, 0.007866768166422844, -0.006397812161594629, -0.019512172788381577, -0.03250180929899216, 0.01593678817152977, 0.007303206715732813, 0.012666282244026661, -0.003762467997148633, -0.0069844708777964115, 0.009663699194788933, -0.014209148474037647, 0.08130072057247162, -0.012841817922890186, -0.005788056645542383, -0.017405744642019272, -0.008033065125346184, 0.009580550715327263, 0.004099681042134762, 0.012629327364265919, 0.022875066846609116, -0.01958608254790306, 0.00930800847709179, 0.005390792153775692, -0.029138917103409767, -0.002473666798323393, 0.0020579244010150433, 0.015844400972127914, 0.011594590730965137, 0.005145966075360775, 0.029138917103409767, -0.011742410250008106, 0.01139133982360363, 0.0026445831172168255, 0.0157889686524868, 0.011104939505457878, 0.0023397053591907024, -0.002344324719160795, 0.016740556806325912, 0.020491477102041245, 0.009049324318766594, -0.010448990389704704, -0.028288954868912697, 2.6741758119896986e-05, 0.0029887252021580935, 0.005704908631742001, -0.00822707824409008, -0.003362893359735608, -0.01788615807890892, -0.008504239842295647, -0.012259778566658497, 0.003774016397073865, 0.0014354658778756857, 0.0005468166200444102, -0.0008441878599114716, -0.000840723339933902, 0.014172193594276905, -0.007626561913639307, 0.00683203199878335, 0.028640026226639748, 0.0022819633595645428, -0.008434949442744255, -0.0057464828714728355, 0.002831667196005583, -0.0012968850787729025, -0.015049871988594532, -0.0072708711959421635, -0.032280080020427704, -0.02076863870024681, 0.010966358706355095, 0.012583133764564991, 0.007986871525645256, -0.022875066846609116, -0.022838111966848373, -0.02243160828948021]\n"
     ]
    }
   ],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "\n",
    "# response = client.embeddings.create(\n",
    "#     input=\"Your text string goes here\",\n",
    "#     model=\"text-embedding-3-large\"\n",
    "# )\n",
    "\n",
    "# print(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.006929283495992422, -0.005336422007530928, 0.01187589205801487, -0.025001877918839455, -0.02469271421432495, 0.039787933230400085, -0.010101565159857273, -0.00940258800983429, -0.013186474330723286, -0.009940262883901596, -0.011680984869599342, 0.007870214991271496, -0.014087079092860222, 0.007769400719553232, 0.010168774053454399, -0.005033980123698711, 0.02294527180492878, -0.001512210350483656, 0.014933916740119457, -0.01026286743581295, 0.004862596280872822, 0.012460612691938877, 0.004909642972052097, 0.010807262733578682, -0.006603318266570568, -0.00035746971843764186, 0.0055783758871257305, -0.012494217604398727, 0.0163721963763237, 0.004529910162091255, 0.0065629929304122925, -0.007070423103868961, -0.015216195955872536, -0.006589876487851143, -0.01872452348470688, 0.0040796073153615, 0.003212606767192483, -0.018993360921740532, 0.030163554474711418, -0.007594656199216843, 0.008233144879341125, 0.00942275021225214, -0.0010887914104387164, -0.00040955696022138, -0.008831308223307133, -0.0286311823874712, 0.0030176995787769556, 0.009295052848756313, 0.016546940430998802, 0.006475620437413454, 0.019436942413449287, 0.01471884734928608, -0.025754621252417564, -0.009913378395140171, -0.003340304596349597, 0.014907033182680607, -0.03691137209534645, 0.01391233503818512, 0.013408265076577663, 0.004385409876704216, -0.004835712723433971, 0.003226048778742552, -0.01103577483445406, 0.0025892402045428753, -0.014248381368815899, -0.018670756369829178, -0.024759924039244652, -0.0035116884391754866, 0.01803898811340332, 0.008501982316374779, 0.03422299772500992, 0.01520275417715311, 0.014127404429018497, -0.00655627204105258, 0.02204466611146927, 0.02205810695886612, 0.005443957168608904, 0.020875222980976105, 0.008300354704260826, -0.01196326408535242, 0.00755433039739728, -0.025122854858636856, 0.010706448927521706, 0.016788894310593605, 0.009563890285789967, -0.0006414292147383094, -0.0030143391340970993, 0.028953786939382553, -0.0328788124024868, -0.03043239191174507, 0.007688749115914106, 0.020203130319714546, 0.010061238892376423, 0.01799866370856762, -0.005699352826923132, 0.008300354704260826, -0.01020237896591425, 0.020310664549469948, 0.022313503548502922, -0.034706905484199524, 0.005302817560732365, 0.013206636533141136, -0.017407221719622612, -0.008206261321902275, -0.029787182807922363, 0.017729826271533966, 0.021533874794840813, -0.016856104135513306, 0.016318429261446, 0.003804048988968134, -0.029195740818977356, 0.012037194333970547, -0.01809275709092617, -0.01724591851234436, 0.003036182140931487, 0.016869546845555305, 0.010753495618700981, -0.01399298571050167, -0.021520432084798813, -0.03021732158958912, 0.010054518468677998, 0.01276305504143238, 0.03027109056711197, -0.017608849331736565, 0.026413273066282272, 0.01962512917816639, -0.0059043411165475845, -0.03134644031524658, 0.006841911468654871, -0.0019490710692480206, 0.037422165274620056, 0.024464203044772148, 0.010787100531160831, 0.014853266067802906, -0.013146148063242435, 0.02298559620976448, -0.03599732741713524, 0.0024800249375402927, -0.027407972142100334, -0.012816822156310081, 0.013052055612206459, 0.02705848403275013, 0.0028563973028212786, -0.003125234739854932, 0.012621914967894554, 0.010995449498295784, -0.0022582339588552713, -0.010558588430285454, 0.02139945700764656, -0.013193194754421711, -0.0028412751853466034, 0.004029200412333012, 0.004785305354744196, -0.014947358518838882, -0.01063924003392458, 0.01720559224486351, 0.005339782685041428, 0.008044959045946598, 0.001977635081857443, -0.030002253130078316, 0.01100889127701521, 0.009241284802556038, 0.016923313960433006, -0.014624753966927528, 0.033927276730537415, 0.051186639815568924, -0.0038242118898779154, 0.005386829376220703, -0.005877457559108734, -0.008239866234362125, -0.029652763158082962, 0.028577415272593498, -0.06414460390806198, 0.01270256657153368, -0.041374072432518005, 0.0036998745054006577, 0.02619820274412632, 0.013441869057714939, -0.020754246041178703, -0.018993360921740532, -0.03992235288023949, 0.002404414350166917, 0.004973491653800011, 0.025163179263472557, -0.027286995202302933, -0.013925776816904545, 1.3021811355429236e-05, 0.004019118845462799, 0.0007703871233388782, 0.003100031055510044, 0.016667917370796204, 0.02205810695886612, 0.009295052848756313, -0.007944145239889622, -0.689083993434906, -0.0038712583482265472, 0.020713921636343002, -0.02631917968392372, 0.025620203465223312, 0.03844374790787697, 0.000806512194685638, 0.018590105697512627, -0.025606760755181313, 0.020673595368862152, -0.019087454304099083, -0.002446420257911086, -0.010961844585835934, -0.0123597988858819, 0.002923606662079692, -0.01264879945665598, 0.010350239463150501, -0.006606678944081068, -0.014759172685444355, 0.00825330801308155, 0.005064224358648062, 0.0325024388730526, 0.0016928354743868113, 0.01434247475117445, 0.019114337861537933, -9.225532994605601e-05, 0.009920099750161171, -0.01879173330962658, -0.001878501265309751, 0.03193788230419159, -0.02943769469857216, -0.003323502140119672, -0.0025388330686837435, 0.010612355545163155, 0.060972318053245544, -0.0205795019865036, -0.00942275021225214, -0.004970131441950798, -0.005554852541536093, 0.0017692861147224903, -0.01814652420580387, -0.02617131918668747, 0.021090293303132057, -0.0044290958903729916, 0.005833771545439959, -0.004059444647282362, 0.005010456778109074, -0.02119782753288746, 0.005259131547063589, 0.005551491864025593, 0.0019171466119587421, -0.0017877686768770218, 0.010867751203477383, -0.001347547397017479, 0.014866707846522331, 0.00979240145534277, 0.04322905093431473, -0.016627592965960503, 0.001235811854712665, 0.015444708056747913, -0.0038141305558383465, 0.02216564305126667, -0.029141973704099655, -0.013636776246130466, 0.0029051240999251604, 0.009563890285789967, -0.011916217394173145, 0.006663806736469269, 0.009698309004306793, -0.00571951549500227, 0.015767313539981842, 0.013979543931782246, -0.02036443166434765, -0.006092527415603399, 0.002054925775155425, 0.026762761175632477, 0.018549779430031776, -0.009321936406195164, -0.02053917571902275, 0.018200291320681572, 0.006690690293908119, -0.0035620953422039747, -0.023133456707000732, -0.016641033813357353, 0.02529759891331196, -0.006052201613783836, -0.022273177281022072, 0.00942947156727314, 0.006707492750138044, 0.004660968203097582, 0.01316631119698286, 0.010928239673376083, -0.008179377764463425, -0.0013912335271015763, 0.004130014218389988, 0.005837131757289171, -4.670524504035711e-05, 0.01427526492625475, -0.007796284276992083, -0.0037973280996084213, -0.014947358518838882, 0.015901731327176094, -0.028389228507876396, 0.0054170736111700535, 0.007715633139014244, -0.0015962220495566726, -0.003224368439987302, 0.019289081916213036, 0.0330132320523262, -0.02372489869594574, -0.006112690083682537, -0.00855574943125248, -0.002769025042653084, -0.013361218385398388, -0.006694050971418619, -0.02392652817070484, 0.010188937187194824, 0.0054170736111700535, 0.02375178411602974, -0.0002759783819783479, 0.020969316363334656, -0.002737100701779127, -0.003039542818441987, 0.0011618816060945392, 0.026803087443113327, 0.008112167939543724, 0.007621539756655693, -0.02139945700764656, -0.02221941016614437, -0.003968711942434311, 0.007567772641777992, -0.0008796023321337998, 0.05422450229525566, -0.016950197517871857, 0.025216946378350258, -0.002506908727809787, 0.018388478085398674, -0.0008035717764869332, 0.016103358939290047, -0.005363306030631065, -0.020001500844955444, 0.015243079513311386, -0.017555082216858864, 0.006478981114923954, 0.010504821315407753, -0.018361594527959824, -0.018697639927268028, -0.0071577955968678, 0.009523564949631691, -0.010336797684431076, 0.0020482048857957125, -0.012615194544196129, -0.02455829456448555, 0.026708994060754776, 0.012164891697466373, -0.01795833744108677, 0.004771863576024771, -0.015780754387378693, -0.0075812144204974174, -0.02145322412252426, -0.0017860884545370936, 0.011344937607645988, -0.011055937968194485, 0.016224335879087448, -0.010135169140994549, -0.004600479733198881, -0.03785230591893196, 0.013307450339198112, -0.016560383141040802, -0.026440156623721123, -0.00317564164288342, -0.020808013156056404, -0.005820329301059246, -0.00016224756836891174, -0.014073637314140797, -0.0043753283098340034, -0.020014943554997444, -0.0020599665585905313, 0.02301247976720333, -0.016331871971488, 0.008643122389912605, 0.013670381158590317, -0.002733740257099271, 0.015310289338231087, 0.0164528489112854, 0.014288707636296749, -0.002017960650846362, 0.01568666100502014, 0.007796284276992083, -0.017850803211331367, 0.0036998745054006577, -0.013388101942837238, -0.01347547397017479, 0.0004797067085746676, -0.01147263590246439, 0.0003049624210689217, -0.008011354133486748, 0.03035174123942852, 0.017366895452141762, 0.008784261532127857, 0.03371220827102661, 0.006243748124688864, 0.04287956282496452, -0.025754621252417564, -0.004566875286400318, -0.021654851734638214, 0.0024380190297961235, -0.022326944395899773, 0.01809275709092617, 0.012628636322915554, 0.003918305039405823, -0.02122471109032631, -0.012171613052487373, 0.010074680671095848, 0.015310289338231087, 0.025889040902256966, -0.0074131907895207405, 0.007930702529847622, -0.0018532976973801851, 0.0082197031006217, 0.004953328985720873, -0.006744457874447107, 0.004056083969771862, -0.012379962019622326, -0.013421706855297089, 0.005665747914463282, 0.026682110503315926, 0.035701606422662735, 0.005564934108406305, -0.02368457429111004, -0.003136996179819107, -0.0038914212491363287, 0.0008510383777320385, 0.010934961028397083, 0.013845125213265419, 0.017743267118930817, 0.029061321169137955, 0.0008770819986239076, 0.029007554054260254, -0.0035452931188046932, -0.010565309785306454, 0.02303936332464218, 0.018711082637310028, -0.007406469900161028, 0.02450452744960785, 0.004519828595221043, 0.020162804052233696, 0.0015550563111901283, -0.01963857188820839, 0.010034355334937572, -0.011197077110409737, 0.012635357677936554, -0.03172281011939049, -0.0063983299769461155, 0.02137257158756256, -0.013280566781759262, 2.2341857402352616e-05, 0.013287288136780262, 0.02623852901160717, 0.01888582669198513, 0.015861405059695244, -0.01895303651690483, -0.00450974702835083, -0.0006338681560009718, 0.01891271024942398, -0.009973866865038872, -0.014060195535421371, -0.004640805535018444, -0.016694800928235054, 0.014234939590096474, -0.007164516486227512, -0.007130911573767662, 0.03524458035826683, -0.008260028436779976, 0.024276016280055046, -0.00027450817287899554, 0.00948323868215084, -0.0009140471229329705, -0.016869546845555305, 0.004355165641754866, 0.001384512521326542, -0.04433128610253334, 0.007292214315384626, 0.007265330292284489, -0.0020986117888242006, -0.019396618008613586, -0.03368532657623291, 0.03744904696941376, 0.003931746818125248, 0.000920768070500344, 0.0003244951367378235, 0.009153912775218487, 0.020808013156056404, 0.002093571238219738, 0.0007048580446280539, 0.023429177701473236, 0.013361218385398388, -0.02876560017466545, -0.002017960650846362, 0.00163822784088552, 0.008239866234362125, 0.01732656918466091, -0.0039384677074849606, -0.027663366869091988, 0.030781881883740425, -0.00698305107653141, 0.0054473173804581165, 0.021856479346752167, 0.006653725169599056, -0.009994029998779297, 0.006042120512574911, 0.018496012315154076, -0.0038510956801474094, -0.02389964461326599, 0.024074388667941093, 0.0071443538181483746, -0.019396618008613586, -0.004516467917710543, 0.03465313836932182, 0.013703986071050167, -0.0035620953422039747, -0.029034437611699104, -0.019262198358774185, 0.00552124809473753, 0.06285417824983597, 0.02119782753288746, -0.004704654216766357, -0.004069525748491287, 0.007272051181644201, 0.005339782685041428, -0.010740053839981556, -0.033308953046798706, 0.008824586868286133, -0.021695178002119064, -0.001752483774907887, 0.004355165641754866, 0.015404382720589638, 0.006848632358014584, 0.015780754387378693, -0.0033470254857093096, 0.010652681812644005, -0.006862074136734009, -0.0013072218280285597, -0.023509830236434937, -0.009738634340465069, 0.027475181967020035, 0.01275633368641138, 0.0046307239681482315, 0.017662616446614265, 0.0002451040782034397, 0.00471473578363657, 0.01069972850382328, 0.0122388219460845, -0.05145547538995743, -0.020162804052233696, 0.031427089124917984, 0.008038237690925598, 0.009987308643758297, -0.005168398842215538, 0.029088204726576805, -0.005474201403558254, -0.015054893679916859, 0.01877829246222973, -0.0020314024295657873, 0.021587641909718513, 0.019383175298571587, 0.01713838428258896, -0.01564633660018444, 0.00942947156727314, -0.00317732198163867, 0.00203980365768075, -0.0006473100511357188, -0.013898893259465694, -0.003646107157692313, 0.0019893967546522617, -0.008071842603385448, -0.007426633033901453, -0.023348527029156685, 0.008165935985744, 0.009819285944104195, -0.00983272772282362, 0.0062303063459694386, -0.01881861686706543, -0.026534250006079674, 0.007803005166351795, 0.008260028436779976, -0.006663806736469269, -0.0037603629752993584, -0.018280941992998123, -0.01310582272708416, -0.0002640067250467837, 0.00370995607227087, -0.020928990095853806, -0.006109329871833324, 0.002518670167773962, 0.008172656409442425, 0.0013273846125230193, 0.01353596244007349, 0.013495637103915215, 0.004019118845462799, 0.0052154455333948135, -0.015323731116950512, -0.023308200761675835, -0.010753495618700981, -0.0012694165343418717, -0.03602420911192894, 0.0070502604357898235, -0.010162053629755974, -0.023590480908751488, 0.016721686348319054, 0.005480922292917967, 0.01149951945990324, 0.005151596385985613, 0.01187589205801487, 0.024222249165177345, 0.003311740467324853, 0.006314318161457777, -0.016909871250391006, 0.003113473067060113, -0.026776203885674477, 0.0018583384808152914, -0.03454560413956642, -0.010578751564025879, -0.019262198358774185, -0.007406469900161028, -0.0033470254857093096, 0.006055562291294336, -0.006505864672362804, 0.012353078462183475, 0.002557315630838275, 0.008575912564992905, 0.03099695034325123, -0.014624753966927528, 0.019477268680930138, -0.00037574226735159755, 0.00045030261389911175, -0.010330076329410076, 0.0004637445090338588, -0.0082197031006217, 0.01186244934797287, 0.008696889504790306, -0.0002280917251482606, 0.008730494417250156, -0.022743642330169678, 0.02541857585310936, -0.05925175920128822, 0.012225380167365074, 0.02949146181344986, 0.0008670006063766778, 0.014450009912252426, -0.011076100170612335, -0.012796659953892231, 0.0031302752904593945, -0.006136213429272175, -0.009624378755688667, 0.0057430388405919075, -0.018254058435559273, -0.014355916529893875, -0.02967964857816696, -0.00782316830009222, -0.016157127916812897, 0.003867897903546691, -0.01895303651690483, -0.007776121608912945, -0.02384587563574314, 0.002355687553063035, 0.008770819753408432, -7.902349170763046e-05, -0.02225973643362522, -0.0246658306568861, 0.0036965140607208014, 0.018240617588162422, -0.006808307021856308, 0.03427676856517792, 0.0008838028879836202, 0.011365100741386414, -0.013925776816904545, -0.018240617588162422, 0.002209507394582033, -0.02719290181994438, -0.014100520871579647, -0.01967889629304409, 0.00985961128026247, 0.024181922897696495, 0.028577415272593498, -0.004335002973675728, 0.012803380377590656, 0.00431484030559659, -0.015955498442053795, 0.00247330404818058, 0.002611083211377263, -0.011754915118217468, -0.018254058435559273, 0.011170193552970886, 0.0054506780579686165, 0.023577038198709488, 0.015458149835467339, -0.009543727152049541, 0.004187142476439476, -0.014960800297558308, 0.012776496820151806, -0.016654476523399353, -0.013858566991984844, -0.024907784536480904, 0.0030059381388127804, 0.008260028436779976, -0.009342099539935589, 0.012258985079824924, -0.016641033813357353, 0.006727655418217182, 0.046240031719207764, 0.010578751564025879, 0.008717052638530731, 0.02703159861266613, 0.000337516947183758, -0.01797178015112877, 0.01269584521651268, -0.0004330802184995264, 0.015444708056747913, -0.018415361642837524, -0.015028010122478008, -0.012205217033624649, 0.0017676057759672403, 0.030889416113495827, 0.01627810299396515, -0.00447950279340148, -0.013052055612206459, 0.00735942367464304, 0.0011996868997812271, -0.017581965774297714, 0.005968189798295498, -0.003002577694132924, 0.01350907888263464, -0.004909642972052097, -0.03204541653394699, 0.0019339489517733455, -0.008367563597857952, -0.024773364886641502, -0.008495261892676353, 0.016036150977015495, -0.013609892688691616, 0.010840867646038532, -0.024087829515337944, -0.006855353247374296, -0.0021254955790936947, 0.0005028099403716624, 0.029894717037677765, 0.01068628579378128, 0.001020741998218, 0.00861623790115118, -0.00989321619272232, -0.005406992044299841, 0.0075072841718792915, -0.008985890075564384, -0.01186917070299387, 0.03502951189875603, 0.016923313960433006, 0.0029924961272627115, -0.0017096378142014146, 0.01560601033270359, 0.022380713373422623, -0.016157127916812897, -0.019961176440119743, 0.015713544562458992, 0.013623334467411041, 0.0047987475991249084, -0.03933091089129448, -0.01627810299396515, -0.050407011061906815, 0.023348527029156685, 0.0164394062012434, -0.020324107259511948, -0.02787843719124794, 0.0005217125290073454, -0.040594447404146194, 0.02531103976070881, -0.018616989254951477, -0.0031151531729847193, 0.01303861290216446, -0.0024531411472707987, -0.03121202066540718, 0.006643644068390131, -0.0022145479451864958, 0.00018776611250359565, 0.006546190474182367, 0.033954162150621414, -0.013273846358060837, 0.014853266067802906, 0.006189981009811163, 0.01958480291068554, -0.006327759940177202, -0.009725192561745644, 0.003318461589515209, 0.019893966615200043, -0.03750281408429146, -0.008898517116904259, -0.0043753283098340034, -0.012527822516858578, 0.008979168720543385, -0.006243748124688864, -0.01728624477982521, -0.003504967549815774, -0.015458149835467339, -0.011560007929801941, 0.01192965917289257, -0.0033066999167203903, -0.021708618849515915, -0.011687705293297768, 0.004355165641754866, -0.022474804893136024, -0.01313942763954401, -0.01149279810488224, 0.013367938809096813, -0.026668669655919075, -0.024948108941316605, -0.00474161934107542, 0.010854309424757957, 0.005282654892653227, -0.0008581793517805636, -0.006620120722800493, 0.019517594948410988, 0.038148026913404465, -0.007097307126969099, 0.015337172895669937, 0.001865059370175004, 0.006734376773238182, -0.03347025439143181, 0.005874096881598234, 0.0003142037021461874, 0.0006443696329370141, 0.02860429883003235, -0.018616989254951477, -0.026440156623721123, 0.008864913135766983, -0.002995856571942568, 0.019893966615200043, 0.013199916109442711, -0.009160634130239487, -0.008582633920013905, 0.02138601429760456, -0.001399634638801217, -0.00858935434371233, -0.024195365607738495, 0.021654851734638214, 0.008723773062229156, -0.005406992044299841, 0.006129492539912462, -0.0164931733161211, 0.0013962741941213608, -0.010175495408475399, -0.0034343977458775043, 0.009389146231114864, -0.02450452744960785, -0.033981047570705414, -0.0014155969256535172, 0.0011349978158250451, 0.004392130766063929, -0.02623852901160717, -0.019127780571579933, -0.015995824709534645, -0.001298820599913597, -0.007043539546430111, -0.005783364176750183, 0.0012794979847967625, -0.009039657190442085, 0.005272573325783014, -0.011566728353500366, 0.0287924837321043, -0.015915174037218094, -0.018455686047673225, -0.021547317504882812, 0.00012528242950793356, -0.02719290181994438, -0.02548578381538391, -0.009153912775218487, 0.02553955279290676, 0.015135545283555984, -0.009100145660340786, -0.012124566361308098, -0.01303861290216446, -0.028281692415475845, -0.011076100170612335, -0.004123293329030275, 0.0018583384808152914, 0.020834896713495255, 0.00865656416863203, -0.0032697347924113274, 0.0329325795173645, 0.0004139675584156066, 0.027663366869091988, -0.006872155703604221, 0.0005187721690163016, -0.007285493426024914, -0.007944145239889622, 0.00370995607227087, 0.008663284592330456, 0.0008010513847693801, -0.014436568133533001, 0.03863193094730377, 0.014584428630769253, 0.005188561510294676, 0.01713838428258896, 0.0027085368055850267, -0.007897098548710346, -0.013401543721556664, -0.002933687996119261, -0.006260550580918789, 0.01059891376644373, 0.005081026814877987, -0.009416029788553715, -0.013253683224320412, 0.0031571590807288885, -0.006976330187171698, -0.0002444740093778819, 0.014463451690971851, 0.009281611070036888, 0.012588310986757278, 0.009866331703960896, -0.010612355545163155, 0.0018381756963208318, -0.016950197517871857, -0.015847964212298393, 0.02548578381538391, -0.0009216081816703081, -0.025230389088392258, 0.020216571167111397, 0.0008392767049372196, -0.00448286347091198, -0.023348527029156685, 0.030513044446706772, -0.01893959380686283, -0.013173031620681286, -0.007695470470935106, -0.017003964632749557, -0.03360467404127121, 0.001272776979021728, -0.007601377088576555, -0.010249425657093525, -0.003468002425506711, -0.005296096671372652, -0.025552993640303612, -0.025956250727176666, -0.0038107698783278465, 0.00534650357440114, -0.03183034807443619, -0.04519156366586685, -0.013670381158590317, 0.02771713398396969, -0.009994029998779297, -0.009234564378857613, -0.006909120827913284, -0.012870590202510357, 0.0123665202409029, 0.013751032762229443, -0.0004687852051574737, -0.0122455433011055, 0.03105071745812893, 0.016170568764209747, -0.011116426438093185, 0.02705848403275013, 0.23098509013652802, -0.015364056453108788, 0.015350614674389362, 0.03959974646568298, 0.005588456988334656, 0.026668669655919075, 0.002925286768004298, 0.002016280312091112, -0.01483982428908348, 0.003972072619944811, -0.008690168149769306, -0.0017961697885766625, 0.019100897014141083, -0.006008515600115061, 0.025620203465223312, 0.007856772281229496, -0.019208431243896484, -0.014060195535421371, -0.01574042998254299, -0.02946457825601101, 0.008683447726070881, 0.0020314024295657873, -0.0018196930177509785, -0.021762385964393616, 0.006159736774861813, -0.005884178448468447, -0.01356956735253334, -0.01232619397342205, 0.027407972142100334, 0.018993360921740532, -0.007036818657070398, -0.009295052848756313, 0.021547317504882812, 0.004980212543159723, -0.017460988834500313, 0.011452472768723965, 0.00902621541172266, -0.004519828595221043, 0.028523646295070648, 0.02709880843758583, 0.011183635331690311, -0.0005200323066674173, -0.012857148423790932, -0.01955791935324669, -0.008434773422777653, 0.017474429681897163, -0.0023640887811779976, -0.008730494417250156, -0.012124566361308098, -0.004335002973675728, 0.008696889504790306, 0.004207305144518614, 0.011257565580308437, 0.012783218175172806, 0.012836985290050507, -0.0026245249900966883, 0.024867458269000053, -0.011264286935329437, -0.03199164941906929, 0.024343226104974747, -0.0011299571488052607, 0.016560383141040802, -0.004785305354744196, 0.009906657971441746, -0.014221497811377048, -0.0064352951012551785, 0.007339260540902615, -0.004644165746867657, 0.005329701118171215, 0.001123236259445548, 0.0009274890180677176, -0.016909871250391006, -0.0028295135125517845, -0.00738630723208189, -0.030889416113495827, -0.010417448356747627, -0.0019961176440119743, 0.019893966615200043, 0.03183034807443619, 0.018321268260478973, -0.027555832639336586, -0.012803380377590656, 0.010504821315407753, -0.02545890025794506, -0.025122854858636856, -0.029840949922800064, 0.001740722102113068, -0.013616614043712616, -0.010706448927521706, -0.008273470215499401, -0.011022333055734634, -0.01634531281888485, -0.00948323868215084, -0.006126131862401962, 0.002093571238219738, -0.008051679469645023, -0.01708461530506611, 0.028900019824504852, 0.007231725845485926, 0.007399749010801315, -0.029168857261538506, 0.02037787437438965, 0.011956542730331421, 0.01356284599751234, -0.012843706645071507, -0.010861030779778957, -0.012433729134500027, 0.011244123801589012, 0.006989771965891123, -0.015054893679916859, -0.003424316179007292, 0.01104249618947506, 0.012964682653546333, 0.004872677847743034, -0.013690544292330742, 0.006314318161457777, 0.015471591614186764, -0.013831683434545994, 0.003847735235467553, -0.0009778960375115275, -0.009631099179387093, -0.01955791935324669, -0.006028678268194199, -0.0008430572343058884, -0.0070166559889912605, -0.012843706645071507, -0.013529242016375065, -0.01881861686706543, 0.012810101732611656, -0.02445076033473015, 0.004976852331310511, 0.010746774263679981, 0.010793820954859257, -0.003160519525408745, -0.008004633709788322, 0.005094468593597412, 0.0024598620366305113, -0.009093424305319786, -0.011849007569253445, 0.033147651702165604, -0.0012450531357899308, -0.008468377403914928, 0.006865434814244509, 0.00017999502597376704, 0.01186244934797287, -0.03446495160460472, -0.0030345020350068808, -0.010135169140994549, -0.007688749115914106, -0.018469128757715225, -0.0286849495023489, -0.013381380587816238, -0.015901731327176094, -0.0008090325281955302, 0.0204585250467062, -0.0163856390863657, -0.031427089124917984, -0.025969691574573517, 0.013468753546476364, 0.00018913130043074489, -0.02123815380036831, 0.005060863681137562, 0.029303275048732758, -0.01467852108180523, 0.004506386816501617, -0.003746921196579933, -0.17624978721141815, 0.0015231318539008498, 0.007063702214509249, -0.011271007359027863, 0.01892615295946598, -0.013320893049240112, 0.0030798683874309063, 0.001045945449732244, -0.002705176128074527, 0.015995824709534645, -0.0003175641759298742, -0.010827425867319107, -0.04325593635439873, -0.012863868847489357, -0.01641252264380455, 0.016788894310593605, -0.028012854978442192, 0.004150177352130413, 0.01153312437236309, 0.030754996463656425, 0.017635732889175415, -0.005070945248007774, 0.019974617287516594, -0.020149361342191696, 0.001513050403445959, 0.0015332131879404187, 0.009738634340465069, 0.03191099688410759, 0.005037340335547924, -0.009147192351520061, -0.006317678838968277, -0.010430891066789627, -0.00782316830009222, 0.004936526529490948, 0.027381088584661484, -0.011667543090879917, 0.004755061119794846, -0.02705848403275013, -0.014947358518838882, 0.025136295706033707, 0.03379286080598831, 0.03217983618378639, 0.002963932231068611, -0.013011729344725609, 0.004183781798928976, 0.016856104135513306, 0.018509455025196075, -0.010155332274734974, 0.011714588850736618, -0.005793445743620396, 0.012198496609926224, -0.04320216923952103, 0.0015206114621832967, -0.01723247580230236, 0.0029706531204283237, 0.02389964461326599, -0.01151968166232109, -0.012232101522386074, 0.00948996003717184, 0.024235690012574196, 0.003757002530619502, -0.020942432805895805, 0.01646628975868225, 0.01059891376644373, 0.013186474330723286, -0.01725936122238636, -0.009644540958106518, 0.009778959676623344, -0.014651637524366379, 0.018576662987470627, -0.024343226104974747, -0.01794489659368992, 0.003404153510928154, -0.01147263590246439, 0.012789938598871231, -0.010645960457623005, -0.022662991657853127, 0.026628343388438225, -0.002847996074706316, -0.02127848006784916, -0.021614525467157364, 0.03333583474159241, -0.015189312398433685, 0.017743267118930817, 0.012218659743666649, 0.02203122340142727, -0.004459340125322342, 0.0037368396297097206, 0.0026614901144057512, 0.0007271211361512542, 0.0325562059879303, -0.006095887627452612, -0.010128448717296124, -0.014261823147535324, 0.022326944395899773, 0.01639907993376255, 0.015444708056747913, 0.0022363909520208836, -0.0002602261956781149, -0.0005973230581730604, -0.013845125213265419, -0.034088581800460815, -0.011943100951611996, 0.0035352115519344807, 0.019248757511377335, 0.008488540537655354, 0.011277728714048862, 0.0006641123327426612, 0.004328282084316015, -0.01971922256052494, -0.03126578778028488, 0.017420662567019463, 0.019826756790280342, 0.029115088284015656, -0.0005939626134932041, 0.010941681452095509, -0.025956250727176666, -0.021681735292077065, 0.00895900558680296, 0.016977081075310707, 0.03769100084900856, -0.005860655102878809, -0.00551788741722703, 0.014557544142007828, -0.00010013142309617251, -0.012070798315107822, -0.065327487885952, -0.021896805614233017, 0.010975286364555359, 0.0287924837321043, 0.0035452931188046932, 0.01705773174762726, -0.0006225265678949654, 0.005800166632980108, -0.0006011035875417292, -0.002846315735951066, -0.003935107495635748, -0.02949146181344986, -0.025579877197742462, -0.013414985500276089, 0.013737590983510017, -0.008905238471925259, 0.0024985074996948242, -0.0019709141924977303, -0.0009274890180677176, 0.022676434367895126, -0.014826381579041481, -0.01979987323284149, 0.0018667395925149322, -0.018186848610639572, -0.009476518258452415, -0.00011835146142402664, -0.03446495160460472, 0.03521769866347313, 0.016923313960433006, -0.01106265839189291, -0.0076148188672959805, -0.004526549484580755, 0.026735877618193626, -0.019194990396499634, -0.02213875949382782, 0.006949446629732847, -0.04457323998212814, -0.004647526424378157, 0.021722061559557915, -0.027233228087425232, -0.007063702214509249, 0.008320516906678677, -0.001902024494484067, -0.023160340264439583, -0.015847964212298393, -0.0037906072102487087, -0.007977749221026897, 0.0411590039730072, 0.005911062005907297, -0.02041819877922535, -0.030970066785812378, -0.009940262883901596, -0.029088204726576805, -0.020727362483739853, 0.023254433646798134, 0.002884961199015379, 0.02633262239396572, 0.0020734083373099566, -0.024491086602211, -0.0003421375877223909, 0.007164516486227512, -0.01646628975868225, -0.005373387131839991, 0.01725936122238636, 0.01272945012897253, 0.009584052488207817, -0.011801961809396744, 0.004953328985720873, 0.011754915118217468, 0.005265852436423302, -0.008891796693205833, 0.008307075127959251, -0.014194614253938198, 0.02552611008286476, -0.017810476943850517, -0.005927864462137222, -0.02302592247724533, -0.030539928004145622, 0.023335086181759834, 0.0014298788737505674, -0.017044290900230408, -0.020982759073376656, -0.0014323992654681206, -0.019517594948410988, 0.005877457559108734, 0.01959824562072754, -0.005353224463760853, 0.008098726160824299, 0.0035788977984339, -0.02634606324136257, 0.014154288917779922, 0.04416998475790024, 0.00651930645108223, -0.020888665691018105, -0.008112167939543724, -0.00511799193918705, 0.010383844375610352, 0.007043539546430111, 0.011546566151082516, -0.002229670062661171, -0.028980670496821404, -0.023442620411515236, -0.06398329883813858, 0.025915924459695816, 0.008347400464117527, -0.008488540537655354, -0.0082197031006217, -0.008911959826946259, -0.010847589001059532, -0.005309538450092077, -0.014194614253938198, 0.005689271260052919, -0.00026001615333370864, -0.005366666242480278, 0.008925401605665684, -0.0034848046489059925, -0.01813308149576187, 0.006478981114923954, 0.02142634056508541, -0.008690168149769306, 0.024827132001519203, 0.004845793824642897, -0.010303192771971226, -0.010464495047926903, 0.0001873460569186136, -0.0009585733059793711, -0.0061059691943228245, 0.015229637734591961, -0.03916960582137108, 0.018993360921740532, -0.010760216042399406, -0.01510866079479456, 0.010847589001059532, -0.0329594649374485, -0.003847735235467553, 0.015068335458636284, -0.005255770869553089, -0.003163879970088601, -0.007776121608912945, 0.04478830844163895, 0.004570235498249531, -0.0012878990964964032, -0.0030445833690464497, -0.022662991657853127, -0.0036696302704513073, -0.019262198358774185, 0.005931224673986435, 0.0203913152217865, -0.019826756790280342, -0.029652763158082962, 0.023939969018101692, 0.0033352638129144907, 0.006095887627452612, 0.0123597988858819, -0.029787182807922363, -0.026816530153155327, -0.015982381999492645, -0.028469879180192947, 0.024464203044772148, -0.015417824499309063, 0.0016231057234108448, -0.01719215139746666, 0.045487284660339355, -0.008360843174159527, -0.010477936826646328, -0.02373834140598774, 0.012474054470658302, -0.00038015286554582417, -0.01709805801510811, 0.013872009702026844, 0.001019901828840375, -0.011754915118217468, -0.013341055251657963, -0.0025136296171694994, -0.004046002868562937, 0.03621239587664604, 0.015888290479779243, 0.016802337020635605, -0.014463451690971851, 0.013623334467411041, -0.008454935625195503, 0.021869922056794167, 0.009953704662621021, 0.005128073040395975, -0.014154288917779922, 0.002847996074706316, 0.03035174123942852, 0.022447921335697174, -0.009543727152049541, -0.001007300103083253, -0.008004633709788322, 0.0029185658786445856, -0.01958480291068554, 0.013226799666881561, -0.0049230847507715225, 2.8353942980174907e-05, 0.010551867075264454, 0.006657085847109556, 0.0017608848866075277, 0.006015236489474773, 0.014893591403961182, 0.022649550810456276, 0.0044660610146820545, -0.008838029578328133, 0.0016046231612563133, -0.0061496552079916, -0.004862596280872822, -0.00019742746371775866, -0.037341512739658356, -0.03113136999309063, 0.012527822516858578, -0.0022212688345462084, -0.010968565009534359, -0.008360843174159527, 0.034921977669000626, 0.01708461530506611, -0.021829595789313316, 0.02388620190322399, 0.001776007004082203, -0.009785681031644344, -0.025768063962459564, 0.02691062167286873, -0.0011803641682490706, 0.007265330292284489, 0.02693750709295273, -0.01315958984196186, 0.014181172475218773, 0.003387351054698229, 0.00864984281361103, -0.024759924039244652, 0.007608097977936268, 0.0027270193677395582, -0.00942275021225214, 0.0027757459320127964, -0.005074305925518274, -0.019194990396499634, 0.0006611719727516174, 0.008065121248364449, -0.010034355334937572, 0.035594068467617035, -0.006835190579295158, 0.0818341001868248, -0.0019877164158970118, -0.0022800771985203028, 0.00896572694182396, -0.004046002868562937, 0.023173782974481583, 0.008925401605665684, 0.020794572308659554, -0.014006427489221096, 0.0028043100610375404, 0.011586891487240791, 0.003100031055510044, -0.008018075488507748, -0.008381005376577377, -0.0164931733161211, -0.01266896165907383, -0.00571951549500227, 0.016210895031690598, -0.016815777868032455, 0.010034355334937572, 0.018670756369829178, 0.012662241235375404, 0.021023083478212357, 0.015081777237355709, -0.012427008710801601, -0.023133456707000732, 0.02299903891980648, -0.014611312188208103, -0.014624753966927528, -0.029867833480238914, -0.011728030629456043, -0.0015399341937154531, -0.026628343388438225, -0.008139051496982574, 0.010713170282542706, 0.010793820954859257, -0.005097828805446625, -0.0328788124024868, -0.003844374557957053, 0.014691962860524654, 0.003286537015810609, 0.02614443562924862, -0.010155332274734974, -0.0032814964652061462, -0.00034801839501596987, 0.010914797894656658, -0.017017407342791557, -4.547132266452536e-05, -0.024047505110502243]\n"
     ]
    }
   ],
   "source": [
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version = \"2024-02-01\",\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=\"Your text string goes here\",\n",
    "    model=\"ada_gcal\"\n",
    ")\n",
    "\n",
    "print(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text, model=\"ada_gcal\"): # model = \"deployment_name\"\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "# df_bills['ada_v2'] = df_bills[\"text\"].apply(lambda x : generate_embeddings (x, model = 'text-embedding-ada-002'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"./pdf/grokking.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "\n",
      "Towards Understanding Grokking:\n",
      "An Effective Theory of Representation Learning\n",
      "Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric J. Michaud, Max Tegmark, Mike Williams\n",
      "Department of Physics, Institute for AI and Fundamental Interactions, MIT\n",
      "{zmliu,kitouni,nnolte,ericjm,tegmark,mwill}@mit.edu\n",
      "Abstract\n",
      "We aim to understand grokking , a phenomenon where models generalize long after\n",
      "overﬁtting their training set. We present both a microscopic analysis anchored by an\n",
      "effective theory and a macroscopic analysis of phase diagrams describing learning\n",
      "performance across hyperparameters. We ﬁnd that generalization originates from\n",
      "structured representations whose training dynamics and dependence on training set\n",
      "size can be predicted by our effective theory in a toy setting. We observe empirically\n",
      "the presence of four learning phases: comprehension ,grokking ,memorization , and\n",
      "confusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\n",
      "(including comprehension and grokking) between memorization and confusion.\n",
      "We ﬁnd on transformers the grokking phase stays closer to the memorization phase\n",
      "(compared to the comprehension phase), leading to delayed generalization. The\n",
      "Goldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\n",
      "evolution, where resource limitations drive discovery of more efﬁcient solutions.\n",
      "This study not only provides intuitive explanations of the origin of grokking, but\n",
      "also highlights the usefulness of physics-inspired tools, e.g., effective theories and\n",
      "phase diagrams, for understanding deep learning.\n",
      "1 Introduction\n",
      "Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\n",
      "ral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\n",
      "generalization with their discovery of grokking . Grokking refers to the surprising phenomenon of\n",
      "delayed generalization where neural networks, on certain learning problems, generalize long after\n",
      "overﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\n",
      "learning intuitions, raising three key puzzles:\n",
      "Q1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\n",
      "how do models generalize at all?\n",
      "Q2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\n",
      "the training set size decreases toward a critical point?\n",
      "Q3Delayed generalization : Under what conditions does delayed generalization occur?\n",
      "We provide evidence that representation learning is central to answering each of these questions. Our\n",
      "answers can be summarized as follows:\n",
      "A1Generalization can be attributed to learning a good representation of the input embeddings,\n",
      "i.e., a representation that has the appropriate structure for the task and which can be predicted\n",
      "from the theory in Section 3. See Figures 1 and 2.\n",
      "A2The critical training set size corresponds to the least amount of training data that can determine\n",
      "such a representation (which, in some cases, is unique up to linear transformations).\n",
      "36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\n",
      "train acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\n",
      "train acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\n",
      "train acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\n",
      "different training stages of a transformer learning modular addition. We observe that generalization\n",
      "coincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\n",
      "A3Grokking is a phase between “comprehension” and “memorization” phases and it can be\n",
      "remedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\n",
      "This paper is organized as follows: In Section 2, we introduce the problem setting and build a\n",
      "simpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\n",
      "theoretical physics, to shed some light on questions Q1andQ2and show the relationship between\n",
      "generalization and the learning of structured representations. In Section 4, we explain Q3by\n",
      "displaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\n",
      "generalization by following intuition developed from the phase diagram. We discuss related work in\n",
      "Section 5, followed by conclusions in Section 6.1\n",
      "2 Problem Setting\n",
      "Power et al. [ 1] observe grokking on a less common task – learning “algorithmic” binary operations.\n",
      "Given some binary operation ◦, a network is tasked with learning the map (a,b)↦→cwherec=a◦b.\n",
      "They use a decoder-only transformer to predict the second to last token in a tokenized equation of the\n",
      "form “<lhs> <op> <rhs> <eq> <result> <eos>”. Each token is represented as a 256-dimensional\n",
      "embedding vector. The embeddings are learnable and initialized randomly. After the transformer, a\n",
      "ﬁnal linear layer maps the output to class logits for each token.\n",
      "Toy Model We primarily study grokking in a simpler toy model, which still retains the key behaviors\n",
      "from the setup of [ 1]. Although [ 1] treated this as a classiﬁcation task, we study both regression\n",
      "(mean-squared error) and classiﬁcation (cross-entropy). The basic setup is as follows: our model\n",
      "takes as input the symbols a,band maps them to trainable embedding vectors Ea,Eb∈Rdin. It\n",
      "then sums Ea,Eband sends the resulting vector through a “decoder” MLP. The target output vector,\n",
      "denoted Yc∈Rdoutis a ﬁxed random vector (regression task) or a one-hot vector (classiﬁcation\n",
      "task). Our model architecture can therefore be compactly described as (a,b)↦→Dec(Ea+Eb),\n",
      "where the embeddings E∗and the decoder are trainable. Despite its simplicity, this toy model can\n",
      "generalize to all abelian groups (discussed in Appendix B). In sections 3-4.1, we consider only the\n",
      "binary operation of addition. We consider modular addition in Section 4.2 to generalize some of our\n",
      "results to a transformer architecture and study general non-abelian operations in Appendix H.\n",
      "Dataset In our toy setting, we are concerned with learning the addition operation. A data sample\n",
      "corresponding to i+jis denoted as (i,j)for simplicity. If i,j∈{0,...,p−1}, there are in total\n",
      "p(p+ 1)/2different samples since we consider i+jandj+ito be the same sample. A dataset D\n",
      "is a set of non-repeating data samples. We denote the full dataset as D0and split it into a training\n",
      "datasetDand a validation dataset D′, i.e.,D⋃D′=D0,D⋂D′=∅. We deﬁne training data\n",
      "fraction =|D|/|D0|where|·|denotes the cardinality of the set.\n",
      "1Project code can be found at: https://github.com/ejmichaud/grokking-squared\n",
      "2RQI: 0.0 — Accuracy - train: 1.0, validation: 0.1\n",
      "02468101214161820(a) Memorization in toy addition\n",
      "RQI: 0.6 — Accuracy - train: 1.0, validation: 0.9\n",
      "02468101214161820 (b) Generalization in toy addition\n",
      "Accuracy - train: 1.0, validation: 0.1\n",
      "0246810\n",
      "(c) Memorization in toy modular addition\n",
      "Accuracy - train: 1.0, validation: 0.9\n",
      "0246810 (d) Generalization in toy modular addition\n",
      "Figure 2: Visualization of the learned set of embeddings ( p= 11 ) and the decoder function associated\n",
      "with it for the case of 2D embeddings. Axes refer to each dimension of the learned embeddings. The\n",
      "decoder is evaluated on a grid of points in embedding-space and the color at each point represents\n",
      "the highest probability class. For visualization purposes, the decoder is trained on inputs of the form\n",
      "(Ei+Ej)/2. One can read off the output of the decoder when fed the operation i◦jfrom this ﬁgure\n",
      "simply by taking the midpoint between the respective embeddings of iandj.\n",
      "3 Why Generalization Occurs: Representations and Dynamics\n",
      "We can see that generalization appears to be linked to the emergence of highly-structured embeddings\n",
      "in Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\n",
      "circle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\n",
      "notion of representation quality and show that it predicts the model’s performance. We then develop a\n",
      "physics-inspired effective theory of learning which can accurately predict the critical training set size\n",
      "and training trajectories of representations. The concept of an effective theory in physics is similar\n",
      "to model reduction in computational methods in that it aims to describe complex phenomena with\n",
      "simple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\n",
      "learning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\n",
      "which depends only on the representations in embedding space and not on the decoder.\n",
      "3.1 Representation quality predicts generalization for the toy model\n",
      "A rigorous deﬁnition for structure in the learned representation is necessary. We propose the following\n",
      "deﬁnition,\n",
      "Deﬁnition 1. (i,j,m,n )is aδ-parallelogram in the representation R≡[E0,···,Ep−1]if\n",
      "|(Ei+Ej)−(Em+En)|≤δ.\n",
      "In the following derivations, we can take δ, which is a small threshold to tolerate numerical errors, to\n",
      "be zero.\n",
      "Proposition 1. When the training loss is zero, any parallelogram (i,j,m,n )in representation R\n",
      "satisﬁesi+j=m+n.\n",
      "Proof. Suppose that this is not the case, i.e., suppose Ei+Ej=Em+Enbuti+j̸=m+n, then\n",
      "Yi+j= Dec( Ei+Ej) = Dec( Em+En) =Ym+nwhere the ﬁrst and last equalities come from\n",
      "the zero training loss assumption. However, since i+j̸=m+n, we have Yi+j̸=Yn+m(almost\n",
      "surely in the regression task), a contradiction.\n",
      "30.0 0.2 0.4 0.6 0.8 1.0\n",
      "training data fraction0.00.20.40.60.81.0Acc\n",
      "Memorization(a)\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "training set fraction0.00.20.40.60.81.0ˆAcc(b)\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "ˆAcc0.00.20.40.60.81.0Acc(c)Figure 3: We compute accuracy (of the full dataset) either measured empirically Acc, or predicted\n",
      "from the representation of the embeddings ˆAcc. These two accuracies as a function of training data\n",
      "fraction are plotted in (a)(b), and their agreement is shown in (c).\n",
      "It is convenient to deﬁne the permissible parallelogram set associated with a training dataset D\n",
      "(“permissible” means consistent with 100% training accuracy) as\n",
      "P0(D) ={(i,j,m,n )|(i,j)∈D,(m,n)∈D, i+j=m+n}. (1)\n",
      "For simplicity, we denote P0≡P0(D0). Given a representation R, we can check how many\n",
      "permissible parallelograms actually exist in Rwithin error δ, so we deﬁne the parallelogram set\n",
      "corresponding to Ras\n",
      "P(R,δ) ={(i,j,m,n )|(i,j,m,n )∈P0,|(Ei+Ej)−(Em+En)|≤δ}. (2)\n",
      "For brevity we will write P(R), suppressing the dependence on δ. We deﬁne the representation\n",
      "quality index (RQI) as\n",
      "RQI(R) =|P(R)|\n",
      "|P0|∈[0,1]. (3)\n",
      "We will use the term linear representation orlinear structure to refer to a representation whose\n",
      "embeddings are of the form Ek=a+kb(k= 0,···,p−1;a,b∈Rdin). A linear representation\n",
      "hasRQI = 1 , while a random representation (sampled from, say, a normal dstribution) has RQI = 0\n",
      "with high probability.\n",
      "Quantitatively, we denote the “predicted accuracy” ˆAcc as the accuracy achievable on the whole\n",
      "dataset given the representation R(see Appendix D for the full details). In Figure 3, we see that\n",
      "the predictedˆAcc aligns well with the true accuracy Acc, establishing good evidence that structured\n",
      "representation of input embeddings leads to generalization. We use an example to illustrate the origin\n",
      "of generalization here. In the setup of Figure 2 (b), suppose the decoder can achieve zero training\n",
      "loss and E6+E8is a training sample hence Dec(E6+E8) =Y14. At validation time, the decoder\n",
      "is tasked with predicting a validation sample E5+E9. Since (5,9,6,8)forms a parallelogram\n",
      "such that E5+E9=E6+E8, the decoder can predict the validation sample correctly because\n",
      "Dec(E5+E9) = Dec( E6+E8) =Y14.\n",
      "3.2 The dynamics of embedding vectors\n",
      "Suppose that we have an ideal model M∗= (Dec∗,R∗)such that:2\n",
      "• (1)M∗can achieve zero training loss;\n",
      "• (2)M∗has an injective decoder, i.e., Dec∗(x1)̸= Dec∗(x2)for any x1̸=x2.\n",
      "Then Proposition 2 provides a mechanism for the formation of parallelograms.\n",
      "2One can verify a posteriori if a trained model Mis close to being an ideal model M∗. Please refer to\n",
      "Appendix E for details.\n",
      "40.00.20.40.60.81.0\n",
      "training data fraction0.00.20.40.60.81.0Probability(linear structure) rc= 0.4(a) Theory: phase transition\n",
      "0.4 0.6 0.8 1.0\n",
      "training data fraction102103104Steps to RQI >0.95\n",
      "rc= 0.4(b) Empirical: phase transition\n",
      "Runs that reached\n",
      "RQI>0.95 in 104steps\n",
      "Runs that didn’t reach\n",
      "RQI>0.95 in 104steps\n",
      "0 500 1000 1500 2000\n",
      "step−1.5−1.0−0.50.00.51.01.51D representation\n",
      "0123456789\n",
      "3nh(c) Theory: trajectory\n",
      "0 500 1000 1500 2000\n",
      "step−1.5−1.0−0.50.00.51.01.51D normalized representation 0123456789\n",
      "3nh(d) Empirical: trajectoryFigure 4: (a) The effective theory predicts a phase transition in the probability of obtaining a linear\n",
      "representation around rc= 0.4. (b) Empirical results display a phase transition of RQI around\n",
      "rc= 0.4, in agreement with the theory (the blue line shows the median of multiple random seeds).\n",
      "The evolution of 1D representations predicted by the effective theory or obtained from neural network\n",
      "training (shown in (c) and (d) respectively) agree creditably well.\n",
      "Proposition 2. If a training set Dcontains two samples (i,j)and(m,n)withi+j=m+n,\n",
      "thenM∗learns a representation R∗such that Ei+Ej=Em+En, i.e., (i,j,m,n )forms a\n",
      "parallelogram.\n",
      "Proof. Due to the zero training loss assumption, we have Dec∗(Ei+Ej) =Yi+j=Ym+n=\n",
      "Dec∗(Em+En). Then the injectivity of Dec∗implies Ei+Ej=Em+En.\n",
      "The dynamics of the trained embedding vectors are determined by various factors interacting in\n",
      "complex ways, for instance: the details of the decoder architecture, the optimizer hyperparameters,\n",
      "and the various kinds of implicit regularization induced by the training procedure. We will see that\n",
      "the dynamics of normalized quantities, namely, the normalized embeddings at time t, deﬁned as\n",
      "˜E(t)\n",
      "k=E(t)\n",
      "k−µt\n",
      "σt, whereµt=1\n",
      "p∑\n",
      "kE(t)\n",
      "kandσt=1\n",
      "p∑\n",
      "k|E(t)\n",
      "k−µt|2, can be qualitatively described\n",
      "by a simple effective loss (in the physics effective theory sense). We will assume that the normalized\n",
      "embedding vectors obey a gradient ﬂow for an effective loss function of the form\n",
      "d˜Ei\n",
      "dt=−∂ℓeff\n",
      "∂˜Ei, (4)\n",
      "ℓeff=ℓ0\n",
      "Z0, ℓ 0≡∑\n",
      "(i,j,m,n )∈P0(D)|˜Ei+˜Ej−˜Em−˜En|2/|P0(D)|, Z 0≡∑\n",
      "k|˜Ek|2, (5)\n",
      "where|·|denotes Euclidean vector norm. Note that the embeddings do not collapse to the trivial\n",
      "solution E0=···=Ep−1= 0unless initialized as such, because two conserved quantities exist, as\n",
      "proven in Appendix F:\n",
      "C=∑\n",
      "kEk, Z 0=∑\n",
      "k|Ek|2. (6)\n",
      "We shall now use the effective dynamics to explain empirical observations such as the existence of a\n",
      "critical training set size for generalization.\n",
      "Degeneracy of ground states (loss optima) We deﬁne ground states as those representations satis-\n",
      "fyingℓeff= 0, which requires the following linear equations to hold:\n",
      "A(P) ={Ei+Ej=Em+En|(i,j,m,n )∈P}. (7)\n",
      "Since each embedding dimension obeys the same set of linear equations, we will assume, without loss\n",
      "of generality, that din= 1. The dimension of the null space of A(P), denoted as n0, is the number of\n",
      "degrees of freedom of the ground states. Given a set of parallelograms implied by a training dataset\n",
      "D, the nullity of A(P(D))could be obtained by computing the singular values 0≤σ1≤···≤σp.\n",
      "We always have n0≥2, i.e.,σ1=σ2= 0because the nullity of A(P0), the set of linear equations\n",
      "given by all possible parallelograms, is Nullity(A(P0)) = 2 which can be attributed to two degrees\n",
      "5of freedom (translation and scaling). If n0= 2, the representation is unique up to translations and\n",
      "scaling factors, and the embeddings have the form Ek=a+kb. Otherwise, when n0>2, the\n",
      "representation is not constrained enough such that all the embeddings lie on a line.\n",
      "We present theoretical predictions alongside empirical results for addition ( p= 10 ) in Figure 4. As\n",
      "shown in Figure 4 (a), our effective theory predicts that the probability that the training set implies a\n",
      "unique linear structure (which would result in perfect generalization) depends on the training data\n",
      "fraction and has a phase transition around rc= 0.4. Empirical results from training different models\n",
      "are shown in Figure 4 (b). The number of steps to reach RQI>0.95is seen to have a phase transition\n",
      "atrc= 0.4, agreeing with the proposed effective theory and with the empirical ﬁndings in [1].\n",
      "Time towards the linear structure We deﬁne the Hessian matrix of ℓ0as\n",
      "Hij=1\n",
      "Z0∂2ℓ0\n",
      "∂Ei∂Ej, (8)\n",
      "Note thatℓeﬀ=1\n",
      "2RTHR,R= [E0,E1,···,Ep−1], so the gradient descent is linear, i.e.,\n",
      "dR\n",
      "dt=−HR. (9)\n",
      "IfHhas eigenvalues λi=σ2\n",
      "i(sorted in increasing order) and eigenvectors ¯vi, and we have the initial\n",
      "condition R(t= 0) =∑\n",
      "iai¯vi, then we have R(t) =∑\n",
      "iai¯vie−λit. The ﬁrst two eigenvalues\n",
      "vanish andth= 1/λ3determines the timescale for the slowest component to decrease by a factor\n",
      "ofe. We callλ3thegrokking rate . When the step size is η, the corresponding number of steps is\n",
      "nh=th/η= 1/(λ3η).\n",
      "We verify the above analysis with empirical results. Figure 4 (c)(d) show the trajectories obtained\n",
      "from the effective theory and from neural network training, respectively. The 1D neural representation\n",
      "in Figure 4 (d) are manually normalized to zero mean and unit variance. The two trajectories agree\n",
      "qualitatively, and it takes about 3nhsteps for two trajectories to converge to the linear structure. The\n",
      "quantitative differences might be due to the absence of the decoder in the effective theory, which\n",
      "assumes the decoder to take inﬁnitesimal step sizes.\n",
      "Dependence of grokking on data size Note thatℓeﬀinvolves averaging over parallelograms in the\n",
      "training set, it is dependent on training data size, so is λ3. In Figure 5 (a), we plot the dependence of\n",
      "λ3on training data fraction. There are many datasets with the same data size, so λ3is a probabilistic\n",
      "function of data size.\n",
      "Two insights on grokking can be extracted from this plot: (i) When the data fraction is below some\n",
      "threshold (around 0.4), λ3is zero with high probability, corresponding to no generalization. This\n",
      "again veriﬁes our critical point in Figure 4. (ii) When data size is above the threshold, λ3(on average)\n",
      "is an increasing function of data size. This implies that grokking time t∼1/λ3decreases as training\n",
      "data size becomes larger, an important observation from [1].\n",
      "To verify our effective theory, we compare the grokking steps obtained from real neural network\n",
      "training (deﬁned as steps to RQI>0.95), and those predicted by our theory tth∼1\n",
      "λ3η(ηis the\n",
      "embedding learning rate), shown in Figure 5 (b). The theory agrees qualitatively with neural networks,\n",
      "showing the trend of decreasing grokking steps as increasing data size. The quantitative differences\n",
      "might be explained as the gap between our effective loss and actual loss.\n",
      "Limitations of the effective theory While our theory deﬁnes an effective loss based on the Euclidean\n",
      "distance between embeddings Ei+EjandEn+Em, one could imagine generalizing the theory to\n",
      "deﬁne a broader notion of parallogram given by some other metric on the representation space. For\n",
      "instance, if we have a decoder like in Figure 2 (d) then the distance between distinct representations\n",
      "within the same “pizza slice” is low, meaning that representations arranged not in parallelograms\n",
      "w.r.t. the Euclidean metric may be parallelograms with respect to the metric deﬁned by the decoder.\n",
      "4 Delayed Generalization: A Phase Diagramw.r.t. the Euclidean metric may be parallelograms with respect to the metric deﬁned by the decoder.\n",
      "4 Delayed Generalization: A Phase Diagram\n",
      "So far, we have (1) observed empirically that generalization on algorithmic datasets corresponds with\n",
      "the emergence of well-structured representations, (2) deﬁned a notion of representation quality in a\n",
      "toy setting and shown that it predicts generalization, and (3) developed an effective theory to describe\n",
      "60.0 0.2 0.4 0.6 0.8 1.0\n",
      "training data fraction0.00.10.20.30.4grokking rate λ3(a)\n",
      "10−210−1100\n",
      "grokking rate λ3103104Steps to RQI >0.95tth= 1\n",
      "/(2λ3η)\n",
      "Runs that didn’t reach RQI >0.95 in 104steps\n",
      "Runs that reached RQI >0.95 in 104steps (b)\n",
      "Figure 5: Effective theory explains the dependence of grokking time on data size, for the addition\n",
      "task. (a) Dependence of λ3on training data fraction. Above the critical data fraction (around 0.4),\n",
      "as data size becomes larger, λ3increases hence grokking time t∼1/λ3(predicted by our effective\n",
      "theory) decreases. (b) Comparing grokking steps (deﬁned as RQI>0.95) predicted by the effective\n",
      "theory with real neural network results. η= 10−3is the learning rate of the embeddings.\n",
      "the learning dynamics of the representations in the same toy setting. We now study how optimizer\n",
      "hyperparameters affect high-level learning performance. In particular, we develop phase diagrams for\n",
      "how learning performance depends on the representation learning rate, decoder learning rate and the\n",
      "decoder weight decay. These parameters are of interest since they most explicitly regulate a kind of\n",
      "competition between the encoder and decoder, as we elaborate below.\n",
      "4.1 Phase diagram of a toy model\n",
      "Training details We update the representation and the decoder with different optimizers. For the\n",
      "1D embeddings, we use the Adam optimizer with learning rate [10−5,10−2]and zero weight decay.\n",
      "For the decoder, we use an AdamW optimizer with the learning rate in [10−5,10−2]and the weight\n",
      "decay in [0,10](regression) or [0,20](classiﬁcation). For training/validation spliting, we choose\n",
      "45/10 for non-modular addition ( p= 10 ) and 24/12 for the permutation group S3. We hard-code\n",
      "addition or matrix multiplication (details in Appendix H) in the decoder for the addition group and\n",
      "the permutation group, respectively.\n",
      "For each choice of learning rate and weight decay, we compute the number of steps to reach high\n",
      "(90%) training/validation accuracy. The 2D plane is split into four phases: comprehension ,grokking ,\n",
      "memorization andconfusion , deﬁned in Table 1 in Appendix A. Both comprehension and grokking are\n",
      "able to generalize (in the “Goldilocks zone”), although the grokking phase has delayed generalization.\n",
      "Memorization is also called overﬁtting, and confusion means failure to even memorize training data.\n",
      "Figure 6 shows the phase diagrams for the addition group and the permutation group. They display\n",
      "quite rich phenomena.\n",
      "Competition between representation learning and decoder overﬁtting In the regression setup of\n",
      "the addition dataset, we show how the competition between representation learning and decoder\n",
      "learning (which depend on both learning rate and weight decay, among other things) lead to different\n",
      "learning phases in Figure 6 (a). As expected, a fast decoder coupled with slow representation learning\n",
      "(bottom right) lead to memorization. In the opposite extreme, although an extremely slow decoder\n",
      "coupled with fast representation learning (top left) will generalize in the end, the generalization time is\n",
      "long due to the inefﬁcient decoder training. The ideal phase (comprehension) requires representation\n",
      "learning to be faster, but not too much, than the decoder.\n",
      "Drawing from an analogy to physical systems, one can think of embedding vectors as a group of\n",
      "particles. In our effective theory from Section 3.2, the dynamics of the particles are described only\n",
      "by their relative positions, in that sense, structure forms mainly due to inter-particle interactions (in\n",
      "reality, these interactions are mediated by the decoder and the loss). The decoder plays the role of an\n",
      "environment exerting external forces on the embeddings. If the magnitude of the external forces are\n",
      "small/large one can expect better/worse representations.\n",
      "71e-5 1e-4 1e-3 1e-2\n",
      "decoder learning rate1e-2\n",
      "1e-3\n",
      "1e-4\n",
      "1e-5representation learning ratecomprehension\n",
      "memorizationgrokking(a) Addition, regression\n",
      "1e-5 1e-4 1e-3 1e-2\n",
      "decoder learning rate0\n",
      "5\n",
      "10 weight decay\n",
      "comprehensionmemorization\n",
      "grokking\n",
      "confusion (b) Addition, regression\n",
      "1e-5 1e-4 1e-3 1e-2\n",
      "decoder learning rate0\n",
      "10\n",
      "20 weight decaycomprehensionmemorization\n",
      "grokking\n",
      "confusion\n",
      "(c) Addition, classiﬁcation\n",
      "1e-5 1e-4 1e-3 1e-2\n",
      "decoder learning rate0\n",
      "10\n",
      "200 weight decaycomprehensionmemorization\n",
      "grokking\n",
      "confusion (d) Permutation, regression\n",
      "Figure 6: Phase diagrams of learning for the addition group and the permutation group. (a) shows the\n",
      "competition between representation and decoder. (b)(c)(d): each phase diagram contains four phases:\n",
      "comprehension, grokking, memorization and confusion, deﬁned in Table 1. In (b)(c)(d), grokking is\n",
      "sandwiched between comprehension and memorization.\n",
      "Universality of phase diagrams We ﬁx the embedding learning rate to be 10−3and sweep instead\n",
      "decoder weight decay in Figure 6 (b)(c)(d). The phase diagrams correspond to addition regression (b),\n",
      "addition classiﬁcation (c) and permutation regression (d), respectively. Common phenomena emerge\n",
      "from these different tasks: (i) they all include four phases; (ii) The top right corner (a fast and capable\n",
      "decoder) is the memorization phase; (iii) the bottom right corner (a fast and simple decoder) is the\n",
      "confusion phase; (iv) grokking is sandwiched between comprehension and memorization, which\n",
      "seems to imply that it is an undesirable phase that stems from improperly tuned hyperparameters.\n",
      "4.2 Beyond the toy model\n",
      "We conjecture that many of the principles which we saw dictate the training dynamics in the toy\n",
      "model also apply more generally. Below, we will see how our framework generalizes to transformer\n",
      "architectures for the task of addition modulo p, a minimal reproducible example of the original\n",
      "grokking paper [1].\n",
      "We ﬁrst encode p= 53 integers into 256D learnable embeddings, then pass two integers to a decoder-\n",
      "only transformer architecture. For simplicity, we do not encode the operation symbols here. The\n",
      "outputs from the last layer are concatenated and passed to a linear layer for classiﬁcation. Training\n",
      "both the encoder and the decoder with the same optimizer (i.e., with the same hyperparameters)\n",
      "leads to the grokking phenomenon. Generalization appears much earlier once we lower the effective\n",
      "decoder capacity with weight decay (full phase diagram in Figure 7).\n",
      "Early on, the model is able to perfectly ﬁt the training set while having no generalization. We study\n",
      "the embeddings at different training times and ﬁnd that neither PCA (shown in Figure 1) nor t-SNE\n",
      "(not shown here) reveal any structure. Eventually, validation accuracy starts to increase, and perfect\n",
      "generalization coincides with the PCA projecting the embeddings into a circle in 2D. Of course, no\n",
      "8100102104\n",
      "step303540455055eﬀective dimension eS\n",
      "eS\n",
      "generalization\n",
      "memorization\n",
      "0.0 0.2 0.4\n",
      "dropout rate101103105epochs to 90% accuracyvalidation\n",
      "train\n",
      "validation - train\n",
      "3e-7 1e-4 4e-2\n",
      "learning rate1e-2\n",
      "2e-1\n",
      "2e+0\n",
      "3e+1weight decay\n",
      "comprehensionmemorization\n",
      "grokking\n",
      "confusionFigure 7: Left: Evolution of the effective dimension of the embeddings (deﬁned as the exponential of\n",
      "the entropy) during training and evaluated over 100 seeds. Center: Effect of dropout on speeding up\n",
      "generalization. Right: Phase diagram of the transformer architecture. A scan is performed over the\n",
      "weight decay and learning rate of the decoder while the learning rate of the embeddings is kept ﬁxed\n",
      "at10−3(with zero weight decay).\n",
      "choice of dimensionality reduction is guaranteed to ﬁnd any structure, and thus, it is challenging\n",
      "to show explicitly that generalization only occurs when a structure exists. Nevertheless, the fact\n",
      "that, when coupled with the implicit regularization of the optimizer for sparse solutions, such a clear\n",
      "structure appears in a simple PCA so quickly at generalization time suggests that our analysis in\n",
      "the toy setting is applicable here as well. This is also seen in the evolution of the entropy of the\n",
      "explained variance ratio in the PCA of the embeddings (deﬁned as S=−∑\n",
      "iσilogσiwhereσiis\n",
      "the fractional variance explained by the ith principal component). As seen in Figure 7, the entropy\n",
      "increases up to generalization time then decreases drastically afterwards which would be consistent\n",
      "with the conjecture that generalization occurs when a low-dimensional structure is discovered. The\n",
      "decoder then primarily relies on the information in this low-dimensional manifold and essentially\n",
      "“prunes” the rest of the high-dimensional embedding space. Another interesting insight appears when\n",
      "we project the embeddings at initialization onto the principal axes at the end of training. Some of the\n",
      "structure required for generalization exists before training hinting at a connection with the Lottery\n",
      "Ticket Hypothesis. See Appendix K for more details.\n",
      "In Figure 7 (right), we show a comparable phase diagram to Figure 6 evaluated now in the transformer\n",
      "setting. Note that, as opposed to the setting in [ 1], weight decay has only been applied to the decoder\n",
      "and not to the embedding layer. Contrary to the toy model, a certain amount of weight decay proves\n",
      "beneﬁcial to generalization and speeds it up signiﬁcantly. We conjecture that this difference comes\n",
      "from the different embedding dimensions. With a highly over-parameterized setting, a non-zero\n",
      "weight decay gives a crucial incentive to reduce complexity in the decoder and help generalize in\n",
      "fewer steps. This is subject to further investigation. We also explore the effect of dropout layers\n",
      "in the decoder blocks of the transformer. With a signiﬁcant dropout rate, the generalization time\n",
      "can be brought down to under 103steps and the grokking phenomenon vanishes completely. The\n",
      "overall trend suggests that constraining the decoder with the same tools used to avoid overﬁtting\n",
      "reduces generalization time and can avoid the grokking phenomenon. This is also observed in an\n",
      "image classiﬁcation task where we were able to induce grokking. See Appendix J for more details.\n",
      "4.3 Grokking Experiment on MNIST\n",
      "We now demonstrate, for the ﬁrst time, that grokking (signiﬁcantly delayed generalization) is a more\n",
      "general phenomenon in machine learning that can occur not only on algorithmic datasets, but also\n",
      "on mainstream benchmark datasets. In particular, we exhibit grokking on MNIST in Figure 8 and\n",
      "demonstrate that we can control grokking by varying optimization hyperparameters. More details on\n",
      "the experimental setup are in Appendix J.\n",
      "5 Related work\n",
      "Relatively few works have analyzed the phenomenon of grokking. [ 2] describe the circuit that\n",
      "transformers use to perform modular addition, track its formation over training, and broadly suggest\n",
      "that grokking is related to the phenomenon of “phase changes” in neural network training. [ 3,4]\n",
      "9102103104105\n",
      "Optimization Steps0.20.40.60.81.0AccuracyTrain Points: 1000 | Initialization Scale: 9.0\n",
      "train\n",
      "val(a)\n",
      "1.00e-06 2.98e-05 8.86e-04 2.64e-02 7.85e-01\n",
      "Last Layer Learning Rate1.00e-05\n",
      "2.98e-04\n",
      "8.86e-03\n",
      "2.64e-01\n",
      "7.85e+00Weight DecayMemorization\n",
      "Grokking\n",
      "Comprehension\n",
      "Confusion (b)\n",
      "Figure 8: Left: Training curves for a run on MNIST, in the setting where we observe grokking. Right:\n",
      "Phase diagram with the four phases of learning dynamics on MNIST.\n",
      "provided earlier speculative, informal conjectures on grokking [ 3,4]. Our work is related to the\n",
      "following broad research directions:\n",
      "Learning mathematical structures [5] trains a neural network to learn arithmetic operation from\n",
      "pictures of digits, but they do not observe grokking due to their abundant training data. Beyond\n",
      "arithmetic relations, machine learning has been applied to learn other mathematical structures,\n",
      "including geometry [6], knot theory [7] and group theory [8].\n",
      "Double descent Grokking is somewhat reminiscent of the phenomena of “epoch-wise” double\n",
      "descent [9], where generalization can improve after a period of overﬁtting. [ 10] ﬁnd that regularization\n",
      "can mitigate double descent, similar perhaps to how weight decay inﬂuences grokking.\n",
      "Representation learning Representation learning lies at the core of machine learning [ 11–14].\n",
      "Representation quality is usually measured by (perhaps vague) semantic meanings or performance on\n",
      "downstream tasks. In our study, the simplicity of arithmetic datasets allows us to deﬁne representation\n",
      "quality and study evolution of representations in a quantitative way.\n",
      "Physics of learning Physics-inspired tools have proved to be useful in understanding deep learning\n",
      "from a theoretical perspective. These tools include effective theories [ 15,16], conservation laws [ 17]\n",
      "and free energy principle [ 18]. In addition, statistical physics has been identiﬁed as a powerful tool in\n",
      "studying generalization in neural networks [ 19–22]. Our work connects a low-level understanding of\n",
      "models with their high-level performance. In a recent work, researchers at Anthropic [ 23], connect a\n",
      "sudden decrease in loss during training with the emergence of induction heads within their models.\n",
      "They analogize their work to statistical physics , since it bridges a “microscopic”, mechanistic\n",
      "understanding of networks with “macroscopic” facts about overall model performance.\n",
      "6 Conclusion\n",
      "We have shown how, in both toy models and general settings, that representation enables generalization\n",
      "when it reﬂects structure in the data. We developed an effective theory of representation learning\n",
      "dynamics (in a toy setting) which predicts the critical dependence of learning on the training data\n",
      "fraction. We then presented four learning phases (comprehension, grokking, memorization and\n",
      "confusion) which depend on the decoder capacity and learning speed (given by, among other things,\n",
      "learning rate and weight decay) in decoder-only architectures. While we have mostly focused on a\n",
      "toy model, we ﬁnd preliminary evidence that our results generalize to the setting of [1].\n",
      "Our work can be viewed as a step towards a statistical physics of deep learning , connecting the\n",
      "“microphysics” of low-level network dynamics with the “thermodynamics” of high-level model\n",
      "behavior. We view the application of theoretical tools from physics, such as effective theories [ 24], to\n",
      "be a rich area for further work. The broader impact of such work, if successful, could be to make\n",
      "models more transparent and predictable [ 23,25,26], crucial to the task of ensuring the safety of\n",
      "advanced AI systems.\n",
      "10References\n",
      "[1]Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Gen-\n",
      "eralization beyond overﬁtting on small algorithmic datasets. arXiv preprint arXiv:2201.02177 ,\n",
      "2022.\n",
      "[2]Neel Nanda and Tom Lieberum. A mechanistic interpretability analysis of\n",
      "grokking, 2022. URL https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/\n",
      "a-mechanistic-interpretability-analysis-of-grokking .\n",
      "[3]Beren Millidge. Grokking ’grokking’. https://beren.io/\n",
      "2022-01-11-Grokking-Grokking/ , 2022.\n",
      "[4]Rohin Shah. Alignment Newsletter #159. https:\n",
      "//www.alignmentforum.org/posts/zvWqPmQasssaAWkrj/\n",
      "an-159-building-agents-that-know-how-to-experiment-by#DEEP_LEARNING_ ,\n",
      "2021.\n",
      "[5] Yedid Hoshen and Shmuel Peleg. Visual learning of arithmetic operation. In AAAI , 2016.\n",
      "[6]Yang-Hui He. Machine-learning mathematical structures. arXiv preprint arXiv:2101.06317 ,\n",
      "2021.\n",
      "[7]Sergei Gukov, James Halverson, Fabian Ruehle, and Piotr Sułkowski. Learning to unknot.\n",
      "Machine Learning: Science and Technology , 2(2):025035, 2021.\n",
      "[8]Alex Davies, Petar Veli ˇckovi ´c, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomašev,\n",
      "Richard Tanburn, Peter Battaglia, Charles Blundell, András Juhász, et al. Advancing mathemat-\n",
      "ics by guiding human intuition with ai. Nature , 600(7887):70–74, 2021.\n",
      "[9]Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.\n",
      "Deep double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics:\n",
      "Theory and Experiment , 2021(12):124003, 2021.\n",
      "[10] Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal regularization can\n",
      "mitigate double descent. arXiv preprint arXiv:2003.01897 , 2020.\n",
      "[11] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and\n",
      "new perspectives. IEEE transactions on pattern analysis and machine intelligence , 35(8):\n",
      "1798–1828, 2013.\n",
      "[12] Yassine Ouali, Céline Hudelot, and Myriam Tami. An overview of deep semi-supervised\n",
      "learning. arXiv preprint arXiv:2006.05278 , 2020.\n",
      "[13] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena\n",
      "Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\n",
      "et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural\n",
      "Information Processing Systems , 33:21271–21284, 2020.\n",
      "[14] Phuc H Le-Khac, Graham Healy, and Alan F Smeaton. Contrastive representation learning: A\n",
      "framework and review. IEEE Access , 8:193907–193934, 2020.\n",
      "[15] James Halverson, Anindita Maiti, and Keegan Stoner. Neural networks and quantum ﬁeld\n",
      "theory. Machine Learning: Science and Technology , 2(3):035002, 2021.\n",
      "[16] Daniel A Roberts, Sho Yaida, and Boris Hanin. The principles of deep learning theory. arXiv\n",
      "preprint arXiv:2106.10165 , 2021.\n",
      "[17] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka.\n",
      "Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. arXiv\n",
      "preprint arXiv:2012.04728 , 2020.\n",
      "[18] Yansong Gao and Pratik Chaudhari. A free-energy principle for representation learning. In\n",
      "International Conference on Machine Learning , pages 3367–3376. PMLR, 2020.\n",
      "11[19] Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mézard, and Lenka Zdeborová.\n",
      "Generalisation error in learning with random features and the hidden manifold model. In\n",
      "International Conference on Machine Learning , pages 3452–3462. PMLR, 2020.\n",
      "[20] Mohammad Pezeshki, Amartya Mitra, Yoshua Bengio, and Guillaume Lajoie. Multi-scale\n",
      "feature learning dynamics: Insights for double descent. In International Conference on Machine\n",
      "Learning , pages 17669–17690. PMLR, 2022.\n",
      "[21] Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc Mezard, and Lenka\n",
      "Zdeborova. The gaussian equivalence of generative models for learning with shallow neural\n",
      "networks. In Joan Bruna, Jan Hesthaven, and Lenka Zdeborova, editors, Proceedings of the\n",
      "2nd Mathematical and Scientiﬁc Machine Learning Conference , volume 145 of Proceedings\n",
      "of Machine Learning Research , pages 426–471. PMLR, 16–19 Aug 2022. URL https:\n",
      "//proceedings.mlr.press/v145/goldt22a.html .\n",
      "[22] R Kuhn and S Bos. Statistical mechanics for neural networks with continuous-time dynamics.\n",
      "Journal of Physics A: Mathematical and General , 26(4):831, 1993.\n",
      "[23] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom\n",
      "Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain,\n",
      "Deep Ganguli, Zac Hatﬁeld-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson\n",
      "Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,\n",
      "Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer\n",
      "Circuits Thread , 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-\n",
      "heads/index.html.\n",
      "[24] Daniel A. Roberts, Sho Yaida, and Boris Hanin. The Principles of Deep Learning Theory .\n",
      "Cambridge University Press, 2022. https://deeplearningtheory.com .\n",
      "[25] Deep Ganguli, Danny Hernandez, Liane Lovitt, Nova DasSarma, Tom Henighan, Andy Jones,\n",
      "Nicholas Joseph, Jackson Kernion, Ben Mann, Amanda Askell, et al. Predictability and surprise\n",
      "in large generative models. arXiv preprint arXiv:2202.07785 , 2022.\n",
      "[26] Jacob Steinhardt. Future ML Systems Will Be Qualitatively Different. https://www.\n",
      "lesswrong.com/s/4aARF2ZoBpFZAhbbe/p/pZaPhGg2hmmPwByHc , 2022.\n",
      "[27] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov,\n",
      "and Alexander J Smola. Deep sets. Advances in neural information processing systems , 30,\n",
      "2017.\n",
      "[28] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the\n",
      "terminal phase of deep learning training. Proceedings of the National Academy of Sciences , 117\n",
      "(40):24652–24663, 2020.\n",
      "[29] Wikipedia contributors. Thomson problem — Wikipedia, the free encyclope-\n",
      "dia. https://en.wikipedia.org/w/index.php?title=Thomson_problem&oldid=\n",
      "1091431454 , 2022. [Online; accessed 29-July-2022].\n",
      "[30] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings\n",
      "of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15750–15758,\n",
      "2021.\n",
      "[31] Zhi-Qin John Xu, Yaoyu Zhang, and Yanyang Xiao. Training behavior of deep neural network\n",
      "in frequency domain. In International Conference on Neural Information Processing , pages\n",
      "264–274. Springer, 2019.\n",
      "[32] Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, and Zheng Ma. A type of generalization error induced\n",
      "by initialization in deep neural networks. In Mathematical and Scientiﬁc Machine Learning ,\n",
      "pages 144–164. PMLR, 2020.\n",
      "[33] Ziming Liu, Eric J. Michaud, and Max Tegmark. Omnigrok: Grokking beyond algorithmic data,\n",
      "2022.\n",
      "12[34] Blake Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay\n",
      "Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models.\n",
      "In Jacob Abernethy and Shivani Agarwal, editors, Proceedings of Thirty Third Conference on\n",
      "Learning Theory , volume 125 of Proceedings of Machine Learning Research , pages 3635–3673.\n",
      "PMLR, 09–12 Jul 2020. URL https://proceedings.mlr.press/v125/woodworth20a.\n",
      "html .\n",
      "Checklist\n",
      "1. For all authors...\n",
      "(a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s\n",
      "contributions and scope? [Yes]\n",
      "(b) Did you describe the limitations of your work? [Yes]\n",
      "(c) Did you discuss any potential negative societal impacts of your work? [N/A]\n",
      "(d)Have you read the ethics review guidelines and ensured that your paper conforms to\n",
      "them? [Yes]\n",
      "2. If you are including theoretical results...\n",
      "(a) Did you state the full set of assumptions of all theoretical results? [Yes]\n",
      "(b) Did you include complete proofs of all theoretical results? [Yes]\n",
      "3. If you ran experiments...\n",
      "(a) Did you include the code, data, and instructions needed to reproduce the main experi-\n",
      "mental results (either in the supplemental material or as a URL)? [Yes]\n",
      "(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they\n",
      "were chosen)? [Yes]\n",
      "(c)Did you report error bars (e.g., with respect to the random seed after running experi-\n",
      "ments multiple times)? [Yes]\n",
      "(d)Did you include the total amount of compute and the type of resources used (e.g., type\n",
      "of GPUs, internal cluster, or cloud provider)? [Yes] All experiments were run on a\n",
      "workstation with two NVIDIA A6000 GPUs within a few days.\n",
      "4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n",
      "(a) If your work uses existing assets, did you cite the creators? [N/A]\n",
      "(b) Did you mention the license of the assets? [N/A]\n",
      "(c)Did you include any new assets either in the supplemental material or as a URL? [N/A]\n",
      "(d)Did you discuss whether and how consent was obtained from people whose data you’re\n",
      "using/curating? [N/A]\n",
      "(e)Did you discuss whether the data you are using/curating contains personally identiﬁable\n",
      "information or offensive content? [N/A]\n",
      "5. If you used crowdsourcing or conducted research with human subjects...\n",
      "(a)Did you include the full text of instructions given to participants and screenshots, if\n",
      "applicable? [N/A]\n",
      "(b)Did you describe any potential participant risks, with links to Institutional Review\n",
      "Board (IRB) approvals, if applicable? [N/A]\n",
      "(c)Did you include the estimated hourly wage paid to participants and the total amount\n",
      "spent on participant compensation? [N/A]\n",
      "13Appendix\n",
      "A Deﬁnitions of the phases of learning\n",
      "Table 1: Deﬁnitions of the four phases of learning\n",
      "criteria\n",
      "Phasetraining acc > 90%\n",
      "within 105stepsvalidation acc > 90%\n",
      "within 105stepsstep(validation acc>90%)\n",
      "−step(training acc>90%)< 103\n",
      "Comprehension Yes Yes Yes\n",
      "Grokking Yes Yes No\n",
      "Memorization Yes No Not Applicable\n",
      "Confusion No No Not Applicable\n",
      "B Applicability of our toy setting\n",
      "In the main paper, we focused on the toy setting with (1) the addition dataset and (2) the addition\n",
      "operation hard coded in the decoder. Although both simpliﬁcations appear to have quite limited\n",
      "applicability, we argue below that the analysis of the toy setting can actually apply to all Abelian\n",
      "groups.\n",
      "The addition dataset is the building block of all Abelian groups A cyclic group is a group that\n",
      "is generated by a single element. A ﬁnite cyclic group with order nisCn={e,g,g2,···,gn−1}\n",
      "whereeis the identify element and gis the generator and gi=gjwheneveri=j(modn). The\n",
      "modulo addition and {0,1,···,n−1}form a cyclic group with e= 0andgcan be any number q\n",
      "coprime tonsuch that (q,n) = 1 . Since algorithmic datasets contain only symbolic but no arithmetic\n",
      "information, the datasets of modulo addition could apply to all other cyclic groups, e.g., modulo\n",
      "multiplication and discrete rotation groups in 2D.\n",
      "Although not all Abelian groups are cyclic, a ﬁnite Abelian group Gcan be always decomposed into\n",
      "a direct product of kcyclic groups G=Cn1×Cn2···Cnk. So after training kneural networks with\n",
      "each handling one cyclic group separately, it is easy to construct a larger neural network that handles\n",
      "the whole Abelian group.\n",
      "The addition operation is valid for all Abelian groups It is proved in [ 27] that for a permutation\n",
      "invariant function f(x1,x2,···,xn), there exists ρandφsuch that\n",
      "f(x1,x2,···,xn) =ρ[n∑\n",
      "i=1φ(xi)], (10)\n",
      "orf(x1,x2) =ρ(φ(x1) +φ(x2))forn= 2. Notice that φ(xi)corresponds to the embedding vector\n",
      "Ei,ρcorresponds to the decoder. The addition operator naturally emerges from the commutativity\n",
      "of the operator, not restricting the operator itself to be addition. For example, multiplication of\n",
      "two numbers x1andx2can be written as x1x2= exp(ln(x1) + ln(x2))whereρ(x) = exp(x)and\n",
      "φ(x) = ln(x).\n",
      "C An illustrative example\n",
      "We use a concrete case to illustrate why parallelograms lead to generalization (see Figure 9). For the\n",
      "purpose of illustration, we exploit a curriculum learning setting, where a neural network is fed with a\n",
      "few new samples each time. We will illustrate that, as we have more samples in the training set, the\n",
      "ideal model M∗(deﬁned in Section 3.2) will arrange the representation R∗in a more structured way,\n",
      "i.e., more parallelograms are formed, which helps generalization to unseen validation samples. For\n",
      "simplicity we choose p= 6.\n",
      "•D1= (0,4)andD2= (1,3)have the same label, so (0,4,1,3)becomes a parallelogram\n",
      "such that E0+E4=E1+E3→E3−E0=E4−E1.D3= (1,5)andD4= (2,4)have\n",
      "14the same label, so (1,5,2,4)becomes a parallelogram such that E1+E5=E2+E4→\n",
      "E4−E1=E5−E2. We can derive from the ﬁrst two equations that E5−E2=E3−E0→\n",
      "E0+E5=E2+E3, which implies that (0,5,2,3)is also a parallelogram (see Figure 9(a)).\n",
      "This means if (0,5)in training set, our model can predict (2,3)correctly.\n",
      "•D5= (0,2)andD6= (1,1)have the same label, so E0+E2= 2E1, i.e., 1is the middle\n",
      "point of 0and2(see Figure 9(b)). Now we can derive that 2E4=E3+E5, i.e., 4is the\n",
      "middle point of 3and5. If(4,4)is in the training data, our model can predict (3,5)correctly.\n",
      "• Finally,D7= (2,4)andD8= (3,3)have the same label, so 2E3=E2+E4, i.e., 3should\n",
      "be placed at the middle point of 2and4, ending up Figure 9(c). This linear structure agrees\n",
      "with the arithmetic structure of R.\n",
      "In summary, although we have p(p+ 1)/2 = 21 different training samples for p= 6, we only need 8\n",
      "training samples to uniquely determine the perfect linear structure (up to linear transformation). The\n",
      "punchline is: representations lead to generalization.\n",
      "Figure 9: As we include more data in the training set, the (ideal) model is capable of discovering\n",
      "increasingly structured representations (better RQI), from (a) to (b) to (c).\n",
      "D Deﬁnition of ˆAcc\n",
      "Given a training set Dand a representation R, if(i,j)is a validation sample, can the neural network\n",
      "correctly predict its output, i.e., Dec(Ei+Ej) =Yi+j? Since neural network has never seen (i,j)\n",
      "in the training set, one possible mechanism of induction is through\n",
      "Dec(Ei+Ej) = Dec( Em+En) =Ym+n(=Yi+j). (11)\n",
      "The ﬁrst equality Dec(Ei+Ej) = Dec( Em+En)holds only when Ei+Ej=Em+En(i.e.,\n",
      "(i,j,m,n )is a parallelogram). The second equality Dec(Em+En) =Ym+n, holds when (m,n)\n",
      "is in the training set, i.e., (m,n)∈D, under the zero training loss assumption. Rigorously, given a\n",
      "training setDand a parallelogram set P(which can be calculated from R), we collect all zero loss\n",
      "samples in an augmented training setD\n",
      "D(D,P) =D⋃\n",
      "{(i,j)|∃(m,n)∈D,(i,j,m,n )∈P}. (12)\n",
      "KeepingDﬁxed, a larger Pwould probably produce a larger D, i.e., ifP1⊆P2, thenD(D,P 1)⊆\n",
      "D(P,P 2), which is why in Eq. (3) our deﬁned RQI∝|P|gets its name “representation quality\n",
      "index\", because higher RQI normally means better generalization. Finally, the expected accuracy\n",
      "from a dataset Dand a parallelogram set Pis:\n",
      "ˆAcc =|D(D,P)|\n",
      "|D0|, (13)\n",
      "which is the estimated accuracy (of the full dataset), and P=P(R)is deﬁned on the representation\n",
      "after training. On the other hand, accuracy Acc can be accessed empirically from trained neural\n",
      "network. We veriﬁed Acc≈ˆAcc in a toy setup (addition dataset p= 10 , 1D embedding space,\n",
      "hard code addition), as shown in Figure 3 (c). Figure 3 (a)(b) show Acc andˆAcc as a function of\n",
      "training set ratio, with each dot corresponding to a different random seed. The dashed red diagonal\n",
      "corresponds to memorization of the training set, and the vertical gap refers to generalization.\n",
      "15Although the agreement is good for 1D embedding vectors, we do not expect such agreement can\n",
      "trivially extend to high dimensional embedding vectors. In high dimensions, our deﬁnition of RQI is\n",
      "too restrictive. For example, suppose we have an embedding space with Ndimensions. Although the\n",
      "representation may form a linear structure in the ﬁrst dimension, the representation can be arbitrary\n",
      "in otherN−1dimensions, leading to RQI≈0. However, the model may still generalize well if the\n",
      "decoder learns to keep only the useful dimension and drop all other N−1useless dimensions. It\n",
      "would be interesting to investigate how to deﬁne an RQI that takes into account the role of decoder in\n",
      "future works.\n",
      "E The gap of a realistic model Mand the ideal model M∗\n",
      "Realistic models Musually form fewer number of parallelograms than ideal models M∗. In this\n",
      "section, we analyze the properties of ideal models and calculated ideal RQI and ideal accuracy,\n",
      "which set upper bounds for empirical RQI and accuracy. The upper bound relations are veriﬁed via\n",
      "numerical experiments in Figure 10.\n",
      "Similar to Eq. (12) where some validation samples can be derived from training samples, we\n",
      "demonstrate how implicit parallelograms can be ‘derived’ from explicit ones in P0(D). The so-called\n",
      "derivation follows a simple geometric argument that: if A1B1is equal and parallel to A2B2, and\n",
      "A2B2is equal and parallel to A3B3, then we can deduce that A1B1is equal and parallel to A3B3\n",
      "(hence (A1,B2,A2,B1)is a parallelogram).\n",
      "Recall that a parallelogram (i,j,m,n )is equivalent to Ei+Ej=Em+En(∗). So we are\n",
      "equivalently asking if equation (∗)can be expressed as a linear combination of equations in\n",
      "A(P0(D)). If yes, then (∗)is dependent on A(P0(D))(deﬁned in Eq. (7)), i.e., A(P0(D))and\n",
      "A(P0(D)⋃(i,j,m,n ))should have the same rank. We augment P0(D)by adding implicit parallel-\n",
      "ograms, and denote the augmented parallelogram set as\n",
      "P(D) =P0(D)⋃\n",
      "{q≡(i,j,m,n )|q∈P0,rank(A(P0(D))) = rank(A(P0(D)⋃\n",
      "q))}.(14)\n",
      "We need to emphasize that an assumption behind Eq. (14) is that we have an ideal model M∗. When\n",
      "the model is not ideal, e.g., when the injectivity of the encoder breaks down, fewer parallelograms\n",
      "are expected to form, i.e.,\n",
      "P(R)⊆P(D). (15)\n",
      "The inequality is saying, whenever a parallelogram is formed in the representation after training, the\n",
      "reason is hidden in the training set. This is not a strict argument, but rather a belief that today’s neural\n",
      "networks can only copy what datasets (explicitly or implicitly) tell it to do, without any autonomous\n",
      "creativity or intelligence. For simplicity we call this belief Alexander Principle . In very rare cases\n",
      "when something lucky happens (e.g., neural networks are initialized at approximate correct weights),\n",
      "Alexander principle may be violated. Alexander principle sets an upper bound for RQI:\n",
      "RQI(R)≤|P(D)|\n",
      "|P0|≡RQI, (16)\n",
      "and sets an upper bound for ˆAcc:\n",
      "ˆAcc≡ˆAcc(D,P(R))≤ˆAcc(D,P(D))≡Acc. (17)\n",
      "In Figure 10 (c)(f), we verify Eq. (16) and Eq. (17). We choose δ= 0.01to compute RQI( R,δ). We\n",
      "ﬁnd the trained models are usually far from being ideal, although we already include a few useful\n",
      "tricks proposed in Section 4 to enhance representation learning. It would be an interesting future\n",
      "direction to develop better algorithms so that the gap due to Alexander principle can be reduced\n",
      "or even closed. In Figure 10 (a)(b)(d)(e), four quantities (RQI, RQI, Acc, Acc) as functions of the\n",
      "training data fraction are shown, each dot corresponding to one random seed. It is interesting to\n",
      "note that it is possible to have RQI = 1 only with<40% training data, i.e., 55×0.4 = 22 samples,\n",
      "agreeing with our observation in Section 3.\n",
      "Realistic representations Suppose an ideal model M∗and a realistic model Mwhich train on the\n",
      "training setDgive the representation R∗andR, respectively. What is the relationship between R\n",
      "andR∗? Due to the Alexander principle we know P(R)⊆P(D) =P(R∗). This means R∗has\n",
      "more parallelograms than R, henceR∗has fewer degrees of freedom than R.\n",
      "160.0 0.2 0.4 0.6 0.8 1.0training data fraction0.00.20.40.60.81.0RQI(a)\n",
      "0.0 0.2 0.4 0.6 0.8 1.0training data fraction0.00.20.40.60.81.0RQI(b)\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "RQI0.00.20.40.60.81.0RQIIdeal AlgorithmAlexander\n",
      "Principle(c)\n",
      "0.0 0.2 0.4 0.6 0.8 1.0training data fraction0.00.20.40.60.81.0Acc\n",
      "Memorization(d)\n",
      "0.0 0.2 0.4 0.6 0.8 1.0training data fraction0.00.20.40.60.81.0Acc\n",
      "Memorization(e)\n",
      "0.0 0.2 0.4 0.6 0.8 1.0Acc0.00.20.40.60.81.0AccIdeal AlgorithmAlexander\n",
      "Principle(f)Figure 10: We compare RQI and Acc for an ideal algorithm (with bar) and a realistic algorithm\n",
      "(without bar). In (a)(b)(d)(e), four quantities (RQI, RQI, Acc, Acc) as functions of training data\n",
      "fraction are shown. In (c)(f), RQI and Acc of the ideal algorithm sets upper bounds for those of the\n",
      "realistic algorithm.\n",
      "We illustrate with the toy case p= 4. The whole dataset contains p(p+ 1)/2 = 10 samples, i.e.,\n",
      "D0={(0,0),(0,1),(0,2),(0,3),(1,1),(1,2),(1,3),(2,2),(2,3),(3,3)}. (18)\n",
      "The parallelogram set contains only three elements, i.e.,\n",
      "P0={(0,1,1,2),(0,1,2,3),(1,2,2,3)}, (19)\n",
      "Or equivalently the equation set\n",
      "A0={A1 :E0+E2= 2E1,A2 :E0+E3=E1+E2,A3 :E1+E3= 2E2}. (20)\n",
      "Pictorially, we can split all possible subsets {A|A⊆A0}into different levels, each level deﬁned by\n",
      "|A|(the number of elements). A subset A1in theithlevel points an direct arrow to another subset\n",
      "A2in the (i+ 1)thlevel ifA2⊂A1, and we say A2is a child of A1, andA1is a parent of A2.\n",
      "Each subset Acan determine a representation Rwithn(A)degrees of freedom. So Rshould be a\n",
      "descendant of R∗, andn(R∗)≤n(R). Numerically, n(A)is equal to the dimension of the null space\n",
      "ofA.\n",
      "Suppose we have a training set\n",
      "D={(0,2),(1,1),(0,3),(1,2),(1,3),(2,2)}, (21)\n",
      "and correspondingly P(D) =P0,A(P) =A0. So an ideal model M∗will have the linear structure\n",
      "Ek=a+kb(see Figure 11 leftmost). However, a realistic model Mmay produce any descendants\n",
      "of the linear structure, depending on various hyperparameters and even random seeds.\n",
      "In Figure 12, we show our algorithms actually generates all possible representations. We have\n",
      "two settings: (1) fast decoder (η1,η2) = (10−3,10−2)(Figure 12 left), and (2) relatively slow\n",
      "decoder (η1,η2) = (10−2,10−3)(Figure 12) right). The relatively slow decoder produces better\n",
      "representations (in the sense of higher RQI) than a fast decoder, agreeing with our observation in\n",
      "Section 4.\n",
      "17Figure 11:p= 4 case. Equation set A(or geometrically, representation) has a hierarchy: a→b\n",
      "meansais a parent of b, andbis a child ofa. A realistic model can only generate representations that\n",
      "are descendants of the representation generated by an ideal model.\n",
      "Figure 12:p= 4case. Representations obtained from training neural networks are displayed. η1and\n",
      "η2are learning rates of the representation and the decoder, respectively. As described in the main text,\n",
      "(η1,η2) = (10−2,10−3)(right) is more ideal than (η1,η2) = (10−3,10−2)(left), thus producing\n",
      "representations containing more parallelograms.\n",
      "F Conservation laws of the effective theory\n",
      "Recall that the effective loss function\n",
      "ℓeff=ℓ0\n",
      "Z0, ℓ 0≡∑\n",
      "(i,j,m,n )∈P0(D)|Ei+Ej−Em−En|2/|P0(D)|, Z 0≡∑\n",
      "k|Ek|2(22)\n",
      "whereℓ0andZ0are both quadratic functions of R={E0,···,Ep−1}, andℓeff= 0remains zero\n",
      "under rescaling and translation E′\n",
      "i=aEi+b. We will ignore the 1/|P0(D)|factor inℓ0since\n",
      "having it is equivalent to rescaing time, which does not affect conservation laws. The representation\n",
      "vector Eievolves according to the gradient descent\n",
      "dEi\n",
      "dt=−∂ℓeff\n",
      "∂Ei. (23)\n",
      "We will prove the following two quantities are conserved:\n",
      "C=∑\n",
      "kEk, Z 0=∑\n",
      "k|Ek|2. (24)\n",
      "Eq. (22) and Eq. (23) give\n",
      "dEi\n",
      "dt=−ℓeff\n",
      "∂Ei=−∂(ℓ0\n",
      "Z0)\n",
      "∂Ei=−1\n",
      "Z0∂ℓ0\n",
      "∂Ei+ℓ0\n",
      "Z2\n",
      "0∂Z0\n",
      "∂Ei. (25)\n",
      "18Then\n",
      "dZ0\n",
      "dt= 2∑\n",
      "iEk·dEk\n",
      "dt(26)\n",
      "=2\n",
      "Z2\n",
      "0∑\n",
      "iEi·(−Z0∂ℓ0\n",
      "∂Ek+ 2ℓ0Ek)\n",
      "=2\n",
      "Z0(−∑\n",
      "k∂ℓ0\n",
      "∂Ek·Ek+ 2ℓ0)\n",
      "= 0.\n",
      "where the last equation uses the fact that\n",
      "∑\n",
      "k∂ℓ0\n",
      "∂Ek·Ek= 2∑\n",
      "k∑\n",
      "(i,j,m,n )∈P0(D)(Ei+Ej−Em−En)(δik+δjk−δmk−δnk)·Ek\n",
      "= 2∑\n",
      "(i,j,m,n )∈P0(D)(Ei+Ej−Em−En)∑\n",
      "k(δik+δjk−δmk−δnk)·Ek\n",
      "=∑\n",
      "(i,j,m,n )∈P0(D)(Ei+Ej−Em−En)·(Ei+Ej−Em−En)\n",
      "= 2ℓ0\n",
      "The conservation of Z0prohibits the representation from collapsing to zero. Now that we have\n",
      "demonstrated that Z0is a conserved quantity, we can also show\n",
      "dC\n",
      "dt=∑\n",
      "kdEk\n",
      "dt(27)\n",
      "=−1\n",
      "Z0∑\n",
      "k∂ℓ0\n",
      "∂Ek\n",
      "=−2\n",
      "Z0∑\n",
      "k∑\n",
      "(i,j,m,n )∈P0(D)(Ei+Ej−Em−En)(δik+δjk−δmk−δnk)\n",
      "=0.\n",
      "The last equality holds because the two summations can be swapped and∑\n",
      "k(δik+δjk−δmk−δnk) =\n",
      "0.\n",
      "G More phase diagrams of the toy setup\n",
      "We study another three hyperparameters in the toy setup by showing phase diagrams similar to\n",
      "Figure 6. The toy setup is: (1) addition without modulo ( p= 10 ); (2) training/validation is split into\n",
      "45/10; (3) hard code addition; (4) 1D embedding. In the following experiments, the decoder is an\n",
      "MLP with size 1-200-200-30. The representation and the encoder are optimized with AdamW with\n",
      "different hyperparameters. The learning rate of the representation is 10−3. We sweep the learning\n",
      "rate of the decoder in range [10−4,10−2]as the x axis, and sweep another hyperparameter as the\n",
      "y axis. By default, we use full batch size 45, initialization scale s= 1 and zero weight decay of\n",
      "representation.\n",
      "Batch size controls the amount of noise in the training dynamics. In Figure 13, the grokking region\n",
      "appears at the top left of the phase diagram (small decoder learning rate and small batch size).\n",
      "However, large batch size (with small learning rate) leads to comprehension, implying that smaller\n",
      "batch size seems harmful. This makes sense since to get crystals (good structures) in experiments,\n",
      "one needs a freezer which gradually decreases temperature, rather than something perturbing the\n",
      "system with noise.\n",
      "Initialization scale controls distances among embedding vectors at initialization. We initialize\n",
      "components of embedding vectors from independent uniform distribution U[−s/2,s/2]wheres\n",
      "191e-4 1e-3 1e-2\n",
      "learning rate1\n",
      "12\n",
      "45 batch size\n",
      "comprehensionmemorizationgrokkingconfusionAddition group (regression)\n",
      "1e-4 1e-3 1e-2\n",
      "learning rate1\n",
      "12\n",
      "45 batch size\n",
      "comprehensionmemorizationgrokkingconfusionAddition group (classiﬁcation)Figure 13: Phase diagrams of decoder learning rate (x axis) and batch size (y axis) for the addition\n",
      "group (left: regression; right: classiﬁcation). Small decoder leanrning rate and large batch size\n",
      "(bottom left) lead to comprehension.\n",
      "1e-4 1e-3 1e-2\n",
      "learning rate0.01\n",
      "1.0\n",
      "100.0 initialization scalecomprehension\n",
      "memorization\n",
      "confusionAddition group (regression)\n",
      "1e-4 1e-3 1e-2\n",
      "learning rate0.01\n",
      "1.0\n",
      "100.0 initialization scalecomprehension\n",
      "memorization\n",
      "confusionAddition group (classiﬁcation)\n",
      "Figure 14: Phase diagrams of decoder learning rate (x axis) and initialization (y axis) for the addition\n",
      "group (left: regression; right: classiﬁcation). Small intialization scale (top) leads to comprehension.\n",
      "is called the initialization scale. Shown in Figure 14, it is beneﬁcial to use a smaller initialization\n",
      "scale. This agrees with the physical intuition that closer particles are more likely to interact and form\n",
      "structures. For example, the distances among molecules in ice are much smaller than distances in gas.\n",
      "Representation weight decay controls the magnitude of embedding vectors. Shown in Figure 15,\n",
      "we see the representation weight decay in general does not affect model performance much.\n",
      "H General groups\n",
      "H.1 Theory\n",
      "We focused on Abelian groups for the most part of the paper. This is, however, simply due to\n",
      "pedagogical reasons. In this section, we show that it is straight-forward to extend deﬁnitions of\n",
      "parallelograms and representation quality index (RQI) to general non-Abelian groups. We will also\n",
      "show that most (if not all) qualitative results for the addition group also apply to the permutation\n",
      "group.\n",
      "201e-4 1e-3 1e-2\n",
      "learning rate0.0\n",
      "5.0\n",
      "10.0weight decay (representation)comprehension\n",
      "memorizationgrokkingAddition group (regression)\n",
      "1e-4 1e-3 1e-2\n",
      "learning rate0.0\n",
      "5.0\n",
      "10.0weight decay (representation)\n",
      "comprehension\n",
      "memorizationgrokking\n",
      "confusionAddition group (classiﬁcation)Figure 15: Phase diagrams of decoder learning rate (x axis) and representation weight decay (y axis)\n",
      "for the addition group (left: regression; right: classiﬁcation). Representation weight decay does not\n",
      "affect model performance much.\n",
      "Matrix representation for general groups Let us ﬁrst review the deﬁnition of group representation.\n",
      "A representation of a group Gon a vector space Vis a group homomorphism from GtoGL(V), the\n",
      "general linear group on V. That is, a representation is a map ρ:G→GL(V)such that\n",
      "ρ(g1g2) =ρ(g1)ρ(g2),∀g1,g2∈G. (28)\n",
      "In the caseVis of ﬁnite dimension n, it is common to identify GL(V)withnbyninvertible matrices.\n",
      "The punchline is that: each group element can be represented as a matrix, and the binary operation is\n",
      "represented as matrix multiplication.\n",
      "A new architecture for general groups Inspired by the matrix representation, we embed each group\n",
      "elementaas a learnable matrix Ea∈Rd×d(as opposed to a vector), and manually do matrix\n",
      "multiplication before sending the product to the deocder for regression or classiﬁcation. More\n",
      "concretly, for a◦b=c, our architecture takes as input two embedding matrices EaandEband\n",
      "aims to predict Ycsuch that Yc= Dec( EaEb), where EaEbmeans the matrix multiplication of Ea\n",
      "andEb. The goal of this simplication is to disentangle learning the representation and learning the\n",
      "arithmetic operation (i.e, the matrix multiplication). We will show that, even with this simpliﬁcation,\n",
      "we are still able to reproduce the characteristic grokking behavior and other rich phenomenon.\n",
      "Generalized parallelograms we deﬁne generalized parallelograms: (a,b,c,d )is a generalized\n",
      "parallelogram in the representation if ||EaEb−EcEd||2\n",
      "F≤δ, whereδ>0is a threshold to tolerate\n",
      "numerical errors. Before presenting the numerical results for the permutation group, we show an\n",
      "intuitive picture about how new parallelograms can be deduced from old ones for general groups,\n",
      "which is the key to generalization.\n",
      "Deduction of parallelograms We ﬁrst recall the case of the Abelian group (e.g., addition group). As\n",
      "shown in Figure 16, when (a,d,b,c )and(c,f,d,e )are two parallelograms, we have\n",
      "Ea+Ed=Eb+Ec,\n",
      "Ec+Ef=Ed+Ed.(29)\n",
      "We can derive that Ea+Ef=Eb+Eeimplying that (a,f,b,e )is also a parallelogram. That is, for\n",
      "Abelian groups, two parallelograms are needed to deduce a new parallelogram.\n",
      "For the non-Abelian group, if we have only two parallelograms such that\n",
      "EaEd=EbEc,\n",
      "EfEc=EeEd,(30)\n",
      "we have E−1\n",
      "bEa=EcE−1\n",
      "d=E−1\n",
      "fEe, but this does not lead to something like EfEa=EeEb,\n",
      "hence useless for generalization. However, if we have a third parallelogram such that\n",
      "EeEh=EfEg (31)\n",
      "21Figure 16: Deduction of parallelograms\n",
      "we have E−1\n",
      "bEa=EcE−1\n",
      "d=E−1\n",
      "fEe=EgE−1\n",
      "h, equivalent to EaEh=EbEg, thus establishing a\n",
      "new parallelogram (a,h,b,g ). That is, for non-Abelian groups, three parallelograms are needed to\n",
      "deduce a new parallelogram.\n",
      "H.2 Numerical Results\n",
      "In this section, we conduct numerical experiments on a simple non-abelian group: the permutation\n",
      "groupS3. The group has 6 group elements, hence the full dataset contains 36 samples. We embed each\n",
      "group element ainto a learnable 3×3embedding matrix Ea. We adopt the new architecture described\n",
      "in the above subsection: we hard code matrix multiplication of two input embedding matrices before\n",
      "feeding to the decoder. After deﬁning the generalized parallelogram in the last subsection, we can\n",
      "continue to deﬁne RQI (as in Section 3) and predict accuracy ˆAcc from representation (as in appendix\n",
      "D). We also compute the number of steps needed to reach RQI = 0.95.\n",
      "Representation We ﬂatten each embedding matrix into a vector, and apply principal component\n",
      "analysis (PCA) to the vectors. We show the ﬁrst three principal components of these group elements\n",
      "in Figure 17. On the plane of PC1 andPC3 , the six points are organized as a hexagon.\n",
      "PC 13\n",
      "2\n",
      "1\n",
      "0123PC 2\n",
      "1.5\n",
      "1.0\n",
      "0.5\n",
      "0.00.51.01.5PC 3\n",
      "1.5\n",
      "1.0\n",
      "0.5\n",
      "0.00.51.01.5\n",
      "[0, 1, 2]\n",
      "[0, 2, 1][1, 0, 2]\n",
      "[1, 2, 0][2, 0, 1]\n",
      "[2, 1, 0]\n",
      "3\n",
      " 2\n",
      " 1\n",
      " 0 1 2 3\n",
      "PC11.5\n",
      "1.0\n",
      "0.5\n",
      "0.00.51.01.5PC2\n",
      "[0, 1, 2] [0, 2, 1]\n",
      "[1, 0, 2][1, 2, 0][2, 0, 1]\n",
      "[2, 1, 0]\n",
      "3\n",
      " 2\n",
      " 1\n",
      " 0 1 2 3\n",
      "PC11.5\n",
      "1.0\n",
      "0.5\n",
      "0.00.51.01.5PC3[0, 1, 2]\n",
      "[0, 2, 1][1, 0, 2]\n",
      "[1, 2, 0][2, 0, 1]\n",
      "[2, 1, 0]\n",
      "1.5\n",
      " 1.0\n",
      " 0.5\n",
      " 0.0 0.5 1.0 1.5\n",
      "PC21.5\n",
      "1.0\n",
      "0.5\n",
      "0.00.51.01.5PC3[0, 1, 2]\n",
      "[0, 2, 1][1, 0, 2]\n",
      "[1, 2, 0][2, 0, 1]\n",
      "[2, 1, 0]\n",
      "Figure 17: Permuation group S3. First three principal components of six embedding matrices R3×3.\n",
      "RQI In Figure 18 (a), we show RQI as a function of training data fraction. For each training data\n",
      "fraction, we run 11 random seeds (shown as scatter points), and the blue line corresponds to the\n",
      "highest RQI.\n",
      "Steps to reach RQI = 0.95In Figure 18 (b), we whow the steps to reach RQI>0.95as a function\n",
      "of training data fraction, and ﬁnd a phase transition at r=rc= 0.5. The blue line corresponds to the\n",
      "best model (smallest number of steps).\n",
      "220.4 0.6 0.8\n",
      "training data fraction0.00.20.40.60.81.0RQI(a)\n",
      "0.4 0.6 0.8\n",
      "training data fraction103104steps to RQI >0.95\n",
      "rc= 0.5(b)\n",
      "0.2 0.4 0.6 0.8 1.0\n",
      "training data fraction0.20.30.40.50.60.70.80.91.0Acc(c)\n",
      "0.2 0.4 0.6 0.8 1.0\n",
      "training data fraction0.20.30.40.50.60.70.80.91.0ˆAcc(d)\n",
      "0.2 0.4 0.6 0.8 1.0\n",
      "ˆAcc0.20.30.40.50.60.70.80.91.0Acc(e)Figure 18: Permutation group S3. (a) RQI increases as training set becomes larger. Each scatter point\n",
      "is a random seed, and the blue line is the highest RQI obtained with a ﬁxed training set ratio; (b)\n",
      "steps to reach RQI>0.95. The blue line is the smallest number of steps required. There is a phase\n",
      "transition around rc= 0.5. (c) real accuracy Acc; (d) predicted accuracy ˆAcc; (e) comparison of Acc\n",
      "andˆAcc:ˆAcc serves as a lower bound of Acc.\n",
      "Accuracy The real accuracy Acc is shown in Figure 18 (c), while the predicted accuracy ˆAcc\n",
      "(calculated from RQI) is shown in Figure 18 (d). Their comparison is shown in (e): ˆAccis a lower\n",
      "bound of Acc, implying that there must be some generalization mechanism beyond RQI.\n",
      "Phase diagram We investigate how the model performance varies under the change of two knobs:\n",
      "decoder learning rate and decoder weight decay. We calculate the number of steps to training accuracy\n",
      "≥0.9and validation accuracy ≥0.9, respectively, shown in Figure 6 (d).\n",
      "I Effective theory for image classiﬁcation\n",
      "In this section, we show our effective theory proposed in Section 3.2 can generalize beyond algorith-\n",
      "mic datasets. In particular, we will apply the effective theory to image classiﬁcations. We ﬁnd that:\n",
      "(i) The effective theory naturally gives rise to a novel self-supervised learning method, which can\n",
      "provably avoid mode collapse without contrastive pairs. (ii) The effective theory can shed light on the\n",
      "neural collapse phenomenon [ 28], in which same-class representations collapse to their class-means.\n",
      "We ﬁrst describe how the effective theory applies to image classiﬁcation. The basic idea is again that,\n",
      "similar to algorithmic datasets, neural networks try to develop a structured representation of the inputs\n",
      "based on the relational information between samples (class labels in the case of image classiﬁcation,\n",
      "sum parallelograms in the case of addition, etc.). The effective theory has two ingredients: (i) samples\n",
      "with the same label are encouraged to have similar representations; (ii) the effective loss function\n",
      "is scale-invariant to avoid all representations collapsing to zero (global collapse). As a result, an\n",
      "effective loss for image classiﬁcation has the form\n",
      "ℓeﬀ=ℓ\n",
      "Z, ℓ =∑\n",
      "(x,y)∈P|f(x)−f(y)|2, Z =∑\n",
      "x|f(x)|2(32)\n",
      "where xis an image, f(x)is its representation, (x,y)∈Prefers to unique pairs xandythat have\n",
      "the same label. Scale invariance means the loss function ℓeﬀdoes not change under the linear scaling\n",
      "f(x)→af(x).\n",
      "Relation to neural collapse It was observed in [ 28] that image representations in the penultimate\n",
      "layer of the model have some interesting features: (i) representations of same-class images collapse\n",
      "to their class-means; (ii) class-means of different classes develop into an equiangular tight frame. Our\n",
      "effective theory is able to predict the same-class collapse, but does not necessarily put class-means\n",
      "into equiangular tight frames. We conjecture that little explicit repulsion among different classes can\n",
      "help class-means develop into an equiangular tight frame, similar to electrons developing into lattice\n",
      "structures on a sphere under repulsive Coulomb forces (the Thomson problem [ 29]). We would like\n",
      "to investigate this modiﬁcation of the effective theory in the future.\n",
      "Experiment on MNIST We directly apply the effective loss Eq. (32) to the MNIST dataset. Firstly,\n",
      "each image xis randomly encoded to a 2D embedding f(x)via the same encoder MLP whose weights\n",
      "are randomly initialized. We then train these embeddings by minimizing the effective loss ℓeﬀwith\n",
      "23Figure 19: Our effective theory applies to MNIST image classiﬁcations. Same-class images collapse\n",
      "to their class-means, while class-means of different classes stay separable. As such, the effective\n",
      "theory serves as a novel self-supervised learning method, as well as shed some light on neural collapse.\n",
      "Please see texts in Appendix I.\n",
      "an Adam optimizer ( 10−3learning rate) for 100 steps. We show the evolution of these embeddings in\n",
      "Figure 19. Images of the same class collapse to their class-means, and different class-means do not\n",
      "collapse. This means that our effective theory can give rise to a good representation learning method\n",
      "which only exploits non-contrastive relational information in datasets.\n",
      "Link to self-supervised learning Note thatℓitself is vulnerable to global collapse, in the context\n",
      "of Siamese learning without contrastive pairs. Various tricks (e.g., decoder with momentum, stop\n",
      "gradient) [ 13,30] have been proposed to avoid global collapse. However, the reasons why these\n",
      "tricks can avoid global collapse are unclear. We argue ℓfails simply because ℓ→a2ℓunder scaling\n",
      "f(x)→af(x)so gradient descent on ℓencouragea→0. Based on this picture, our effective theory\n",
      "provides another possible ﬁx: make the loss function ℓscale-invariant (by the normalized loss ℓeﬀ),\n",
      "so the gradient ﬂow has no incentive to change representation scales. In fact, we can prove that\n",
      "24the gradient ﬂow on ℓeﬀpreserveZ(variance of representations) so that global collapse is avoided\n",
      "provably:\n",
      "∂ℓeﬀ\n",
      "∂f(x)=1\n",
      "Z∂ℓ\n",
      "∂f(x)−l\n",
      "Z2∂Z\n",
      "∂f(x)=2\n",
      "Z∑\n",
      "y∼x(f(x)−f(y))−2ℓ\n",
      "Z2f(x),\n",
      "dZ\n",
      "dt= 2∑\n",
      "xf(x)·df(x)\n",
      "dt= 2∑\n",
      "xf(x)·∂ℓeﬀ\n",
      "∂f(x)\n",
      "=4\n",
      "Z∑\n",
      "xf(x)·(∑\n",
      "y∼x(f(x)−f(y))−ℓ\n",
      "Zf(x))\n",
      "=4\n",
      "Z[∑\n",
      "xf(x)·∑\n",
      "y∼x(f(x)−f(y))−∑\n",
      "xℓ\n",
      "Z|f(x)|2]\n",
      "= 0.(33)\n",
      "where we use the fact that\n",
      "∑\n",
      "xf(x)·∑\n",
      "y∼x(f(x)−f(y)) =∑\n",
      "(x,y)∈P(f(x)−f(y))·(f(x)−f(y)) =ℓ (34)\n",
      "J Grokking on MNIST\n",
      "To induce grokking on MNIST, we make two nonstandard decisions: (1) we reduce the size of the\n",
      "training set from 50k to 1k samples (by taking a random subset) and (2) we increase the scale of the\n",
      "weight initialization distribution (by multiplying the initial weights, sampled with Kaiming uniform\n",
      "initialization, by a constant >1).\n",
      "The choice of large initializations is justiﬁed by [ 31–33] which ﬁnd large initializations overﬁt data\n",
      "easily but prone to poor generalization. Relevant to this, initialization scale is found to regulate\n",
      "“kernel” vs “rich” learning regimes in networks [34].\n",
      "With these modiﬁcations to training set size and initialization scale, we train a depth-3 width-200\n",
      "MLP with ReLU activations with the AdamW optimizer. We use MSE loss with one-hot targets,\n",
      "rather than cross-entropy. With this setup, we ﬁnd that the network quickly ﬁts the train set, and then\n",
      "much later in training validation accuracy improves, as shown in Figure 8a. This closely follows the\n",
      "stereotypical grokking learning, ﬁrst observed in algorithmic datasets.\n",
      "With this setup, we also compute a phase diagram over the model weight decay and the last layer\n",
      "learning rate. See Figure 8b. While in MLPs it is less clear what parts of the network to consider\n",
      "the “encoder” vs the “decoder”, for our purposes here we consider the last layer to be the “decoder”\n",
      "and vary its learning rate relative to the rest of the network. The resulting phase diagram has some\n",
      "similarity to Figure 7. We observe a “confusion”phase in the bottom right (high learning rate and\n",
      "high weight decay), a “comprehension” phase bordering it, a “grokking” phase as one decreases\n",
      "weight decay and decoder learning rate, and a “memorization“ phase at low weight decay and low\n",
      "learning rate. Instead of an accuracy threshold of 95%, we use a threshold of 60% here for validation\n",
      "accuracy for runs to count as comprehension or grokking. This phase diagram demonstrates that with\n",
      "sufﬁcient regularization, we can again “de-grok” learning.\n",
      "We also investigate the effect of training set size on time to generalization on MNIST. We ﬁnd a result\n",
      "similar to what Power et al. [ 1] observed, namely that generalization time increases rapidly once one\n",
      "drops below a certain amount of training data. See Figure 20.\n",
      "K Lottery Ticket Hypothesis Connection\n",
      "In Figure 21, we show the projection of the learned embeddings after generalization to their ﬁrst\n",
      "two principal components. Compared to the projection at initialization, structure clearly emerges in\n",
      "embedding space when the neural network is able to generalize ( >99% validation accuracy). What\n",
      "is intriguing is that the projection of the embeddings at initialization to the principal components\n",
      "of the embeddings at generalization seem to already contain much of that structure. In this sense,\n",
      "250 5000 10000 15000 20000 25000 30000\n",
      "Train Points104105Steps to Validation Accuracy > 60%Steps until generalization for MNIST (weight decay 5e-3)\n",
      "Mean\n",
      "Runs that didn't reach 60% val acc in 10^5 steps\n",
      "Runs that reached 60% val acc in 10^5 stepsFigure 20: Time to generalize as a function of training set size, on MNIST.\n",
      "−4−2 0 2 4\n",
      "Generalization PCA 1−4−2024Generalization PCA 2After generalization\n",
      "−5.0−2.5 0.0 2.5 5.0 7.5\n",
      "Initialization PCA 1−6−4−20246Initialization PCA 2At initialization\n",
      "−4−2 0 2 4\n",
      "Generalization PCA 1−3−2−101234Generalization PCA 2At initialization\n",
      "Figure 21: (Left) Input embeddings after generalization projected on their ﬁrst 2 principal compo-\n",
      "nents. (Center) Input embeddings at initialization projected on their ﬁrst 2 principal components.\n",
      "(Right) Input embeddings at initialization projected on the ﬁrst 2 principal components of the\n",
      "embeddings after generalization at the end of training (same PCA as the left ﬁgure).\n",
      "the structured representation necessary for generalization already existed (partially) at initialization.\n",
      "The training procedure essentially prunes other unnecessary dimensions and forms the required\n",
      "parallelograms for generalization. This is a nonstandard interpretation of the lottery ticket hypothesis\n",
      "where the winning tickets are not weights or subnetworks but instead particular axes or linear\n",
      "combinations of the weights (the learned embeddings).\n",
      "In Figure 22, we show the original training curves (dashed lines). In solid lines, we recompute\n",
      "accuracy with models which use embeddings that are projected onto the nprincipal components of\n",
      "the embeddings at the end of training (and back). Clearly, the ﬁrst few principal components contain\n",
      "enough information to reach 99% accuracy. The ﬁrst few PCs explain the most variance by deﬁnition,\n",
      "however, we note that this is not necessarily the main reason for why they can generalize so well. In\n",
      "fact, embeddings reconstructed from the PCA at the end of training (solid lines) perform better than\n",
      "current highest variance axes (dotted line). This behavior is consistent across seeds.\n",
      "26102103104\n",
      "Epoch0.00.20.40.60.81.0Accuracy10 Components\n",
      "Test Reconstructed (Final PCA)\n",
      "Train Reconstructed (Final PCA)\n",
      "Train Reconstructed (Current PCA)\n",
      "Test Reconstructed (Current PCA)\n",
      "Test\n",
      "Train\n",
      "102103104\n",
      "Epoch0.00.20.40.60.81.0Accuracy6 Components\n",
      "Test Reconstructed (Final PCA)\n",
      "Train Reconstructed (Final PCA)\n",
      "Train Reconstructed (Current PCA)\n",
      "Test Reconstructed (Current PCA)\n",
      "Test\n",
      "Train\n",
      "102103104\n",
      "Epoch0.00.20.40.60.81.0Accuracy5 Components\n",
      "Test Reconstructed (Final PCA)\n",
      "Train Reconstructed (Final PCA)\n",
      "Train Reconstructed (Current PCA)\n",
      "Test Reconstructed (Current PCA)\n",
      "Test\n",
      "Train\n",
      "102103104\n",
      "Epoch0.00.20.40.60.81.0Accuracy2 Components\n",
      "Test Reconstructed (Final PCA)\n",
      "Train Reconstructed (Final PCA)\n",
      "Train Reconstructed (Current PCA)\n",
      "Test Reconstructed (Current PCA)\n",
      "Test\n",
      "TrainFigure 22: Train and test accuracy computed while using actual embeddings (dashed line) and\n",
      "embeddings projected onto and reconstructed from their ﬁrst nprincipal components (dotted lines)\n",
      "and, ﬁnally, using embeddings projected onto and reconstructed from the ﬁrst nPCs of the embeddings\n",
      "at the end of training (solid lines).\n",
      "27L Derivation of the effective loss\n",
      "In this section, we will further motivate the use of our effective loss to study the dynamics of\n",
      "representation learning by deriving it from the gradient ﬂow dynamics on the actual MSE loss in\n",
      "linear regression. The loss landscape of a neural network is in general nonlinear, but the linear case\n",
      "may shed some light on how the effective loss can be derived from actual loss. For a sample r(which\n",
      "is the sum of two embeddings EiandEj), the prediction of the linear network is D(r) =Ar+b.\n",
      "The loss function is ( yis its corresponding label):\n",
      "ℓ=1\n",
      "2|Ar+b−y|2\n",
      "\n",
      "ℓpred+γ\n",
      "2||A||2\n",
      "F\n",
      "\n",
      "ℓreg, (35)\n",
      "where the ﬁrst and the second term are prediction error and regularization, respectively. Both the\n",
      "model (A,b)and the input rare updated via gradient ﬂow, with learning rate ηAandηx, respectively:\n",
      "dA\n",
      "dt=−ηA∂ℓ\n",
      "∂A,db\n",
      "dt=−ηA∂ℓ\n",
      "∂b,dr\n",
      "dt=−ηx∂ℓ\n",
      "∂r. (36)\n",
      "Insertingℓinto the above equations, we obtain the gradient ﬂow:\n",
      "dA\n",
      "dt=−ηA∂ℓ\n",
      "∂A=−ηA[A(rrT+γ) + (b−y)rT],\n",
      "db\n",
      "dt=−ηA∂ℓ\n",
      "∂b=−ηA(Ar+b−y)\n",
      "dr\n",
      "dt=−ηx∂ℓ\n",
      "∂r=−ηxAT(Ar+b−y).(37)\n",
      "For thedb/dtequation, after ignoring the Arterm and set the initial condition b(0) = 0, we obtain\n",
      "analytically b(t) = (1−e−2ηAt)y. Inserting this into the ﬁrst and third equations, we have\n",
      "dA\n",
      "dt=−ηA[A(rrT+γ)−e−2ηAtyrT],\n",
      "dr\n",
      "dt=−ηxATAr\n",
      "internal interaction+ηxe−2ηAtATy\n",
      "external force.(38)\n",
      "For the second equation on the evolution of dr/dt, we can artiﬁcially decompose the right hand side\n",
      "into two terms, based on whether they depend on the label y. In this way, we call the ﬁrst term\n",
      "\"internal interaction\" since it does not depend on y, while the second term \"external force\". Note\n",
      "this distinction seems a bit artiﬁcial from a mathematical perspective, but it can be conceptually\n",
      "helpful from a physics perspective. We will show below the internal interaction term is important for\n",
      "representations to form. Because we are interested in how two samples interact, we now consider\n",
      "another sample at r′, and the evolution becomes\n",
      "dA\n",
      "dt=−ηA[A(rrT+r′r′T+ 2γ)−e−2ηAty(r+r′)T],\n",
      "dr\n",
      "dt=−ηxATAr+ηxe−2ηAtATy,\n",
      "dr′\n",
      "dt=−ηxATAr′+ηxe−2ηAtATy.(39)\n",
      "Subtractingdr/dtbydr′/dtand setting r′=−r, the above equations further simply to\n",
      "dA\n",
      "dt=−2ηAA(rrT+γ),\n",
      "dr\n",
      "dt=−ηxATAr.(40)\n",
      "The second equation implies that the pair of samples interact via a quadratic potential U(r) =\n",
      "1\n",
      "2rTATAr, leading to a linear attractive force f(r)∝r. We now consider the adiabatic limit where\n",
      "ηA→0.\n",
      "28The adiabatic limit Using the standard initialization (e.g., Xavier initialization) of neural networks,\n",
      "we have AT\n",
      "0A0≈I. As a result, the quadratic potential becomes U(r) =1\n",
      "2rTr, which is time-\n",
      "independent because ηA→0. We are now in the position to analyze the addition problem. For two\n",
      "samples x(1)=Ei+Ejandx(2)=Em+Enwith the same label ( i+j=m+n), they contribute\n",
      "to an interaction term\n",
      "U(i,j,m,n ) =1\n",
      "2|Ei+Ej−Em−En|2\n",
      "2. (41)\n",
      "Averaging over all possible quadruples in the training dataset D, the total energy of the system is\n",
      "ℓ0=∑\n",
      "(i,j,m,n )∈P0(D)1\n",
      "2|Ei+Ej−Em−En|2\n",
      "2/|P0(D)|, (42)\n",
      "whereP0(D) ={(i,j,m,n )|i+j=m+n,(i,j)∈D,(m,n)∈D}. To make it scale-invariant,\n",
      "we deﬁne the normalized Hamiltonian Eq. (42) as\n",
      "ℓeﬀ=ℓ0\n",
      "Z0, Z 0=∑\n",
      "i|Ei|2\n",
      "2 (43)\n",
      "which is the effective loss we used in Section 3.2.\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(pages))\n",
    "print()\n",
    "\n",
    "full_document = \"\"\n",
    "\n",
    "for page in pages:\n",
    "  full_document += page.page_content\n",
    "\n",
    "print(full_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction\n",
      "\n",
      "2 Problem Setting\n",
      "\n",
      "3 Why Generalization Occurs: Representations and Dynamics\n",
      "\n",
      "4 Delayed Generalization: A Phase Diagram\n",
      "\n",
      "5 Related work\n",
      "\n",
      "6 Conclusion\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Regular expression pattern for section titles\n",
    "pattern = r\"\\n\\d+ [A-Z][a-zA-Z\\s:]+\\n\"\n",
    "# TODO: Write a regular expression pattern to match Abstract\n",
    "\n",
    "# Find all matches\n",
    "matches = re.findall(pattern, full_document)\n",
    "\n",
    "matches.insert(0, \"Abstract\")\n",
    "\n",
    "# Print all section titles\n",
    "for match in matches:\n",
    "    print(match.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['Abstract', '1 Introduction', '2 Problem Setting', '3 Why Generalization Occurs: Representations and Dynamics', '4 Delayed Generalization: A Phase Diagram', '5 Related work', '6 Conclusion'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"\\n\\d+ [A-Z][a-zA-Z\\s:]+\\n\"\n",
    "\n",
    "# Split the text into sections\n",
    "sections = re.split(pattern, full_document)\n",
    "\n",
    "# number of sections should match with number of section titles (aka matches of regex pattern)\n",
    "# print(sections[0])\n",
    "# print(sections[2])\n",
    "# print(len(sections))\n",
    "\n",
    "# number of sections should match with number of section titles (aka matches of regex pattern)\n",
    "print(len(sections))\n",
    "\n",
    "# Create a dictionary to store section titles and contents\n",
    "section_contents = {}\n",
    "\n",
    "# Use zip to iterate over matches and sections simultaneously\n",
    "for match, section in zip(matches, sections):\n",
    "    section_title = match.strip()\n",
    "    content = section.strip()\n",
    "    section_contents[section_title] = content\n",
    "\n",
    "section_contents.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We have shown how, in both toy models and general settings, that representation enables generalization\\nwhen it reﬂects structure in the data. We developed an effective theory of representation learning\\ndynamics (in a toy setting) which predicts the critical dependence of learning on the training data\\nfraction. We then presented four learning phases (comprehension, grokking, memorization and\\nconfusion) which depend on the decoder capacity and learning speed (given by, among other things,\\nlearning rate and weight decay) in decoder-only architectures. While we have mostly focused on a\\ntoy model, we ﬁnd preliminary evidence that our results generalize to the setting of [1].\\nOur work can be viewed as a step towards a statistical physics of deep learning , connecting the\\n“microphysics” of low-level network dynamics with the “thermodynamics” of high-level model\\nbehavior. We view the application of theoretical tools from physics, such as effective theories [ 24], to\\nbe a rich area for further work. The broader impact of such work, if successful, could be to make\\nmodels more transparent and predictable [ 23,25,26], crucial to the task of ensuring the safety of\\nadvanced AI systems.\\n10References\\n[1]Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Gen-\\neralization beyond overﬁtting on small algorithmic datasets. arXiv preprint arXiv:2201.02177 ,\\n2022.\\n[2]Neel Nanda and Tom Lieberum. A mechanistic interpretability analysis of\\ngrokking, 2022. URL https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/\\na-mechanistic-interpretability-analysis-of-grokking .\\n[3]Beren Millidge. Grokking ’grokking’. https://beren.io/\\n2022-01-11-Grokking-Grokking/ , 2022.\\n[4]Rohin Shah. Alignment Newsletter #159. https:\\n//www.alignmentforum.org/posts/zvWqPmQasssaAWkrj/\\nan-159-building-agents-that-know-how-to-experiment-by#DEEP_LEARNING_ ,\\n2021.\\n[5] Yedid Hoshen and Shmuel Peleg. Visual learning of arithmetic operation. In AAAI , 2016.\\n[6]Yang-Hui He. Machine-learning mathematical structures. arXiv preprint arXiv:2101.06317 ,\\n2021.\\n[7]Sergei Gukov, James Halverson, Fabian Ruehle, and Piotr Sułkowski. Learning to unknot.\\nMachine Learning: Science and Technology , 2(2):025035, 2021.\\n[8]Alex Davies, Petar Veli ˇckovi ´c, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomašev,\\nRichard Tanburn, Peter Battaglia, Charles Blundell, András Juhász, et al. Advancing mathemat-\\nics by guiding human intuition with ai. Nature , 600(7887):70–74, 2021.\\n[9]Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.\\nDeep double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics:\\nTheory and Experiment , 2021(12):124003, 2021.\\n[10] Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal regularization can\\nmitigate double descent. arXiv preprint arXiv:2003.01897 , 2020.\\n[11] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and\\nnew perspectives. IEEE transactions on pattern analysis and machine intelligence , 35(8):\\n1798–1828, 2013.\\n[12] Yassine Ouali, Céline Hudelot, and Myriam Tami. An overview of deep semi-supervised\\nlearning. arXiv preprint arXiv:2006.05278 , 2020.\\n[13] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena\\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\\net al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural\\nInformation Processing Systems , 33:21271–21284, 2020.\\n[14] Phuc H Le-Khac, Graham Healy, and Alan F Smeaton. Contrastive representation learning: A\\nframework and review. IEEE Access , 8:193907–193934, 2020.\\n[15] James Halverson, Anindita Maiti, and Keegan Stoner. Neural networks and quantum ﬁeld\\ntheory. Machine Learning: Science and Technology , 2(3):035002, 2021.\\n[16] Daniel A Roberts, Sho Yaida, and Boris Hanin. The principles of deep learning theory. arXiv\\npreprint arXiv:2106.10165 , 2021.\\n[17] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka.\\nNeural mechanics: Symmetry and broken conservation laws in deep learning dynamics. arXiv\\npreprint arXiv:2012.04728 , 2020.\\n[18] Yansong Gao and Pratik Chaudhari. A free-energy principle for representation learning. In\\nInternational Conference on Machine Learning , pages 3367–3376. PMLR, 2020.\\n11[19] Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mézard, and Lenka Zdeborová.\\nGeneralisation error in learning with random features and the hidden manifold model. In\\nInternational Conference on Machine Learning , pages 3452–3462. PMLR, 2020.\\n[20] Mohammad Pezeshki, Amartya Mitra, Yoshua Bengio, and Guillaume Lajoie. Multi-scale\\nfeature learning dynamics: Insights for double descent. In International Conference on Machine\\nLearning , pages 17669–17690. PMLR, 2022.\\n[21] Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc Mezard, and Lenka\\nZdeborova. The gaussian equivalence of generative models for learning with shallow neural\\nnetworks. In Joan Bruna, Jan Hesthaven, and Lenka Zdeborova, editors, Proceedings of the\\n2nd Mathematical and Scientiﬁc Machine Learning Conference , volume 145 of Proceedings\\nof Machine Learning Research , pages 426–471. PMLR, 16–19 Aug 2022. URL https:\\n//proceedings.mlr.press/v145/goldt22a.html .\\n[22] R Kuhn and S Bos. Statistical mechanics for neural networks with continuous-time dynamics.\\nJournal of Physics A: Mathematical and General , 26(4):831, 1993.\\n[23] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom\\nHenighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain,\\nDeep Ganguli, Zac Hatﬁeld-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson\\nKernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,\\nSam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer\\nCircuits Thread , 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-\\nheads/index.html.\\n[24] Daniel A. Roberts, Sho Yaida, and Boris Hanin. The Principles of Deep Learning Theory .\\nCambridge University Press, 2022. https://deeplearningtheory.com .\\n[25] Deep Ganguli, Danny Hernandez, Liane Lovitt, Nova DasSarma, Tom Henighan, Andy Jones,\\nNicholas Joseph, Jackson Kernion, Ben Mann, Amanda Askell, et al. Predictability and surprise\\nin large generative models. arXiv preprint arXiv:2202.07785 , 2022.\\n[26] Jacob Steinhardt. Future ML Systems Will Be Qualitatively Different. https://www.\\nlesswrong.com/s/4aARF2ZoBpFZAhbbe/p/pZaPhGg2hmmPwByHc , 2022.\\n[27] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov,\\nand Alexander J Smola. Deep sets. Advances in neural information processing systems , 30,\\n2017.\\n[28] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the\\nterminal phase of deep learning training. Proceedings of the National Academy of Sciences , 117\\n(40):24652–24663, 2020.\\n[29] Wikipedia contributors. Thomson problem — Wikipedia, the free encyclope-\\ndia. https://en.wikipedia.org/w/index.php?title=Thomson_problem&oldid=\\n1091431454 , 2022. [Online; accessed 29-July-2022].\\n[30] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15750–15758,\\n2021.\\n[31] Zhi-Qin John Xu, Yaoyu Zhang, and Yanyang Xiao. Training behavior of deep neural network\\nin frequency domain. In International Conference on Neural Information Processing , pages\\n264–274. Springer, 2019.\\n[32] Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, and Zheng Ma. A type of generalization error induced\\nby initialization in deep neural networks. In Mathematical and Scientiﬁc Machine Learning ,\\npages 144–164. PMLR, 2020.\\n[33] Ziming Liu, Eric J. Michaud, and Max Tegmark. Omnigrok: Grokking beyond algorithmic data,\\n2022.\\n12[34] Blake Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay\\nGolan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models.\\nIn Jacob Abernethy and Shivani Agarwal, editors, Proceedings of Thirty Third Conference on\\nLearning Theory , volume 125 of Proceedings of Machine Learning Research , pages 3635–3673.\\nPMLR, 09–12 Jul 2020. URL https://proceedings.mlr.press/v125/woodworth20a.\\nhtml .\\nChecklist\\n1. For all authors...\\n(a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s\\ncontributions and scope? [Yes]\\n(b) Did you describe the limitations of your work? [Yes]\\n(c) Did you discuss any potential negative societal impacts of your work? [N/A]\\n(d)Have you read the ethics review guidelines and ensured that your paper conforms to\\nthem? [Yes]\\n2. If you are including theoretical results...\\n(a) Did you state the full set of assumptions of all theoretical results? [Yes]\\n(b) Did you include complete proofs of all theoretical results? [Yes]\\n3. If you ran experiments...\\n(a) Did you include the code, data, and instructions needed to reproduce the main experi-\\nmental results (either in the supplemental material or as a URL)? [Yes]\\n(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they\\nwere chosen)? [Yes]\\n(c)Did you report error bars (e.g., with respect to the random seed after running experi-\\nments multiple times)? [Yes]\\n(d)Did you include the total amount of compute and the type of resources used (e.g., type\\nof GPUs, internal cluster, or cloud provider)? [Yes] All experiments were run on a\\nworkstation with two NVIDIA A6000 GPUs within a few days.\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n(a) If your work uses existing assets, did you cite the creators? [N/A]\\n(b) Did you mention the license of the assets? [N/A]\\n(c)Did you include any new assets either in the supplemental material or as a URL? [N/A]\\n(d)Did you discuss whether and how consent was obtained from people whose data you’re\\nusing/curating? [N/A]\\n(e)Did you discuss whether the data you are using/curating contains personally identiﬁable\\ninformation or offensive content? [N/A]\\n5. If you used crowdsourcing or conducted research with human subjects...\\n(a)Did you include the full text of instructions given to participants and screenshots, if\\napplicable? [N/A]\\n(b)Did you describe any potential participant risks, with links to Institutional Review\\nBoard (IRB) approvals, if applicable? [N/A]\\n(c)Did you include the estimated hourly wage paid to participants and the total amount\\nspent on participant compensation? [N/A]\\n13Appendix\\nA Deﬁnitions of the phases of learning\\nTable 1: Deﬁnitions of the four phases of learning\\ncriteria\\nPhasetraining acc > 90%\\nwithin 105stepsvalidation acc > 90%\\nwithin 105stepsstep(validation acc>90%)\\n−step(training acc>90%)< 103\\nComprehension Yes Yes Yes\\nGrokking Yes Yes No\\nMemorization Yes No Not Applicable\\nConfusion No No Not Applicable\\nB Applicability of our toy setting\\nIn the main paper, we focused on the toy setting with (1) the addition dataset and (2) the addition\\noperation hard coded in the decoder. Although both simpliﬁcations appear to have quite limited\\napplicability, we argue below that the analysis of the toy setting can actually apply to all Abelian\\ngroups.\\nThe addition dataset is the building block of all Abelian groups A cyclic group is a group that\\nis generated by a single element. A ﬁnite cyclic group with order nisCn={e,g,g2,···,gn−1}\\nwhereeis the identify element and gis the generator and gi=gjwheneveri=j(modn). The\\nmodulo addition and {0,1,···,n−1}form a cyclic group with e= 0andgcan be any number q\\ncoprime tonsuch that (q,n) = 1 . Since algorithmic datasets contain only symbolic but no arithmetic\\ninformation, the datasets of modulo addition could apply to all other cyclic groups, e.g., modulo\\nmultiplication and discrete rotation groups in 2D.\\nAlthough not all Abelian groups are cyclic, a ﬁnite Abelian group Gcan be always decomposed into\\na direct product of kcyclic groups G=Cn1×Cn2···Cnk. So after training kneural networks with\\neach handling one cyclic group separately, it is easy to construct a larger neural network that handles\\nthe whole Abelian group.\\nThe addition operation is valid for all Abelian groups It is proved in [ 27] that for a permutation\\ninvariant function f(x1,x2,···,xn), there exists ρandφsuch that\\nf(x1,x2,···,xn) =ρ[n∑\\ni=1φ(xi)], (10)\\norf(x1,x2) =ρ(φ(x1) +φ(x2))forn= 2. Notice that φ(xi)corresponds to the embedding vector\\nEi,ρcorresponds to the decoder. The addition operator naturally emerges from the commutativity\\nof the operator, not restricting the operator itself to be addition. For example, multiplication of\\ntwo numbers x1andx2can be written as x1x2= exp(ln(x1) + ln(x2))whereρ(x) = exp(x)and\\nφ(x) = ln(x).\\nC An illustrative example\\nWe use a concrete case to illustrate why parallelograms lead to generalization (see Figure 9). For the\\npurpose of illustration, we exploit a curriculum learning setting, where a neural network is fed with a\\nfew new samples each time. We will illustrate that, as we have more samples in the training set, the\\nideal model M∗(deﬁned in Section 3.2) will arrange the representation R∗in a more structured way,\\ni.e., more parallelograms are formed, which helps generalization to unseen validation samples. For\\nsimplicity we choose p= 6.\\n•D1= (0,4)andD2= (1,3)have the same label, so (0,4,1,3)becomes a parallelogram\\nsuch that E0+E4=E1+E3→E3−E0=E4−E1.D3= (1,5)andD4= (2,4)have\\n14the same label, so (1,5,2,4)becomes a parallelogram such that E1+E5=E2+E4→\\nE4−E1=E5−E2. We can derive from the ﬁrst two equations that E5−E2=E3−E0→\\nE0+E5=E2+E3, which implies that (0,5,2,3)is also a parallelogram (see Figure 9(a)).\\nThis means if (0,5)in training set, our model can predict (2,3)correctly.\\n•D5= (0,2)andD6= (1,1)have the same label, so E0+E2= 2E1, i.e., 1is the middle\\npoint of 0and2(see Figure 9(b)). Now we can derive that 2E4=E3+E5, i.e., 4is the\\nmiddle point of 3and5. If(4,4)is in the training data, our model can predict (3,5)correctly.\\n• Finally,D7= (2,4)andD8= (3,3)have the same label, so 2E3=E2+E4, i.e., 3should\\nbe placed at the middle point of 2and4, ending up Figure 9(c). This linear structure agrees\\nwith the arithmetic structure of R.\\nIn summary, although we have p(p+ 1)/2 = 21 different training samples for p= 6, we only need 8\\ntraining samples to uniquely determine the perfect linear structure (up to linear transformation). The\\npunchline is: representations lead to generalization.\\nFigure 9: As we include more data in the training set, the (ideal) model is capable of discovering\\nincreasingly structured representations (better RQI), from (a) to (b) to (c).\\nD Deﬁnition of ˆAcc\\nGiven a training set Dand a representation R, if(i,j)is a validation sample, can the neural network\\ncorrectly predict its output, i.e., Dec(Ei+Ej) =Yi+j? Since neural network has never seen (i,j)\\nin the training set, one possible mechanism of induction is through\\nDec(Ei+Ej) = Dec( Em+En) =Ym+n(=Yi+j). (11)\\nThe ﬁrst equality Dec(Ei+Ej) = Dec( Em+En)holds only when Ei+Ej=Em+En(i.e.,\\n(i,j,m,n )is a parallelogram). The second equality Dec(Em+En) =Ym+n, holds when (m,n)\\nis in the training set, i.e., (m,n)∈D, under the zero training loss assumption. Rigorously, given a\\ntraining setDand a parallelogram set P(which can be calculated from R), we collect all zero loss\\nsamples in an augmented training setD\\nD(D,P) =D⋃\\n{(i,j)|∃(m,n)∈D,(i,j,m,n )∈P}. (12)\\nKeepingDﬁxed, a larger Pwould probably produce a larger D, i.e., ifP1⊆P2, thenD(D,P 1)⊆\\nD(P,P 2), which is why in Eq. (3) our deﬁned RQI∝|P|gets its name “representation quality\\nindex\", because higher RQI normally means better generalization. Finally, the expected accuracy\\nfrom a dataset Dand a parallelogram set Pis:\\nˆAcc =|D(D,P)|\\n|D0|, (13)\\nwhich is the estimated accuracy (of the full dataset), and P=P(R)is deﬁned on the representation\\nafter training. On the other hand, accuracy Acc can be accessed empirically from trained neural\\nnetwork. We veriﬁed Acc≈ˆAcc in a toy setup (addition dataset p= 10 , 1D embedding space,\\nhard code addition), as shown in Figure 3 (c). Figure 3 (a)(b) show Acc andˆAcc as a function of\\ntraining set ratio, with each dot corresponding to a different random seed. The dashed red diagonal\\ncorresponds to memorization of the training set, and the vertical gap refers to generalization.\\n15Although the agreement is good for 1D embedding vectors, we do not expect such agreement can\\ntrivially extend to high dimensional embedding vectors. In high dimensions, our deﬁnition of RQI is\\ntoo restrictive. For example, suppose we have an embedding space with Ndimensions. Although the\\nrepresentation may form a linear structure in the ﬁrst dimension, the representation can be arbitrary\\nin otherN−1dimensions, leading to RQI≈0. However, the model may still generalize well if the\\ndecoder learns to keep only the useful dimension and drop all other N−1useless dimensions. It\\nwould be interesting to investigate how to deﬁne an RQI that takes into account the role of decoder in\\nfuture works.\\nE The gap of a realistic model Mand the ideal model M∗\\nRealistic models Musually form fewer number of parallelograms than ideal models M∗. In this\\nsection, we analyze the properties of ideal models and calculated ideal RQI and ideal accuracy,\\nwhich set upper bounds for empirical RQI and accuracy. The upper bound relations are veriﬁed via\\nnumerical experiments in Figure 10.\\nSimilar to Eq. (12) where some validation samples can be derived from training samples, we\\ndemonstrate how implicit parallelograms can be ‘derived’ from explicit ones in P0(D). The so-called\\nderivation follows a simple geometric argument that: if A1B1is equal and parallel to A2B2, and\\nA2B2is equal and parallel to A3B3, then we can deduce that A1B1is equal and parallel to A3B3\\n(hence (A1,B2,A2,B1)is a parallelogram).\\nRecall that a parallelogram (i,j,m,n )is equivalent to Ei+Ej=Em+En(∗). So we are\\nequivalently asking if equation (∗)can be expressed as a linear combination of equations in\\nA(P0(D)). If yes, then (∗)is dependent on A(P0(D))(deﬁned in Eq. (7)), i.e., A(P0(D))and\\nA(P0(D)⋃(i,j,m,n ))should have the same rank. We augment P0(D)by adding implicit parallel-\\nograms, and denote the augmented parallelogram set as\\nP(D) =P0(D)⋃\\n{q≡(i,j,m,n )|q∈P0,rank(A(P0(D))) = rank(A(P0(D)⋃\\nq))}.(14)\\nWe need to emphasize that an assumption behind Eq. (14) is that we have an ideal model M∗. When\\nthe model is not ideal, e.g., when the injectivity of the encoder breaks down, fewer parallelograms\\nare expected to form, i.e.,\\nP(R)⊆P(D). (15)\\nThe inequality is saying, whenever a parallelogram is formed in the representation after training, the\\nreason is hidden in the training set. This is not a strict argument, but rather a belief that today’s neural\\nnetworks can only copy what datasets (explicitly or implicitly) tell it to do, without any autonomous\\ncreativity or intelligence. For simplicity we call this belief Alexander Principle . In very rare cases\\nwhen something lucky happens (e.g., neural networks are initialized at approximate correct weights),\\nAlexander principle may be violated. Alexander principle sets an upper bound for RQI:\\nRQI(R)≤|P(D)|\\n|P0|≡RQI, (16)\\nand sets an upper bound for ˆAcc:\\nˆAcc≡ˆAcc(D,P(R))≤ˆAcc(D,P(D))≡Acc. (17)\\nIn Figure 10 (c)(f), we verify Eq. (16) and Eq. (17). We choose δ= 0.01to compute RQI( R,δ). We\\nﬁnd the trained models are usually far from being ideal, although we already include a few useful\\ntricks proposed in Section 4 to enhance representation learning. It would be an interesting future\\ndirection to develop better algorithms so that the gap due to Alexander principle can be reduced\\nor even closed. In Figure 10 (a)(b)(d)(e), four quantities (RQI, RQI, Acc, Acc) as functions of the\\ntraining data fraction are shown, each dot corresponding to one random seed. It is interesting to\\nnote that it is possible to have RQI = 1 only with<40% training data, i.e., 55×0.4 = 22 samples,\\nagreeing with our observation in Section 3.\\nRealistic representations Suppose an ideal model M∗and a realistic model Mwhich train on the\\ntraining setDgive the representation R∗andR, respectively. What is the relationship between R\\nandR∗? Due to the Alexander principle we know P(R)⊆P(D) =P(R∗). This means R∗has\\nmore parallelograms than R, henceR∗has fewer degrees of freedom than R.\\n160.0 0.2 0.4 0.6 0.8 1.0training data fraction0.00.20.40.60.81.0RQI(a)\\n0.0 0.2 0.4 0.6 0.8 1.0training data fraction0.00.20.40.60.81.0RQI(b)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nRQI0.00.20.40.60.81.0RQIIdeal AlgorithmAlexander\\nPrinciple(c)\\n0.0 0.2 0.4 0.6 0.8 1.0training data fraction0.00.20.40.60.81.0Acc\\nMemorization(d)\\n0.0 0.2 0.4 0.6 0.8 1.0training data fraction0.00.20.40.60.81.0Acc\\nMemorization(e)\\n0.0 0.2 0.4 0.6 0.8 1.0Acc0.00.20.40.60.81.0AccIdeal AlgorithmAlexander\\nPrinciple(f)Figure 10: We compare RQI and Acc for an ideal algorithm (with bar) and a realistic algorithm\\n(without bar). In (a)(b)(d)(e), four quantities (RQI, RQI, Acc, Acc) as functions of training data\\nfraction are shown. In (c)(f), RQI and Acc of the ideal algorithm sets upper bounds for those of the\\nrealistic algorithm.\\nWe illustrate with the toy case p= 4. The whole dataset contains p(p+ 1)/2 = 10 samples, i.e.,\\nD0={(0,0),(0,1),(0,2),(0,3),(1,1),(1,2),(1,3),(2,2),(2,3),(3,3)}. (18)\\nThe parallelogram set contains only three elements, i.e.,\\nP0={(0,1,1,2),(0,1,2,3),(1,2,2,3)}, (19)\\nOr equivalently the equation set\\nA0={A1 :E0+E2= 2E1,A2 :E0+E3=E1+E2,A3 :E1+E3= 2E2}. (20)\\nPictorially, we can split all possible subsets {A|A⊆A0}into different levels, each level deﬁned by\\n|A|(the number of elements). A subset A1in theithlevel points an direct arrow to another subset\\nA2in the (i+ 1)thlevel ifA2⊂A1, and we say A2is a child of A1, andA1is a parent of A2.\\nEach subset Acan determine a representation Rwithn(A)degrees of freedom. So Rshould be a\\ndescendant of R∗, andn(R∗)≤n(R). Numerically, n(A)is equal to the dimension of the null space\\nofA.\\nSuppose we have a training set\\nD={(0,2),(1,1),(0,3),(1,2),(1,3),(2,2)}, (21)\\nand correspondingly P(D) =P0,A(P) =A0. So an ideal model M∗will have the linear structure\\nEk=a+kb(see Figure 11 leftmost). However, a realistic model Mmay produce any descendants\\nof the linear structure, depending on various hyperparameters and even random seeds.\\nIn Figure 12, we show our algorithms actually generates all possible representations. We have\\ntwo settings: (1) fast decoder (η1,η2) = (10−3,10−2)(Figure 12 left), and (2) relatively slow\\ndecoder (η1,η2) = (10−2,10−3)(Figure 12) right). The relatively slow decoder produces better\\nrepresentations (in the sense of higher RQI) than a fast decoder, agreeing with our observation in\\nSection 4.\\n17Figure 11:p= 4 case. Equation set A(or geometrically, representation) has a hierarchy: a→b\\nmeansais a parent of b, andbis a child ofa. A realistic model can only generate representations that\\nare descendants of the representation generated by an ideal model.\\nFigure 12:p= 4case. Representations obtained from training neural networks are displayed. η1and\\nη2are learning rates of the representation and the decoder, respectively. As described in the main text,\\n(η1,η2) = (10−2,10−3)(right) is more ideal than (η1,η2) = (10−3,10−2)(left), thus producing\\nrepresentations containing more parallelograms.\\nF Conservation laws of the effective theory\\nRecall that the effective loss function\\nℓeff=ℓ0\\nZ0, ℓ 0≡∑\\n(i,j,m,n )∈P0(D)|Ei+Ej−Em−En|2/|P0(D)|, Z 0≡∑\\nk|Ek|2(22)\\nwhereℓ0andZ0are both quadratic functions of R={E0,···,Ep−1}, andℓeff= 0remains zero\\nunder rescaling and translation E′\\ni=aEi+b. We will ignore the 1/|P0(D)|factor inℓ0since\\nhaving it is equivalent to rescaing time, which does not affect conservation laws. The representation\\nvector Eievolves according to the gradient descent\\ndEi\\ndt=−∂ℓeff\\n∂Ei. (23)\\nWe will prove the following two quantities are conserved:\\nC=∑\\nkEk, Z 0=∑\\nk|Ek|2. (24)\\nEq. (22) and Eq. (23) give\\ndEi\\ndt=−ℓeff\\n∂Ei=−∂(ℓ0\\nZ0)\\n∂Ei=−1\\nZ0∂ℓ0\\n∂Ei+ℓ0\\nZ2\\n0∂Z0\\n∂Ei. (25)\\n18Then\\ndZ0\\ndt= 2∑\\niEk·dEk\\ndt(26)\\n=2\\nZ2\\n0∑\\niEi·(−Z0∂ℓ0\\n∂Ek+ 2ℓ0Ek)\\n=2\\nZ0(−∑\\nk∂ℓ0\\n∂Ek·Ek+ 2ℓ0)\\n= 0.\\nwhere the last equation uses the fact that\\n∑\\nk∂ℓ0\\n∂Ek·Ek= 2∑\\nk∑\\n(i,j,m,n )∈P0(D)(Ei+Ej−Em−En)(δik+δjk−δmk−δnk)·Ek\\n= 2∑\\n(i,j,m,n )∈P0(D)(Ei+Ej−Em−En)∑\\nk(δik+δjk−δmk−δnk)·Ek\\n=∑\\n(i,j,m,n )∈P0(D)(Ei+Ej−Em−En)·(Ei+Ej−Em−En)\\n= 2ℓ0\\nThe conservation of Z0prohibits the representation from collapsing to zero. Now that we have\\ndemonstrated that Z0is a conserved quantity, we can also show\\ndC\\ndt=∑\\nkdEk\\ndt(27)\\n=−1\\nZ0∑\\nk∂ℓ0\\n∂Ek\\n=−2\\nZ0∑\\nk∑\\n(i,j,m,n )∈P0(D)(Ei+Ej−Em−En)(δik+δjk−δmk−δnk)\\n=0.\\nThe last equality holds because the two summations can be swapped and∑\\nk(δik+δjk−δmk−δnk) =\\n0.\\nG More phase diagrams of the toy setup\\nWe study another three hyperparameters in the toy setup by showing phase diagrams similar to\\nFigure 6. The toy setup is: (1) addition without modulo ( p= 10 ); (2) training/validation is split into\\n45/10; (3) hard code addition; (4) 1D embedding. In the following experiments, the decoder is an\\nMLP with size 1-200-200-30. The representation and the encoder are optimized with AdamW with\\ndifferent hyperparameters. The learning rate of the representation is 10−3. We sweep the learning\\nrate of the decoder in range [10−4,10−2]as the x axis, and sweep another hyperparameter as the\\ny axis. By default, we use full batch size 45, initialization scale s= 1 and zero weight decay of\\nrepresentation.\\nBatch size controls the amount of noise in the training dynamics. In Figure 13, the grokking region\\nappears at the top left of the phase diagram (small decoder learning rate and small batch size).\\nHowever, large batch size (with small learning rate) leads to comprehension, implying that smaller\\nbatch size seems harmful. This makes sense since to get crystals (good structures) in experiments,\\none needs a freezer which gradually decreases temperature, rather than something perturbing the\\nsystem with noise.\\nInitialization scale controls distances among embedding vectors at initialization. We initialize\\ncomponents of embedding vectors from independent uniform distribution U[−s/2,s/2]wheres\\n191e-4 1e-3 1e-2\\nlearning rate1\\n12\\n45 batch size\\ncomprehensionmemorizationgrokkingconfusionAddition group (regression)\\n1e-4 1e-3 1e-2\\nlearning rate1\\n12\\n45 batch size\\ncomprehensionmemorizationgrokkingconfusionAddition group (classiﬁcation)Figure 13: Phase diagrams of decoder learning rate (x axis) and batch size (y axis) for the addition\\ngroup (left: regression; right: classiﬁcation). Small decoder leanrning rate and large batch size\\n(bottom left) lead to comprehension.\\n1e-4 1e-3 1e-2\\nlearning rate0.01\\n1.0\\n100.0 initialization scalecomprehension\\nmemorization\\nconfusionAddition group (regression)\\n1e-4 1e-3 1e-2\\nlearning rate0.01\\n1.0\\n100.0 initialization scalecomprehension\\nmemorization\\nconfusionAddition group (classiﬁcation)\\nFigure 14: Phase diagrams of decoder learning rate (x axis) and initialization (y axis) for the addition\\ngroup (left: regression; right: classiﬁcation). Small intialization scale (top) leads to comprehension.\\nis called the initialization scale. Shown in Figure 14, it is beneﬁcial to use a smaller initialization\\nscale. This agrees with the physical intuition that closer particles are more likely to interact and form\\nstructures. For example, the distances among molecules in ice are much smaller than distances in gas.\\nRepresentation weight decay controls the magnitude of embedding vectors. Shown in Figure 15,\\nwe see the representation weight decay in general does not affect model performance much.\\nH General groups\\nH.1 Theory\\nWe focused on Abelian groups for the most part of the paper. This is, however, simply due to\\npedagogical reasons. In this section, we show that it is straight-forward to extend deﬁnitions of\\nparallelograms and representation quality index (RQI) to general non-Abelian groups. We will also\\nshow that most (if not all) qualitative results for the addition group also apply to the permutation\\ngroup.\\n201e-4 1e-3 1e-2\\nlearning rate0.0\\n5.0\\n10.0weight decay (representation)comprehension\\nmemorizationgrokkingAddition group (regression)\\n1e-4 1e-3 1e-2\\nlearning rate0.0\\n5.0\\n10.0weight decay (representation)\\ncomprehension\\nmemorizationgrokking\\nconfusionAddition group (classiﬁcation)Figure 15: Phase diagrams of decoder learning rate (x axis) and representation weight decay (y axis)\\nfor the addition group (left: regression; right: classiﬁcation). Representation weight decay does not\\naffect model performance much.\\nMatrix representation for general groups Let us ﬁrst review the deﬁnition of group representation.\\nA representation of a group Gon a vector space Vis a group homomorphism from GtoGL(V), the\\ngeneral linear group on V. That is, a representation is a map ρ:G→GL(V)such that\\nρ(g1g2) =ρ(g1)ρ(g2),∀g1,g2∈G. (28)\\nIn the caseVis of ﬁnite dimension n, it is common to identify GL(V)withnbyninvertible matrices.\\nThe punchline is that: each group element can be represented as a matrix, and the binary operation is\\nrepresented as matrix multiplication.\\nA new architecture for general groups Inspired by the matrix representation, we embed each group\\nelementaas a learnable matrix Ea∈Rd×d(as opposed to a vector), and manually do matrix\\nmultiplication before sending the product to the deocder for regression or classiﬁcation. More\\nconcretly, for a◦b=c, our architecture takes as input two embedding matrices EaandEband\\naims to predict Ycsuch that Yc= Dec( EaEb), where EaEbmeans the matrix multiplication of Ea\\nandEb. The goal of this simplication is to disentangle learning the representation and learning the\\narithmetic operation (i.e, the matrix multiplication). We will show that, even with this simpliﬁcation,\\nwe are still able to reproduce the characteristic grokking behavior and other rich phenomenon.\\nGeneralized parallelograms we deﬁne generalized parallelograms: (a,b,c,d )is a generalized\\nparallelogram in the representation if ||EaEb−EcEd||2\\nF≤δ, whereδ>0is a threshold to tolerate\\nnumerical errors. Before presenting the numerical results for the permutation group, we show an\\nintuitive picture about how new parallelograms can be deduced from old ones for general groups,\\nwhich is the key to generalization.\\nDeduction of parallelograms We ﬁrst recall the case of the Abelian group (e.g., addition group). As\\nshown in Figure 16, when (a,d,b,c )and(c,f,d,e )are two parallelograms, we have\\nEa+Ed=Eb+Ec,\\nEc+Ef=Ed+Ed.(29)\\nWe can derive that Ea+Ef=Eb+Eeimplying that (a,f,b,e )is also a parallelogram. That is, for\\nAbelian groups, two parallelograms are needed to deduce a new parallelogram.\\nFor the non-Abelian group, if we have only two parallelograms such that\\nEaEd=EbEc,\\nEfEc=EeEd,(30)\\nwe have E−1\\nbEa=EcE−1\\nd=E−1\\nfEe, but this does not lead to something like EfEa=EeEb,\\nhence useless for generalization. However, if we have a third parallelogram such that\\nEeEh=EfEg (31)\\n21Figure 16: Deduction of parallelograms\\nwe have E−1\\nbEa=EcE−1\\nd=E−1\\nfEe=EgE−1\\nh, equivalent to EaEh=EbEg, thus establishing a\\nnew parallelogram (a,h,b,g ). That is, for non-Abelian groups, three parallelograms are needed to\\ndeduce a new parallelogram.\\nH.2 Numerical Results\\nIn this section, we conduct numerical experiments on a simple non-abelian group: the permutation\\ngroupS3. The group has 6 group elements, hence the full dataset contains 36 samples. We embed each\\ngroup element ainto a learnable 3×3embedding matrix Ea. We adopt the new architecture described\\nin the above subsection: we hard code matrix multiplication of two input embedding matrices before\\nfeeding to the decoder. After deﬁning the generalized parallelogram in the last subsection, we can\\ncontinue to deﬁne RQI (as in Section 3) and predict accuracy ˆAcc from representation (as in appendix\\nD). We also compute the number of steps needed to reach RQI = 0.95.\\nRepresentation We ﬂatten each embedding matrix into a vector, and apply principal component\\nanalysis (PCA) to the vectors. We show the ﬁrst three principal components of these group elements\\nin Figure 17. On the plane of PC1 andPC3 , the six points are organized as a hexagon.\\nPC 13\\n2\\n1\\n0123PC 2\\n1.5\\n1.0\\n0.5\\n0.00.51.01.5PC 3\\n1.5\\n1.0\\n0.5\\n0.00.51.01.5\\n[0, 1, 2]\\n[0, 2, 1][1, 0, 2]\\n[1, 2, 0][2, 0, 1]\\n[2, 1, 0]\\n3\\n 2\\n 1\\n 0 1 2 3\\nPC11.5\\n1.0\\n0.5\\n0.00.51.01.5PC2\\n[0, 1, 2] [0, 2, 1]\\n[1, 0, 2][1, 2, 0][2, 0, 1]\\n[2, 1, 0]\\n3\\n 2\\n 1\\n 0 1 2 3\\nPC11.5\\n1.0\\n0.5\\n0.00.51.01.5PC3[0, 1, 2]\\n[0, 2, 1][1, 0, 2]\\n[1, 2, 0][2, 0, 1]\\n[2, 1, 0]\\n1.5\\n 1.0\\n 0.5\\n 0.0 0.5 1.0 1.5\\nPC21.5\\n1.0\\n0.5\\n0.00.51.01.5PC3[0, 1, 2]\\n[0, 2, 1][1, 0, 2]\\n[1, 2, 0][2, 0, 1]\\n[2, 1, 0]\\nFigure 17: Permuation group S3. First three principal components of six embedding matrices R3×3.\\nRQI In Figure 18 (a), we show RQI as a function of training data fraction. For each training data\\nfraction, we run 11 random seeds (shown as scatter points), and the blue line corresponds to the\\nhighest RQI.\\nSteps to reach RQI = 0.95In Figure 18 (b), we whow the steps to reach RQI>0.95as a function\\nof training data fraction, and ﬁnd a phase transition at r=rc= 0.5. The blue line corresponds to the\\nbest model (smallest number of steps).\\n220.4 0.6 0.8\\ntraining data fraction0.00.20.40.60.81.0RQI(a)\\n0.4 0.6 0.8\\ntraining data fraction103104steps to RQI >0.95\\nrc= 0.5(b)\\n0.2 0.4 0.6 0.8 1.0\\ntraining data fraction0.20.30.40.50.60.70.80.91.0Acc(c)\\n0.2 0.4 0.6 0.8 1.0\\ntraining data fraction0.20.30.40.50.60.70.80.91.0ˆAcc(d)\\n0.2 0.4 0.6 0.8 1.0\\nˆAcc0.20.30.40.50.60.70.80.91.0Acc(e)Figure 18: Permutation group S3. (a) RQI increases as training set becomes larger. Each scatter point\\nis a random seed, and the blue line is the highest RQI obtained with a ﬁxed training set ratio; (b)\\nsteps to reach RQI>0.95. The blue line is the smallest number of steps required. There is a phase\\ntransition around rc= 0.5. (c) real accuracy Acc; (d) predicted accuracy ˆAcc; (e) comparison of Acc\\nandˆAcc:ˆAcc serves as a lower bound of Acc.\\nAccuracy The real accuracy Acc is shown in Figure 18 (c), while the predicted accuracy ˆAcc\\n(calculated from RQI) is shown in Figure 18 (d). Their comparison is shown in (e): ˆAccis a lower\\nbound of Acc, implying that there must be some generalization mechanism beyond RQI.\\nPhase diagram We investigate how the model performance varies under the change of two knobs:\\ndecoder learning rate and decoder weight decay. We calculate the number of steps to training accuracy\\n≥0.9and validation accuracy ≥0.9, respectively, shown in Figure 6 (d).\\nI Effective theory for image classiﬁcation\\nIn this section, we show our effective theory proposed in Section 3.2 can generalize beyond algorith-\\nmic datasets. In particular, we will apply the effective theory to image classiﬁcations. We ﬁnd that:\\n(i) The effective theory naturally gives rise to a novel self-supervised learning method, which can\\nprovably avoid mode collapse without contrastive pairs. (ii) The effective theory can shed light on the\\nneural collapse phenomenon [ 28], in which same-class representations collapse to their class-means.\\nWe ﬁrst describe how the effective theory applies to image classiﬁcation. The basic idea is again that,\\nsimilar to algorithmic datasets, neural networks try to develop a structured representation of the inputs\\nbased on the relational information between samples (class labels in the case of image classiﬁcation,\\nsum parallelograms in the case of addition, etc.). The effective theory has two ingredients: (i) samples\\nwith the same label are encouraged to have similar representations; (ii) the effective loss function\\nis scale-invariant to avoid all representations collapsing to zero (global collapse). As a result, an\\neffective loss for image classiﬁcation has the form\\nℓeﬀ=ℓ\\nZ, ℓ =∑\\n(x,y)∈P|f(x)−f(y)|2, Z =∑\\nx|f(x)|2(32)\\nwhere xis an image, f(x)is its representation, (x,y)∈Prefers to unique pairs xandythat have\\nthe same label. Scale invariance means the loss function ℓeﬀdoes not change under the linear scaling\\nf(x)→af(x).\\nRelation to neural collapse It was observed in [ 28] that image representations in the penultimate\\nlayer of the model have some interesting features: (i) representations of same-class images collapse\\nto their class-means; (ii) class-means of different classes develop into an equiangular tight frame. Our\\neffective theory is able to predict the same-class collapse, but does not necessarily put class-means\\ninto equiangular tight frames. We conjecture that little explicit repulsion among different classes can\\nhelp class-means develop into an equiangular tight frame, similar to electrons developing into lattice\\nstructures on a sphere under repulsive Coulomb forces (the Thomson problem [ 29]). We would like\\nto investigate this modiﬁcation of the effective theory in the future.\\nExperiment on MNIST We directly apply the effective loss Eq. (32) to the MNIST dataset. Firstly,\\neach image xis randomly encoded to a 2D embedding f(x)via the same encoder MLP whose weights\\nare randomly initialized. We then train these embeddings by minimizing the effective loss ℓeﬀwith\\n23Figure 19: Our effective theory applies to MNIST image classiﬁcations. Same-class images collapse\\nto their class-means, while class-means of different classes stay separable. As such, the effective\\ntheory serves as a novel self-supervised learning method, as well as shed some light on neural collapse.\\nPlease see texts in Appendix I.\\nan Adam optimizer ( 10−3learning rate) for 100 steps. We show the evolution of these embeddings in\\nFigure 19. Images of the same class collapse to their class-means, and different class-means do not\\ncollapse. This means that our effective theory can give rise to a good representation learning method\\nwhich only exploits non-contrastive relational information in datasets.\\nLink to self-supervised learning Note thatℓitself is vulnerable to global collapse, in the context\\nof Siamese learning without contrastive pairs. Various tricks (e.g., decoder with momentum, stop\\ngradient) [ 13,30] have been proposed to avoid global collapse. However, the reasons why these\\ntricks can avoid global collapse are unclear. We argue ℓfails simply because ℓ→a2ℓunder scaling\\nf(x)→af(x)so gradient descent on ℓencouragea→0. Based on this picture, our effective theory\\nprovides another possible ﬁx: make the loss function ℓscale-invariant (by the normalized loss ℓeﬀ),\\nso the gradient ﬂow has no incentive to change representation scales. In fact, we can prove that\\n24the gradient ﬂow on ℓeﬀpreserveZ(variance of representations) so that global collapse is avoided\\nprovably:\\n∂ℓeﬀ\\n∂f(x)=1\\nZ∂ℓ\\n∂f(x)−l\\nZ2∂Z\\n∂f(x)=2\\nZ∑\\ny∼x(f(x)−f(y))−2ℓ\\nZ2f(x),\\ndZ\\ndt= 2∑\\nxf(x)·df(x)\\ndt= 2∑\\nxf(x)·∂ℓeﬀ\\n∂f(x)\\n=4\\nZ∑\\nxf(x)·(∑\\ny∼x(f(x)−f(y))−ℓ\\nZf(x))\\n=4\\nZ[∑\\nxf(x)·∑\\ny∼x(f(x)−f(y))−∑\\nxℓ\\nZ|f(x)|2]\\n= 0.(33)\\nwhere we use the fact that\\n∑\\nxf(x)·∑\\ny∼x(f(x)−f(y)) =∑\\n(x,y)∈P(f(x)−f(y))·(f(x)−f(y)) =ℓ (34)\\nJ Grokking on MNIST\\nTo induce grokking on MNIST, we make two nonstandard decisions: (1) we reduce the size of the\\ntraining set from 50k to 1k samples (by taking a random subset) and (2) we increase the scale of the\\nweight initialization distribution (by multiplying the initial weights, sampled with Kaiming uniform\\ninitialization, by a constant >1).\\nThe choice of large initializations is justiﬁed by [ 31–33] which ﬁnd large initializations overﬁt data\\neasily but prone to poor generalization. Relevant to this, initialization scale is found to regulate\\n“kernel” vs “rich” learning regimes in networks [34].\\nWith these modiﬁcations to training set size and initialization scale, we train a depth-3 width-200\\nMLP with ReLU activations with the AdamW optimizer. We use MSE loss with one-hot targets,\\nrather than cross-entropy. With this setup, we ﬁnd that the network quickly ﬁts the train set, and then\\nmuch later in training validation accuracy improves, as shown in Figure 8a. This closely follows the\\nstereotypical grokking learning, ﬁrst observed in algorithmic datasets.\\nWith this setup, we also compute a phase diagram over the model weight decay and the last layer\\nlearning rate. See Figure 8b. While in MLPs it is less clear what parts of the network to consider\\nthe “encoder” vs the “decoder”, for our purposes here we consider the last layer to be the “decoder”\\nand vary its learning rate relative to the rest of the network. The resulting phase diagram has some\\nsimilarity to Figure 7. We observe a “confusion”phase in the bottom right (high learning rate and\\nhigh weight decay), a “comprehension” phase bordering it, a “grokking” phase as one decreases\\nweight decay and decoder learning rate, and a “memorization“ phase at low weight decay and low\\nlearning rate. Instead of an accuracy threshold of 95%, we use a threshold of 60% here for validation\\naccuracy for runs to count as comprehension or grokking. This phase diagram demonstrates that with\\nsufﬁcient regularization, we can again “de-grok” learning.\\nWe also investigate the effect of training set size on time to generalization on MNIST. We ﬁnd a result\\nsimilar to what Power et al. [ 1] observed, namely that generalization time increases rapidly once one\\ndrops below a certain amount of training data. See Figure 20.\\nK Lottery Ticket Hypothesis Connection\\nIn Figure 21, we show the projection of the learned embeddings after generalization to their ﬁrst\\ntwo principal components. Compared to the projection at initialization, structure clearly emerges in\\nembedding space when the neural network is able to generalize ( >99% validation accuracy). What\\nis intriguing is that the projection of the embeddings at initialization to the principal components\\nof the embeddings at generalization seem to already contain much of that structure. In this sense,\\n250 5000 10000 15000 20000 25000 30000\\nTrain Points104105Steps to Validation Accuracy > 60%Steps until generalization for MNIST (weight decay 5e-3)\\nMean\\nRuns that didn\\'t reach 60% val acc in 10^5 steps\\nRuns that reached 60% val acc in 10^5 stepsFigure 20: Time to generalize as a function of training set size, on MNIST.\\n−4−2 0 2 4\\nGeneralization PCA 1−4−2024Generalization PCA 2After generalization\\n−5.0−2.5 0.0 2.5 5.0 7.5\\nInitialization PCA 1−6−4−20246Initialization PCA 2At initialization\\n−4−2 0 2 4\\nGeneralization PCA 1−3−2−101234Generalization PCA 2At initialization\\nFigure 21: (Left) Input embeddings after generalization projected on their ﬁrst 2 principal compo-\\nnents. (Center) Input embeddings at initialization projected on their ﬁrst 2 principal components.\\n(Right) Input embeddings at initialization projected on the ﬁrst 2 principal components of the\\nembeddings after generalization at the end of training (same PCA as the left ﬁgure).\\nthe structured representation necessary for generalization already existed (partially) at initialization.\\nThe training procedure essentially prunes other unnecessary dimensions and forms the required\\nparallelograms for generalization. This is a nonstandard interpretation of the lottery ticket hypothesis\\nwhere the winning tickets are not weights or subnetworks but instead particular axes or linear\\ncombinations of the weights (the learned embeddings).\\nIn Figure 22, we show the original training curves (dashed lines). In solid lines, we recompute\\naccuracy with models which use embeddings that are projected onto the nprincipal components of\\nthe embeddings at the end of training (and back). Clearly, the ﬁrst few principal components contain\\nenough information to reach 99% accuracy. The ﬁrst few PCs explain the most variance by deﬁnition,\\nhowever, we note that this is not necessarily the main reason for why they can generalize so well. In\\nfact, embeddings reconstructed from the PCA at the end of training (solid lines) perform better than\\ncurrent highest variance axes (dotted line). This behavior is consistent across seeds.\\n26102103104\\nEpoch0.00.20.40.60.81.0Accuracy10 Components\\nTest Reconstructed (Final PCA)\\nTrain Reconstructed (Final PCA)\\nTrain Reconstructed (Current PCA)\\nTest Reconstructed (Current PCA)\\nTest\\nTrain\\n102103104\\nEpoch0.00.20.40.60.81.0Accuracy6 Components\\nTest Reconstructed (Final PCA)\\nTrain Reconstructed (Final PCA)\\nTrain Reconstructed (Current PCA)\\nTest Reconstructed (Current PCA)\\nTest\\nTrain\\n102103104\\nEpoch0.00.20.40.60.81.0Accuracy5 Components\\nTest Reconstructed (Final PCA)\\nTrain Reconstructed (Final PCA)\\nTrain Reconstructed (Current PCA)\\nTest Reconstructed (Current PCA)\\nTest\\nTrain\\n102103104\\nEpoch0.00.20.40.60.81.0Accuracy2 Components\\nTest Reconstructed (Final PCA)\\nTrain Reconstructed (Final PCA)\\nTrain Reconstructed (Current PCA)\\nTest Reconstructed (Current PCA)\\nTest\\nTrainFigure 22: Train and test accuracy computed while using actual embeddings (dashed line) and\\nembeddings projected onto and reconstructed from their ﬁrst nprincipal components (dotted lines)\\nand, ﬁnally, using embeddings projected onto and reconstructed from the ﬁrst nPCs of the embeddings\\nat the end of training (solid lines).\\n27L Derivation of the effective loss\\nIn this section, we will further motivate the use of our effective loss to study the dynamics of\\nrepresentation learning by deriving it from the gradient ﬂow dynamics on the actual MSE loss in\\nlinear regression. The loss landscape of a neural network is in general nonlinear, but the linear case\\nmay shed some light on how the effective loss can be derived from actual loss. For a sample r(which\\nis the sum of two embeddings EiandEj), the prediction of the linear network is D(r) =Ar+b.\\nThe loss function is ( yis its corresponding label):\\nℓ=1\\n2|Ar+b−y|2\\n\\ued19\\ued18\\ued17\\ued1a\\nℓpred+γ\\n2||A||2\\nF\\n\\ued19\\ued18\\ued17\\ued1a\\nℓreg, (35)\\nwhere the ﬁrst and the second term are prediction error and regularization, respectively. Both the\\nmodel (A,b)and the input rare updated via gradient ﬂow, with learning rate ηAandηx, respectively:\\ndA\\ndt=−ηA∂ℓ\\n∂A,db\\ndt=−ηA∂ℓ\\n∂b,dr\\ndt=−ηx∂ℓ\\n∂r. (36)\\nInsertingℓinto the above equations, we obtain the gradient ﬂow:\\ndA\\ndt=−ηA∂ℓ\\n∂A=−ηA[A(rrT+γ) + (b−y)rT],\\ndb\\ndt=−ηA∂ℓ\\n∂b=−ηA(Ar+b−y)\\ndr\\ndt=−ηx∂ℓ\\n∂r=−ηxAT(Ar+b−y).(37)\\nFor thedb/dtequation, after ignoring the Arterm and set the initial condition b(0) = 0, we obtain\\nanalytically b(t) = (1−e−2ηAt)y. Inserting this into the ﬁrst and third equations, we have\\ndA\\ndt=−ηA[A(rrT+γ)−e−2ηAtyrT],\\ndr\\ndt=−ηxATAr\\ued19\\ued18\\ued17\\ued1a\\ninternal interaction+ηxe−2ηAtATy\\ued19\\ued18\\ued17\\ued1a\\nexternal force.(38)\\nFor the second equation on the evolution of dr/dt, we can artiﬁcially decompose the right hand side\\ninto two terms, based on whether they depend on the label y. In this way, we call the ﬁrst term\\n\"internal interaction\" since it does not depend on y, while the second term \"external force\". Note\\nthis distinction seems a bit artiﬁcial from a mathematical perspective, but it can be conceptually\\nhelpful from a physics perspective. We will show below the internal interaction term is important for\\nrepresentations to form. Because we are interested in how two samples interact, we now consider\\nanother sample at r′, and the evolution becomes\\ndA\\ndt=−ηA[A(rrT+r′r′T+ 2γ)−e−2ηAty(r+r′)T],\\ndr\\ndt=−ηxATAr+ηxe−2ηAtATy,\\ndr′\\ndt=−ηxATAr′+ηxe−2ηAtATy.(39)\\nSubtractingdr/dtbydr′/dtand setting r′=−r, the above equations further simply to\\ndA\\ndt=−2ηAA(rrT+γ),\\ndr\\ndt=−ηxATAr.(40)\\nThe second equation implies that the pair of samples interact via a quadratic potential U(r) =\\n1\\n2rTATAr, leading to a linear attractive force f(r)∝r. We now consider the adiabatic limit where\\nηA→0.\\n28The adiabatic limit Using the standard initialization (e.g., Xavier initialization) of neural networks,\\nwe have AT\\n0A0≈I. As a result, the quadratic potential becomes U(r) =1\\n2rTr, which is time-\\nindependent because ηA→0. We are now in the position to analyze the addition problem. For two\\nsamples x(1)=Ei+Ejandx(2)=Em+Enwith the same label ( i+j=m+n), they contribute\\nto an interaction term\\nU(i,j,m,n ) =1\\n2|Ei+Ej−Em−En|2\\n2. (41)\\nAveraging over all possible quadruples in the training dataset D, the total energy of the system is\\nℓ0=∑\\n(i,j,m,n )∈P0(D)1\\n2|Ei+Ej−Em−En|2\\n2/|P0(D)|, (42)\\nwhereP0(D) ={(i,j,m,n )|i+j=m+n,(i,j)∈D,(m,n)∈D}. To make it scale-invariant,\\nwe deﬁne the normalized Hamiltonian Eq. (42) as\\nℓeﬀ=ℓ0\\nZ0, Z 0=∑\\ni|Ei|2\\n2 (43)\\nwhich is the effective loss we used in Section 3.2.\\n29'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_contents[\"6 Conclusion\"] #TODO: process this to ONLY the abstract content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
