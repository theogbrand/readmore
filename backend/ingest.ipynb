{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-text-splitters\n",
      "  Using cached langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.28 in ./venv/lib/python3.9/site-packages (from langchain-text-splitters) (0.1.44)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./venv/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./venv/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./venv/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (0.1.48)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./venv/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./venv/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.7.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./venv/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (8.2.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./venv/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (3.10.1)\n",
      "Requirement already satisfied: requests<3,>=2 in ./venv/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.18.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.9/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.9/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.9/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2024.2.2)\n",
      "Using cached langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: langchain-text-splitters\n",
      "Successfully installed langchain-text-splitters-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install pinecone-client\n",
    "# %pip install python-dotenv\n",
    "# %pip install openai\n",
    "# %pip install chromadb-client\n",
    "# %pip install cohere\n",
    "# %pip install pypdf\n",
    "# %pip install langchain-community\n",
    "# %pip install -U langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyenv _lzma import error when importing cohere/others\n",
    "# CFLAGS=\"-I$(brew --prefix xz)/include\" LDFLAGS=\"-L$(brew --prefix xz)/lib\" pyenv install 3.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI, AzureOpenAI\n",
    "import os\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pinecone import Pinecone\n",
    "\n",
    "# pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "# index = pc.Index(\"test-pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "\n",
    "# response = client.embeddings.create(\n",
    "#     input=\"Your text string goes here\",\n",
    "#     model=\"text-embedding-3-large\"\n",
    "# )\n",
    "\n",
    "# print(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version = \"2024-02-01\",\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=\"Your text string goes here\",\n",
    "    model=\"ada_gcal\"\n",
    ")\n",
    "\n",
    "print(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text, model=\"ada_gcal\"): # model = \"deployment_name\"\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "# df_bills['ada_v2'] = df_bills[\"text\"].apply(lambda x : generate_embeddings (x, model = 'text-embedding-ada-002'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"./pdf/Attention.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 20231 Introduction\n",
      "Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks [ 21] and conditional\n",
      "computation [ 32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequenceMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\n",
      "of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next.\n",
      "2Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [ 11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512 .\n",
      "Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\n",
      "the matrix of outputs as:\n",
      "Attention( Q, K, V ) = softmax(QKT\n",
      "√dk)V (1)\n",
      "The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of1√dk. Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dkthe two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk[3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneficial to linearly project the queries, keys and values htimes with different, learned\n",
      "linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "4To illustrate why the dot products get large, assume that the components of qandkare independent random\n",
      "variables with mean 0and variance 1. Then their dot product, q·k=Pdk\n",
      "i=1qiki, has mean 0and variance dk.\n",
      "4output values. These are concatenated and once again projected, resulting in the final values, as\n",
      "depicted in Figure 2.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation\n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\n",
      "where head i= Attention( QWQ\n",
      "i, KWK\n",
      "i, V WV\n",
      "i)\n",
      "Where the projections are parameter matrices WQ\n",
      "i∈Rdmodel×dk,WK\n",
      "i∈Rdmodel×dk,WV\n",
      "i∈Rdmodel×dv\n",
      "andWO∈Rhdv×dmodel.\n",
      "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
      "dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.\n",
      "3.2.3 Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information flow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN( x) = max(0 , xW 1+b1)W2+b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\n",
      "dff= 2048 .\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      "mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
      "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
      "linear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\n",
      "5Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. nis the sequence length, dis the representation dimension, kis the kernel\n",
      "size of convolutions and rthe size of the neighborhood in restricted self-attention.\n",
      "Layer Type Complexity per Layer Sequential Maximum Path Length\n",
      "Operations\n",
      "Self-Attention O(n2·d) O(1) O(1)\n",
      "Recurrent O(n·d2) O(n) O(n)\n",
      "Convolutional O(k·n·d2) O(1) O(logk(n))\n",
      "Self-Attention (restricted) O(r·n·d) O(1) O(n/r)\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
      "learned and fixed [9].\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "PE(pos,2i)=sin(pos/100002i/d model)\n",
      "PE(pos,2i+1)=cos(pos/100002i/d model)\n",
      "where posis the position and iis the dimension. That is, each dimension of the positional encoding\n",
      "corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\n",
      "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
      "relative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\n",
      "PEpos.\n",
      "We also experimented with using learned positional embeddings [ 9] instead, and found that the two\n",
      "versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
      "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
      "during training.\n",
      "4 Why Self-Attention\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
      "tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
      "(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      "consider three desiderata.\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can\n",
      "be parallelized, as measured by the minimum number of sequential operations required.\n",
      "The third is the path length between long-range dependencies in the network. Learning long-range\n",
      "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
      "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
      "traverse in the network. The shorter these paths between any combination of positions in the input\n",
      "and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\n",
      "the maximum path length between any two input and output positions in networks composed of the\n",
      "different layer types.\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
      "executed operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\n",
      "computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "6length nis smaller than the representation dimensionality d, which is most often the case with\n",
      "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
      "[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\n",
      "very long sequences, self-attention could be restricted to considering only a neighborhood of size rin\n",
      "the input sequence centered around the respective output position. This would increase the maximum\n",
      "path length to O(n/r). We plan to investigate this approach further in future work.\n",
      "A single convolutional layer with kernel width k < n does not connect all pairs of input and output\n",
      "positions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\n",
      "orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\n",
      "between any two positions in the network. Convolutional layers are generally more expensive than\n",
      "recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\n",
      "considerably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\n",
      "convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\n",
      "the approach we take in our model.\n",
      "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\n",
      "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
      "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
      "and semantic structure of the sentences.\n",
      "5 Training\n",
      "This section describes the training regime for our models.\n",
      "5.1 Training Data and Batching\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
      "sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\n",
      "target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n",
      "2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\n",
      "vocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\n",
      "batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n",
      "target tokens.\n",
      "5.2 Hardware and Schedule\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\n",
      "the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\n",
      "trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n",
      "bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
      "(3.5 days).\n",
      "5.3 Optimizer\n",
      "We used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\n",
      "rate over the course of training, according to the formula:\n",
      "lrate =d−0.5\n",
      "model·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\n",
      "This corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\n",
      "and decreasing it thereafter proportionally to the inverse square root of the step number. We used\n",
      "warmup _steps = 4000 .\n",
      "5.4 Regularization\n",
      "We employ three types of regularization during training:\n",
      "7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      "English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "ModelBLEU Training Cost (FLOPs)\n",
      "EN-DE EN-FR EN-DE EN-FR\n",
      "ByteNet [18] 23.75\n",
      "Deep-Att + PosUnk [39] 39.2 1.0·1020\n",
      "GNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\n",
      "ConvS2S [9] 25.16 40.46 9.6·10181.5·1020\n",
      "MoE [32] 26.03 40.56 2.0·10191.2·1020\n",
      "Deep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\n",
      "GNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\n",
      "ConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\n",
      "Transformer (base model) 27.3 38.1 3.3·1018\n",
      "Transformer (big) 28.4 41.8 2.3·1019\n",
      "Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\n",
      "sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
      "positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
      "Pdrop= 0.1.\n",
      "Label Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\n",
      "hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "6 Results\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\n",
      "BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\n",
      "listed in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\n",
      "surpasses all previously published models and ensembles, at a fraction of the training cost of any of\n",
      "the competitive models.\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\n",
      "outperforming all of the previously published single models, at less than 1/4the training cost of the\n",
      "previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\n",
      "dropout rate Pdrop= 0.1, instead of 0.3.\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\n",
      "were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\n",
      "used beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\n",
      "were chosen after experimentation on the development set. We set the maximum output length during\n",
      "inference to input length + 50, but terminate early when possible [38].\n",
      "Table 2 summarizes our results and compares our translation quality and training costs to other model\n",
      "architectures from the literature. We estimate the number of floating point operations used to train a\n",
      "model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\n",
      "single-precision floating-point capacity of each GPU5.\n",
      "6.2 Model Variations\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model\n",
      "in different ways, measuring the change in performance on English-to-German translation on the\n",
      "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "8Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\n",
      "model. All metrics are on the English-to-German translation development set, newstest2013. Listed\n",
      "perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\n",
      "per-word perplexities.\n",
      "N d model dff h d k dvPdrop ϵlstrain PPL BLEU params\n",
      "steps (dev) (dev) ×106\n",
      "base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n",
      "(A)1 512 512 5.29 24.9\n",
      "4 128 128 5.00 25.5\n",
      "16 32 32 4.91 25.8\n",
      "32 16 16 5.01 25.4\n",
      "(B)16 5.16 25.1 58\n",
      "32 5.01 25.4 60\n",
      "(C)2 6.11 23.7 36\n",
      "4 5.19 25.3 50\n",
      "8 4.88 25.5 80\n",
      "256 32 32 5.75 24.5 28\n",
      "1024 128 128 4.66 26.0 168\n",
      "1024 5.12 25.4 53\n",
      "4096 4.75 26.2 90\n",
      "(D)0.0 5.77 24.6\n",
      "0.2 4.95 25.5\n",
      "0.0 4.67 25.3\n",
      "0.2 5.47 25.7\n",
      "(E) positional embedding instead of sinusoids 4.92 25.7\n",
      "big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no\n",
      "checkpoint averaging. We present these results in Table 3.\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\n",
      "keeping the amount of computation constant, as described in Section 3.2.2. While single-head\n",
      "attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\n",
      "bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\n",
      "results to the base model.\n",
      "6.3 English Constituency Parsing\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English\n",
      "constituency parsing. This task presents specific challenges: the output is subject to strong structural\n",
      "constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\n",
      "models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\n",
      "Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\n",
      "using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n",
      "[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\n",
      "for the semi-supervised setting.\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual\n",
      "(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base translation model. During inference, we\n",
      "9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
      "of WSJ)\n",
      "Parser Training WSJ 23 F1\n",
      "Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\n",
      "Petrov et al. (2006) [29] WSJ only, discriminative 90.4\n",
      "Zhu et al. (2013) [40] WSJ only, discriminative 90.4\n",
      "Dyer et al. (2016) [8] WSJ only, discriminative 91.7\n",
      "Transformer (4 layers) WSJ only, discriminative 91.3\n",
      "Zhu et al. (2013) [40] semi-supervised 91.3\n",
      "Huang & Harper (2009) [14] semi-supervised 91.3\n",
      "McClosky et al. (2006) [26] semi-supervised 92.1\n",
      "Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\n",
      "Transformer (4 layers) semi-supervised 92.7\n",
      "Luong et al. (2015) [23] multi-task 93.0\n",
      "Dyer et al. (2016) [8] generative 93.3\n",
      "increased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\n",
      "for both WSJ only and the semi-supervised setting.\n",
      "Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\n",
      "prisingly well, yielding better results than all previously reported models with the exception of the\n",
      "Recurrent Neural Network Grammar [8].\n",
      "In contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\n",
      "Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based\n",
      "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
      "English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
      "model outperforms even all previously reported ensembles.\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We\n",
      "plan to extend the Transformer to problems involving input and output modalities other than text and\n",
      "to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\n",
      "such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "The code we used to train and evaluate our models is available at https://github.com/\n",
      "tensorflow/tensor2tensor .\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "References\n",
      "[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
      "arXiv:1607.06450 , 2016.\n",
      "[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      "learning to align and translate. CoRR , abs/1409.0473, 2014.\n",
      "[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\n",
      "machine translation architectures. CoRR , abs/1703.03906, 2017.\n",
      "[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\n",
      "reading. arXiv preprint arXiv:1601.06733 , 2016.\n",
      "10[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\n",
      "and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\n",
      "machine translation. CoRR , abs/1406.1078, 2014.\n",
      "[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\n",
      "preprint arXiv:1610.02357 , 2016.\n",
      "[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\n",
      "of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\n",
      "[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\n",
      "network grammars. In Proc. of NAACL , 2016.\n",
      "[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\n",
      "tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\n",
      "[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\n",
      "arXiv:1308.0850 , 2013.\n",
      "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\n",
      "age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition , pages 770–778, 2016.\n",
      "[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\n",
      "recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      "[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\n",
      "9(8):1735–1780, 1997.\n",
      "[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\n",
      "across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\n",
      "Language Processing , pages 832–841. ACL, August 2009.\n",
      "[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
      "the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n",
      "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\n",
      "Information Processing Systems, (NIPS) , 2016.\n",
      "[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\n",
      "on Learning Representations (ICLR) , 2016.\n",
      "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\n",
      "ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\n",
      "2017.\n",
      "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "InInternational Conference on Learning Representations , 2017.\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n",
      "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "arXiv:1703.10722 , 2017.\n",
      "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "arXiv:1703.03130 , 2017.\n",
      "[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\n",
      "sequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\n",
      "[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
      "based neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n",
      "11[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      "corpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\n",
      "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\n",
      "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\n",
      "pages 152–159. ACL, June 2006.\n",
      "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "model. In Empirical Methods in Natural Language Processing , 2016.\n",
      "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
      "summarization. arXiv preprint arXiv:1705.04304 , 2017.\n",
      "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\n",
      "and interpretable tree annotation. In Proceedings of the 21st International Conference on\n",
      "Computational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\n",
      "2006.\n",
      "[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\n",
      "preprint arXiv:1608.05859 , 2016.\n",
      "[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\n",
      "with subword units. arXiv preprint arXiv:1508.07909 , 2015.\n",
      "[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
      "and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\n",
      "layer. arXiv preprint arXiv:1701.06538 , 2017.\n",
      "[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\n",
      "nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\n",
      "Learning Research , 15(1):1929–1958, 2014.\n",
      "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\n",
      "networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\n",
      "Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\n",
      "Inc., 2015.\n",
      "[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\n",
      "networks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\n",
      "[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
      "Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n",
      "[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\n",
      "Advances in Neural Information Processing Systems , 2015.\n",
      "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\n",
      "translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      "arXiv:1609.08144 , 2016.\n",
      "[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\n",
      "fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n",
      "[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\n",
      "shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n",
      "1: Long Papers) , pages 434–443. ACL, August 2013.\n",
      "12Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
      "encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
      "the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\n",
      "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
      "13Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(pages))\n",
    "print()\n",
    "\n",
    "full_document = \"\"\n",
    "\n",
    "for page in pages:\n",
    "  full_document += page.page_content\n",
    "\n",
    "print(full_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "\n",
      "Attention Is All You Need\n",
      "\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n",
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks [ 21] and conditional\n",
      "computation [ 32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequenceMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\n",
      "of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next.\n",
      "2Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [ 11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512 .\n",
      "Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\n",
      "the matrix of outputs as:\n",
      "Attention( Q, K, V ) = softmax(QKT\n",
      "√dk)V (1)\n",
      "The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of1√dk. Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dkthe two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk[3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneficial to linearly project the queries, keys and values htimes with different, learned\n",
      "linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "4To illustrate why the dot products get large, assume that the components of qandkare independent random\n",
      "variables with mean 0and variance 1. Then their dot product, q·k=Pdk\n",
      "i=1qiki, has mean 0and variance dk.\n",
      "4output values. These are concatenated and once again projected, resulting in the final values, as\n",
      "depicted in Figure 2.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation\n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\n",
      "where head i= Attention( QWQ\n",
      "i, KWK\n",
      "i, V WV\n",
      "i)\n",
      "Where the projections are parameter matrices WQ\n",
      "i∈Rdmodel×dk,WK\n",
      "i∈Rdmodel×dk,WV\n",
      "i∈Rdmodel×dv\n",
      "andWO∈Rhdv×dmodel.\n",
      "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
      "dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.\n",
      "3.2.3 Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information flow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN( x) = max(0 , xW 1+b1)W2+b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\n",
      "dff= 2048 .\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      "mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
      "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
      "linear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\n",
      "5Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. nis the sequence length, dis the representation dimension, kis the kernel\n",
      "size of convolutions and rthe size of the neighborhood in restricted self-attention.\n",
      "Layer Type Complexity per Layer Sequential Maximum Path Length\n",
      "Operations\n",
      "Self-Attention O(n2·d) O(1) O(1)\n",
      "Recurrent O(n·d2) O(n) O(n)\n",
      "Convolutional O(k·n·d2) O(1) O(logk(n))\n",
      "Self-Attention (restricted) O(r·n·d) O(1) O(n/r)\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
      "learned and fixed [9].\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "PE(pos,2i)=sin(pos/100002i/d model)\n",
      "PE(pos,2i+1)=cos(pos/100002i/d model)\n",
      "where posis the position and iis the dimension. That is, each dimension of the positional encoding\n",
      "corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\n",
      "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
      "relative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\n",
      "PEpos.\n",
      "We also experimented with using learned positional embeddings [ 9] instead, and found that the two\n",
      "versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
      "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
      "during training.\n",
      "4 Why Self Attention\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
      "tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
      "(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      "consider three desiderata.\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can\n",
      "be parallelized, as measured by the minimum number of sequential operations required.\n",
      "The third is the path length between long-range dependencies in the network. Learning long-range\n",
      "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
      "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
      "traverse in the network. The shorter these paths between any combination of positions in the input\n",
      "and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\n",
      "the maximum path length between any two input and output positions in networks composed of the\n",
      "different layer types.\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
      "executed operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\n",
      "computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "6length nis smaller than the representation dimensionality d, which is most often the case with\n",
      "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
      "[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\n",
      "very long sequences, self-attention could be restricted to considering only a neighborhood of size rin\n",
      "the input sequence centered around the respective output position. This would increase the maximum\n",
      "path length to O(n/r). We plan to investigate this approach further in future work.\n",
      "A single convolutional layer with kernel width k < n does not connect all pairs of input and output\n",
      "positions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\n",
      "orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\n",
      "between any two positions in the network. Convolutional layers are generally more expensive than\n",
      "recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\n",
      "considerably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\n",
      "convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\n",
      "the approach we take in our model.\n",
      "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\n",
      "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
      "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
      "and semantic structure of the sentences.\n",
      "5 Training\n",
      "This section describes the training regime for our models.\n",
      "5.1 Training Data and Batching\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
      "sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\n",
      "target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n",
      "2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\n",
      "vocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\n",
      "batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n",
      "target tokens.\n",
      "5.2 Hardware and Schedule\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\n",
      "the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\n",
      "trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n",
      "bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
      "(3.5 days).\n",
      "5.3 Optimizer\n",
      "We used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\n",
      "rate over the course of training, according to the formula:\n",
      "lrate =d−0.5\n",
      "model·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\n",
      "This corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\n",
      "and decreasing it thereafter proportionally to the inverse square root of the step number. We used\n",
      "warmup _steps = 4000 .\n",
      "5.4 Regularization\n",
      "We employ three types of regularization during training:\n",
      "7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      "English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "ModelBLEU Training Cost (FLOPs)\n",
      "EN-DE EN-FR EN-DE EN-FR\n",
      "ByteNet [18] 23.75\n",
      "Deep-Att + PosUnk [39] 39.2 1.0·1020\n",
      "GNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\n",
      "ConvS2S [9] 25.16 40.46 9.6·10181.5·1020\n",
      "MoE [32] 26.03 40.56 2.0·10191.2·1020\n",
      "Deep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\n",
      "GNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\n",
      "ConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\n",
      "Transformer (base model) 27.3 38.1 3.3·1018\n",
      "Transformer (big) 28.4 41.8 2.3·1019\n",
      "Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\n",
      "sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
      "positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
      "Pdrop= 0.1.\n",
      "Label Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\n",
      "hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "6 Results\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\n",
      "BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\n",
      "listed in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\n",
      "surpasses all previously published models and ensembles, at a fraction of the training cost of any of\n",
      "the competitive models.\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\n",
      "outperforming all of the previously published single models, at less than 1/4the training cost of the\n",
      "previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\n",
      "dropout rate Pdrop= 0.1, instead of 0.3.\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\n",
      "were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\n",
      "used beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\n",
      "were chosen after experimentation on the development set. We set the maximum output length during\n",
      "inference to input length + 50, but terminate early when possible [38].\n",
      "Table 2 summarizes our results and compares our translation quality and training costs to other model\n",
      "architectures from the literature. We estimate the number of floating point operations used to train a\n",
      "model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\n",
      "single-precision floating-point capacity of each GPU5.\n",
      "6.2 Model Variations\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model\n",
      "in different ways, measuring the change in performance on English-to-German translation on the\n",
      "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "8Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\n",
      "model. All metrics are on the English-to-German translation development set, newstest2013. Listed\n",
      "perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\n",
      "per-word perplexities.\n",
      "N d model dff h d k dvPdrop ϵlstrain PPL BLEU params\n",
      "steps (dev) (dev) ×106\n",
      "base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n",
      "(A)1 512 512 5.29 24.9\n",
      "4 128 128 5.00 25.5\n",
      "16 32 32 4.91 25.8\n",
      "32 16 16 5.01 25.4\n",
      "(B)16 5.16 25.1 58\n",
      "32 5.01 25.4 60\n",
      "(C)2 6.11 23.7 36\n",
      "4 5.19 25.3 50\n",
      "8 4.88 25.5 80\n",
      "256 32 32 5.75 24.5 28\n",
      "1024 128 128 4.66 26.0 168\n",
      "1024 5.12 25.4 53\n",
      "4096 4.75 26.2 90\n",
      "(D)0.0 5.77 24.6\n",
      "0.2 4.95 25.5\n",
      "0.0 4.67 25.3\n",
      "0.2 5.47 25.7\n",
      "(E) positional embedding instead of sinusoids 4.92 25.7\n",
      "big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no\n",
      "checkpoint averaging. We present these results in Table 3.\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\n",
      "keeping the amount of computation constant, as described in Section 3.2.2. While single-head\n",
      "attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\n",
      "bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\n",
      "results to the base model.\n",
      "6.3 English Constituency Parsing\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English\n",
      "constituency parsing. This task presents specific challenges: the output is subject to strong structural\n",
      "constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\n",
      "models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\n",
      "Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\n",
      "using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n",
      "[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\n",
      "for the semi-supervised setting.\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual\n",
      "(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base translation model. During inference, we\n",
      "9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
      "of WSJ)\n",
      "Parser Training WSJ 23 F1\n",
      "Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\n",
      "Petrov et al. (2006) [29] WSJ only, discriminative 90.4\n",
      "Zhu et al. (2013) [40] WSJ only, discriminative 90.4\n",
      "Dyer et al. (2016) [8] WSJ only, discriminative 91.7\n",
      "Transformer (4 layers) WSJ only, discriminative 91.3\n",
      "Zhu et al. (2013) [40] semi-supervised 91.3\n",
      "Huang & Harper (2009) [14] semi-supervised 91.3\n",
      "McClosky et al. (2006) [26] semi-supervised 92.1\n",
      "Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\n",
      "Transformer (4 layers) semi-supervised 92.7\n",
      "Luong et al. (2015) [23] multi-task 93.0\n",
      "Dyer et al. (2016) [8] generative 93.3\n",
      "increased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\n",
      "for both WSJ only and the semi-supervised setting.\n",
      "Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\n",
      "prisingly well, yielding better results than all previously reported models with the exception of the\n",
      "Recurrent Neural Network Grammar [8].\n",
      "In contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\n",
      "Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based\n",
      "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
      "English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
      "model outperforms even all previously reported ensembles.\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We\n",
      "plan to extend the Transformer to problems involving input and output modalities other than text and\n",
      "to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\n",
      "such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "The code we used to train and evaluate our models is available at https://github.com/\n",
      "tensorflow/tensor2tensor .\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "References\n",
      "[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
      "arXiv:1607.06450 , 2016.\n",
      "[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      "learning to align and translate. CoRR , abs/1409.0473, 2014.\n",
      "[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\n",
      "machine translation architectures. CoRR , abs/1703.03906, 2017.\n",
      "[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\n",
      "reading. arXiv preprint arXiv:1601.06733 , 2016.\n",
      "10[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\n",
      "and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\n",
      "machine translation. CoRR , abs/1406.1078, 2014.\n",
      "[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\n",
      "preprint arXiv:1610.02357 , 2016.\n",
      "[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\n",
      "of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\n",
      "[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\n",
      "network grammars. In Proc. of NAACL , 2016.\n",
      "[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\n",
      "tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\n",
      "[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\n",
      "arXiv:1308.0850 , 2013.\n",
      "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\n",
      "age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition , pages 770–778, 2016.\n",
      "[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\n",
      "recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      "[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\n",
      "9(8):1735–1780, 1997.\n",
      "[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\n",
      "across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\n",
      "Language Processing , pages 832–841. ACL, August 2009.\n",
      "[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
      "the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n",
      "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\n",
      "Information Processing Systems, (NIPS) , 2016.\n",
      "[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\n",
      "on Learning Representations (ICLR) , 2016.\n",
      "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\n",
      "ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\n",
      "2017.\n",
      "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "InInternational Conference on Learning Representations , 2017.\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n",
      "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "arXiv:1703.10722 , 2017.\n",
      "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "arXiv:1703.03130 , 2017.\n",
      "[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\n",
      "sequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\n",
      "[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
      "based neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n",
      "11[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      "corpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\n",
      "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\n",
      "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\n",
      "pages 152–159. ACL, June 2006.\n",
      "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "model. In Empirical Methods in Natural Language Processing , 2016.\n",
      "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
      "summarization. arXiv preprint arXiv:1705.04304 , 2017.\n",
      "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\n",
      "and interpretable tree annotation. In Proceedings of the 21st International Conference on\n",
      "Computational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\n",
      "2006.\n",
      "[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\n",
      "preprint arXiv:1608.05859 , 2016.\n",
      "[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\n",
      "with subword units. arXiv preprint arXiv:1508.07909 , 2015.\n",
      "[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
      "and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\n",
      "layer. arXiv preprint arXiv:1701.06538 , 2017.\n",
      "[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\n",
      "nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\n",
      "Learning Research , 15(1):1929–1958, 2014.\n",
      "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\n",
      "networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\n",
      "Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\n",
      "Inc., 2015.\n",
      "[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\n",
      "networks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\n",
      "[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
      "Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n",
      "[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\n",
      "Advances in Neural Information Processing Systems , 2015.\n",
      "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\n",
      "translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      "arXiv:1609.08144 , 2016.\n",
      "[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\n",
      "fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n",
      "[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\n",
      "shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n",
      "1: Long Papers) , pages 434–443. ACL, August 2013.\n",
      "12Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
      "encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
      "the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\n",
      "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
      "13Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('proc/attention.txt', 'r') as file:\n",
    "    edited_text = file.read()\n",
    "\n",
    "print(edited_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract\n",
      "\n",
      "1 Introduction\n",
      "\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "\n",
      "3 Model Architecture\n",
      "\n",
      "4 Why Self Attention\n",
      "\n",
      "5 Training\n",
      "\n",
      "6 Results\n",
      "\n",
      "7 Conclusion\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# first, extract Section Headers\n",
    "pattern = r\"\\n\\d+ [A-Z][a-zA-Z\\s:]+\\n\"\n",
    "\n",
    "# Find all matches\n",
    "matches = re.findall(pattern, edited_text)\n",
    "\n",
    "matches.insert(0, \"Abstract\")\n",
    "\n",
    "# Print all section titles\n",
    "for match in matches:\n",
    "    print(match.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abstract',\n",
       " '\\n1 Introduction\\n',\n",
       " '\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n',\n",
       " '\\n3 Model Architecture\\n',\n",
       " '\\n4 Why Self Attention\\n',\n",
       " '\\n5 Training\\n',\n",
       " '\\n6 Results\\n',\n",
       " '\\n7 Conclusion\\n']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Abstract', '1 Introduction', '2 Problem Setting', '3 Why Generalization Occurs: Representations and Dynamics', '4 Delayed Generalization: A Phase Diagram', '5 Related work', '6 Conclusion'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"\\n\\d+ [A-Z][a-zA-Z\\s:]+\\n\"\n",
    "\n",
    "# Split the text into sections\n",
    "sections = re.split(pattern, full_document)\n",
    "\n",
    "# number of sections should match with number of section titles (aka matches of regex pattern)\n",
    "# print(sections[0])\n",
    "# print(sections[2])\n",
    "# print(len(sections))\n",
    "\n",
    "# Create a dictionary to store section titles and contents\n",
    "section_contents = {}\n",
    "\n",
    "# Use zip to iterate over matches and sections simultaneously\n",
    "for match, section in zip(matches, sections):\n",
    "    section_title = match.strip()\n",
    "    content = section.strip()\n",
    "    section_contents[section_title] = content\n",
    "\n",
    "section_contents.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We have shown how, in both toy models and general settings, that representation enables generalization\\nwhen it reﬂects structure in the data. We developed an effective theory of representation learning\\ndynamics (in a toy setting) which predicts the critical dependence of learning on the training data\\nfraction. We then presented four learning phases (comprehension, grokking, memorization and\\nconfusion) which depend on the decoder capacity and learning speed (given by, among other things,\\nlearning rate and weight decay) in decoder-only architectures. While we have mostly focused on a\\ntoy model, we ﬁnd preliminary evidence that our results generalize to the setting of [1].\\nOur work can be viewed as a step towards a statistical physics of deep learning , connecting the\\n“microphysics” of low-level network dynamics with the “thermodynamics” of high-level model\\nbehavior. We view the application of theoretical tools from physics, such as effective theories [ 24], to\\nbe a rich area for further work. The broader impact of such work, if successful, could be to make\\nmodels more transparent and predictable [ 23,25,26], crucial to the task of ensuring the safety of\\nadvanced AI systems.\\n10References\\n[1]Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Gen-\\neralization beyond overﬁtting on small algorithmic datasets. arXiv preprint arXiv:2201.02177 ,\\n2022.\\n[2]Neel Nanda and Tom Lieberum. A mechanistic interpretability analysis of\\ngrokking, 2022. URL https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/\\na-mechanistic-interpretability-analysis-of-grokking .\\n[3]Beren Millidge. Grokking ’grokking’. https://beren.io/\\n2022-01-11-Grokking-Grokking/ , 2022.\\n[4]Rohin Shah. Alignment Newsletter #159. https:\\n//www.alignmentforum.org/posts/zvWqPmQasssaAWkrj/\\nan-159-building-agents-that-know-how-to-experiment-by#DEEP_LEARNING_ ,\\n2021.\\n[5] Yedid Hoshen and Shmuel Peleg. Visual learning of arithmetic operation. In AAAI , 2016.\\n[6]Yang-Hui He. Machine-learning mathematical structures. arXiv preprint arXiv:2101.06317 ,\\n2021.\\n[7]Sergei Gukov, James Halverson, Fabian Ruehle, and Piotr Sułkowski. Learning to unknot.\\nMachine Learning: Science and Technology , 2(2):025035, 2021.\\n[8]Alex Davies, Petar Veli ˇckovi ´c, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomašev,\\nRichard Tanburn, Peter Battaglia, Charles Blundell, András Juhász, et al. Advancing mathemat-\\nics by guiding human intuition with ai. Nature , 600(7887):70–74, 2021.\\n[9]Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.\\nDeep double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics:\\nTheory and Experiment , 2021(12):124003, 2021.\\n[10] Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal regularization can\\nmitigate double descent. arXiv preprint arXiv:2003.01897 , 2020.\\n[11] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and\\nnew perspectives. IEEE transactions on pattern analysis and machine intelligence , 35(8):\\n1798–1828, 2013.\\n[12] Yassine Ouali, Céline Hudelot, and Myriam Tami. An overview of deep semi-supervised\\nlearning. arXiv preprint arXiv:2006.05278 , 2020.\\n[13] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena\\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\\net al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural\\nInformation Processing Systems , 33:21271–21284, 2020.\\n[14] Phuc H Le-Khac, Graham Healy, and Alan F Smeaton. Contrastive representation learning: A\\nframework and review. IEEE Access , 8:193907–193934, 2020.\\n[15] James Halverson, Anindita Maiti, and Keegan Stoner. Neural networks and quantum ﬁeld\\ntheory. Machine Learning: Science and Technology , 2(3):035002, 2021.\\n[16] Daniel A Roberts, Sho Yaida, and Boris Hanin. The principles of deep learning theory. arXiv\\npreprint arXiv:2106.10165 , 2021.\\n[17] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka.\\nNeural mechanics: Symmetry and broken conservation laws in deep learning dynamics. arXiv\\npreprint arXiv:2012.04728 , 2020.\\n[18] Yansong Gao and Pratik Chaudhari. A free-energy principle for representation learning. In\\nInternational Conference on Machine Learning , pages 3367–3376. PMLR, 2020.\\n11[19] Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mézard, and Lenka Zdeborová.\\nGeneralisation error in learning with random features and the hidden manifold model. In\\nInternational Conference on Machine Learning , pages 3452–3462. PMLR, 2020.\\n[20] Mohammad Pezeshki, Amartya Mitra, Yoshua Bengio, and Guillaume Lajoie. Multi-scale\\nfeature learning dynamics: Insights for double descent. In International Conference on Machine\\nLearning , pages 17669–17690. PMLR, 2022.\\n[21] Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc Mezard, and Lenka\\nZdeborova. The gaussian equivalence of generative models for learning with shallow neural\\nnetworks. In Joan Bruna, Jan Hesthaven, and Lenka Zdeborova, editors, Proceedings of the\\n2nd Mathematical and Scientiﬁc Machine Learning Conference , volume 145 of Proceedings\\nof Machine Learning Research , pages 426–471. PMLR, 16–19 Aug 2022. URL https:\\n//proceedings.mlr.press/v145/goldt22a.html .\\n[22] R Kuhn and S Bos. Statistical mechanics for neural networks with continuous-time dynamics.\\nJournal of Physics A: Mathematical and General , 26(4):831, 1993.\\n[23] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom\\nHenighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain,\\nDeep Ganguli, Zac Hatﬁeld-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson\\nKernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,\\nSam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer\\nCircuits Thread , 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-\\nheads/index.html.\\n[24] Daniel A. Roberts, Sho Yaida, and Boris Hanin. The Principles of Deep Learning Theory .\\nCambridge University Press, 2022. https://deeplearningtheory.com .\\n[25] Deep Ganguli, Danny Hernandez, Liane Lovitt, Nova DasSarma, Tom Henighan, Andy Jones,\\nNicholas Joseph, Jackson Kernion, Ben Mann, Amanda Askell, et al. Predictability and surprise\\nin large generative models. arXiv preprint arXiv:2202.07785 , 2022.\\n[26] Jacob Steinhardt. Future ML Systems Will Be Qualitatively Different. https://www.\\nlesswrong.com/s/4aARF2ZoBpFZAhbbe/p/pZaPhGg2hmmPwByHc , 2022.\\n[27] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov,\\nand Alexander J Smola. Deep sets. Advances in neural information processing systems , 30,\\n2017.\\n[28] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the\\nterminal phase of deep learning training. Proceedings of the National Academy of Sciences , 117\\n(40):24652–24663, 2020.\\n[29] Wikipedia contributors. Thomson problem — Wikipedia, the free encyclope-\\ndia. https://en.wikipedia.org/w/index.php?title=Thomson_problem&oldid=\\n1091431454 , 2022. [Online; accessed 29-July-2022].\\n[30] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15750–15758,\\n2021.\\n[31] Zhi-Qin John Xu, Yaoyu Zhang, and Yanyang Xiao. Training behavior of deep neural network\\nin frequency domain. In International Conference on Neural Information Processing , pages\\n264–274. Springer, 2019.\\n[32] Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, and Zheng Ma. A type of generalization error induced\\nby initialization in deep neural networks. In Mathematical and Scientiﬁc Machine Learning ,\\npages 144–164. PMLR, 2020.\\n[33] Ziming Liu, Eric J. Michaud, and Max Tegmark. Omnigrok: Grokking beyond algorithmic data,\\n2022.\\n12[34] Blake Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay\\nGolan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models.\\nIn Jacob Abernethy and Shivani Agarwal, editors, Proceedings of Thirty Third Conference on\\nLearning Theory , volume 125 of Proceedings of Machine Learning Research , pages 3635–3673.\\nPMLR, 09–12 Jul 2020. URL https://proceedings.mlr.press/v125/woodworth20a.\\nhtml .\\nChecklist\\n1. For all authors...\\n(a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s\\ncontributions and scope? [Yes]\\n(b) Did you describe the limitations of your work? [Yes]\\n(c) Did you discuss any potential negative societal impacts of your work? [N/A]\\n(d)Have you read the ethics review guidelines and ensured that your paper conforms to\\nthem? [Yes]\\n2. If you are including theoretical results...\\n(a) Did you state the full set of assumptions of all theoretical results? [Yes]\\n(b) Did you include complete proofs of all theoretical results? [Yes]\\n3. If you ran experiments...\\n(a) Did you include the code, data, and instructions needed to reproduce the main experi-\\nmental results (either in the supplemental material or as a URL)? [Yes]\\n(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they\\nwere chosen)? [Yes]\\n(c)Did you report error bars (e.g., with respect to the random seed after running experi-\\nments multiple times)? [Yes]\\n(d)Did you include the total amount of compute and the type of resources used (e.g., type\\nof GPUs, internal cluster, or cloud provider)? [Yes] All experiments were run on a\\nworkstation with two NVIDIA A6000 GPUs within a few days.\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n(a) If your work uses existing assets, did you cite the creators? [N/A]\\n(b) Did you mention the license of the assets? [N/A]\\n(c)Did you include any new assets either in the supplemental material or as a URL? [N/A]\\n(d)Did you discuss whether and how consent was obtained from people whose data you’re\\nusing/curating? [N/A]\\n(e)Did you discuss whether the data you are using/curating contains personally identiﬁable\\ninformation or offensive content? [N/A]\\n5. If you used crowdsourcing or conducted research with human subjects...\\n(a)Did you include the full text of instructions given to participants and screenshots, if\\napplicable? [N/A]\\n(b)Did you describe any potential participant risks, with links to Institutional Review\\nBoard (IRB) approvals, if applicable? [N/A]\\n(c)Did you include the estimated hourly wage paid to participants and the total amount\\nspent on participant compensation? [N/A]\\n13Appendix\\nA Deﬁnitions of the phases of learning\\nTable 1: Deﬁnitions of the four phases of learning\\ncriteria\\nPhasetraining acc > 90%\\nwithin 105stepsvalidation acc > 90%\\nwithin 105stepsstep(validation acc>90%)\\n−step(training acc>90%)< 103\\nComprehension Yes Yes Yes\\nGrokking Yes Yes No\\nMemorization Yes No Not Applicable\\nConfusion No No Not Applicable\\nB Applicability of our toy setting\\nIn the main paper, we focused on the toy setting with (1) the addition dataset and (2) the addition\\noperation hard coded in the decoder. Although both simpliﬁcations appear to have quite limited\\napplicability, we argue below that the analysis of the toy setting can actually apply to all Abelian\\ngroups.\\nThe addition dataset is the building block of all Abelian groups A cyclic group is a group that\\nis generated by a single element. A ﬁnite cyclic group with order nisCn={e,g,g2,···,gn−1}\\nwhereeis the identify element and gis the generator and gi=gjwheneveri=j(modn). The\\nmodulo addition and {0,1,···,n−1}form a cyclic group with e= 0andgcan be any number q\\ncoprime tonsuch that (q,n) = 1 . Since algorithmic datasets contain only symbolic but no arithmetic\\ninformation, the datasets of modulo addition could apply to all other cyclic groups, e.g., modulo\\nmultiplication and discrete rotation groups in 2D.\\nAlthough not all Abelian groups are cyclic, a ﬁnite Abelian group Gcan be always decomposed into\\na direct product of kcyclic groups G=Cn1×Cn2···Cnk. So after training kneural networks with\\neach handling one cyclic group separately, it is easy to construct a larger neural network that handles\\nthe whole Abelian group.\\nThe addition operation is valid for all Abelian groups It is proved in [ 27] that for a permutation\\ninvariant function f(x1,x2,···,xn), there exists ρandφsuch that\\nf(x1,x2,···,xn) =ρ[n∑\\ni=1φ(xi)], (10)\\norf(x1,x2) =ρ(φ(x1) +φ(x2))forn= 2. Notice that φ(xi)corresponds to the embedding vector\\nEi,ρcorresponds to the decoder. The addition operator naturally emerges from the commutativity\\nof the operator, not restricting the operator itself to be addition. For example, multiplication of\\ntwo numbers x1andx2can be written as x1x2= exp(ln(x1) + ln(x2))whereρ(x) = exp(x)and\\nφ(x) = ln(x).\\nC An illustrative example\\nWe use a concrete case to illustrate why parallelograms lead to generalization (see Figure 9). For the\\npurpose of illustration, we exploit a curriculum learning setting, where a neural network is fed with a\\nfew new samples each time. We will illustrate that, as we have more samples in the training set, the\\nideal model M∗(deﬁned in Section 3.2) will arrange the representation R∗in a more structured way,\\ni.e., more parallelograms are formed, which helps generalization to unseen validation samples. For\\nsimplicity we choose p= 6.\\n•D1= (0,4)andD2= (1,3)have the same label, so (0,4,1,3)becomes a parallelogram\\nsuch that E0+E4=E1+E3→E3−E0=E4−E1.D3= (1,5)andD4= (2,4)have\\n14the same label, so (1,5,2,4)becomes a parallelogram such that E1+E5=E2+E4→\\nE4−E1=E5−E2. We can derive from the ﬁrst two equations that E5−E2=E3−E0→\\nE0+E5=E2+E3, which implies that (0,5,2,3)is also a parallelogram (see Figure 9(a)).\\nThis means if (0,5)in training set, our model can predict (2,3)correctly.\\n•D5= (0,2)andD6= (1,1)have the same label, so E0+E2= 2E1, i.e., 1is the middle\\npoint of 0and2(see Figure 9(b)). Now we can derive that 2E4=E3+E5, i.e., 4is the\\nmiddle point of 3and5. If(4,4)is in the training data, our model can predict (3,5)correctly.\\n• Finally,D7= (2,4)andD8= (3,3)have the same label, so 2E3=E2+E4, i.e., 3should\\nbe placed at the middle point of 2and4, ending up Figure 9(c). This linear structure agrees\\nwith the arithmetic structure of R.\\nIn summary, although we have p(p+ 1)/2 = 21 different training samples for p= 6, we only need 8\\ntraining samples to uniquely determine the perfect linear structure (up to linear transformation). The\\npunchline is: representations lead to generalization.\\nFigure 9: As we include more data in the training set, the (ideal) model is capable of discovering\\nincreasingly structured representations (better RQI), from (a) to (b) to (c).\\nD Deﬁnition of ˆAcc\\nGiven a training set Dand a representation R, if(i,j)is a validation sample, can the neural network\\ncorrectly predict its output, i.e., Dec(Ei+Ej) =Yi+j? Since neural network has never seen (i,j)\\nin the training set, one possible mechanism of induction is through\\nDec(Ei+Ej) = Dec( Em+En) =Ym+n(=Yi+j). (11)\\nThe ﬁrst equality Dec(Ei+Ej) = Dec( Em+En)holds only when Ei+Ej=Em+En(i.e.,\\n(i,j,m,n )is a parallelogram). The second equality Dec(Em+En) =Ym+n, holds when (m,n)\\nis in the training set, i.e., (m,n)∈D, under the zero training loss assumption. Rigorously, given a\\ntraining setDand a parallelogram set P(which can be calculated from R), we collect all zero loss\\nsamples in an augmented training setD\\nD(D,P) =D⋃\\n{(i,j)|∃(m,n)∈D,(i,j,m,n )∈P}. (12)\\nKeepingDﬁxed, a larger Pwould probably produce a larger D, i.e., ifP1⊆P2, thenD(D,P 1)⊆\\nD(P,P 2), which is why in Eq. (3) our deﬁned RQI∝|P|gets its name “representation quality\\nindex\", because higher RQI normally means better generalization. Finally, the expected accuracy\\nfrom a dataset Dand a parallelogram set Pis:\\nˆAcc =|D(D,P)|\\n|D0|, (13)\\nwhich is the estimated accuracy (of the full dataset), and P=P(R)is deﬁned on the representation\\nafter training. On the other hand, accuracy Acc can be accessed empirically from trained neural\\nnetwork. We veriﬁed Acc≈ˆAcc in a toy setup (addition dataset p= 10 , 1D embedding space,\\nhard code addition), as shown in Figure 3 (c). Figure 3 (a)(b) show Acc andˆAcc as a function of\\ntraining set ratio, with each dot corresponding to a different random seed. The dashed red diagonal\\ncorresponds to memorization of the training set, and the vertical gap refers to generalization.\\n15Although the agreement is good for 1D embedding vectors, we do not expect such agreement can\\ntrivially extend to high dimensional embedding vectors. In high dimensions, our deﬁnition of RQI is\\ntoo restrictive. For example, suppose we have an embedding space with Ndimensions. Although the\\nrepresentation may form a linear structure in the ﬁrst dimension, the representation can be arbitrary\\nin otherN−1dimensions, leading to RQI≈0. However, the model may still generalize well if the\\ndecoder learns to keep only the useful dimension and drop all other N−1useless dimensions. It\\nwould be interesting to investigate how to deﬁne an RQI that takes into account the role of decoder in\\nfuture works.\\nE The gap of a realistic model Mand the ideal model M∗\\nRealistic models Musually form fewer number of parallelograms than ideal models M∗. In this\\nsection, we analyze the properties of ideal models and calculated ideal RQI and ideal accuracy,\\nwhich set upper bounds for empirical RQI and accuracy. The upper bound relations are veriﬁed via\\nnumerical experiments in Figure 10.\\nSimilar to Eq. (12) where some validation samples can be derived from training samples, we\\ndemonstrate how implicit parallelograms can be ‘derived’ from explicit ones in P0(D). The so-called\\nderivation follows a simple geometric argument that: if A1B1is equal and parallel to A2B2, and\\nA2B2is equal and parallel to A3B3, then we can deduce that A1B1is equal and parallel to A3B3\\n(hence (A1,B2,A2,B1)is a parallelogram).\\nRecall that a parallelogram (i,j,m,n )is equivalent to Ei+Ej=Em+En(∗). So we are\\nequivalently asking if equation (∗)can be expressed as a linear combination of equations in\\nA(P0(D)). If yes, then (∗)is dependent on A(P0(D))(deﬁned in Eq. (7)), i.e., A(P0(D))and\\nA(P0(D)⋃(i,j,m,n ))should have the same rank. We augment P0(D)by adding implicit parallel-\\nograms, and denote the augmented parallelogram set as\\nP(D) =P0(D)⋃\\n{q≡(i,j,m,n )|q∈P0,rank(A(P0(D))) = rank(A(P0(D)⋃\\nq))}.(14)\\nWe need to emphasize that an assumption behind Eq. (14) is that we have an ideal model M∗. When\\nthe model is not ideal, e.g., when the injectivity of the encoder breaks down, fewer parallelograms\\nare expected to form, i.e.,\\nP(R)⊆P(D). (15)\\nThe inequality is saying, whenever a parallelogram is formed in the representation after training, the\\nreason is hidden in the training set. This is not a strict argument, but rather a belief that today’s neural\\nnetworks can only copy what datasets (explicitly or implicitly) tell it to do, without any autonomous\\ncreativity or intelligence. For simplicity we call this belief Alexander Principle . In very rare cases\\nwhen something lucky happens (e.g., neural networks are initialized at approximate correct weights),\\nAlexander principle may be violated. Alexander principle sets an upper bound for RQI:\\nRQI(R)≤|P(D)|\\n|P0|≡RQI, (16)\\nand sets an upper bound for ˆAcc:\\nˆAcc≡ˆAcc(D,P(R))≤ˆAcc(D,P(D))≡Acc. (17)\\nIn Figure 10 (c)(f), we verify Eq. (16) and Eq. (17). We choose δ= 0.01to compute RQI( R,δ). We\\nﬁnd the trained models are usually far from being ideal, although we already include a few useful\\ntricks proposed in Section 4 to enhance representation learning. It would be an interesting future\\ndirection to develop better algorithms so that the gap due to Alexander principle can be reduced\\nor even closed. In Figure 10 (a)(b)(d)(e), four quantities (RQI, RQI, Acc, Acc) as functions of the\\ntraining data fraction are shown, each dot corresponding to one random seed. It is interesting to\\nnote that it is possible to have RQI = 1 only with<40% training data, i.e., 55×0.4 = 22 samples,\\nagreeing with our observation in Section 3.\\nRealistic representations Suppose an ideal model M∗and a realistic model Mwhich train on the\\ntraining setDgive the representation R∗andR, respectively. What is the relationship between R\\nandR∗? Due to the Alexander principle we know P(R)⊆P(D) =P(R∗). This means R∗has\\nmore parallelograms than R, henceR∗has fewer degrees of freedom than R.\\n160.0 0.2 0.4 0.6 0.8 1.0training data fraction0.00.20.40.60.81.0RQI(a)\\n0.0 0.2 0.4 0.6 0.8 1.0training data fraction0.00.20.40.60.81.0RQI(b)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nRQI0.00.20.40.60.81.0RQIIdeal AlgorithmAlexander\\nPrinciple(c)\\n0.0 0.2 0.4 0.6 0.8 1.0training data fraction0.00.20.40.60.81.0Acc\\nMemorization(d)\\n0.0 0.2 0.4 0.6 0.8 1.0training data fraction0.00.20.40.60.81.0Acc\\nMemorization(e)\\n0.0 0.2 0.4 0.6 0.8 1.0Acc0.00.20.40.60.81.0AccIdeal AlgorithmAlexander\\nPrinciple(f)Figure 10: We compare RQI and Acc for an ideal algorithm (with bar) and a realistic algorithm\\n(without bar). In (a)(b)(d)(e), four quantities (RQI, RQI, Acc, Acc) as functions of training data\\nfraction are shown. In (c)(f), RQI and Acc of the ideal algorithm sets upper bounds for those of the\\nrealistic algorithm.\\nWe illustrate with the toy case p= 4. The whole dataset contains p(p+ 1)/2 = 10 samples, i.e.,\\nD0={(0,0),(0,1),(0,2),(0,3),(1,1),(1,2),(1,3),(2,2),(2,3),(3,3)}. (18)\\nThe parallelogram set contains only three elements, i.e.,\\nP0={(0,1,1,2),(0,1,2,3),(1,2,2,3)}, (19)\\nOr equivalently the equation set\\nA0={A1 :E0+E2= 2E1,A2 :E0+E3=E1+E2,A3 :E1+E3= 2E2}. (20)\\nPictorially, we can split all possible subsets {A|A⊆A0}into different levels, each level deﬁned by\\n|A|(the number of elements). A subset A1in theithlevel points an direct arrow to another subset\\nA2in the (i+ 1)thlevel ifA2⊂A1, and we say A2is a child of A1, andA1is a parent of A2.\\nEach subset Acan determine a representation Rwithn(A)degrees of freedom. So Rshould be a\\ndescendant of R∗, andn(R∗)≤n(R). Numerically, n(A)is equal to the dimension of the null space\\nofA.\\nSuppose we have a training set\\nD={(0,2),(1,1),(0,3),(1,2),(1,3),(2,2)}, (21)\\nand correspondingly P(D) =P0,A(P) =A0. So an ideal model M∗will have the linear structure\\nEk=a+kb(see Figure 11 leftmost). However, a realistic model Mmay produce any descendants\\nof the linear structure, depending on various hyperparameters and even random seeds.\\nIn Figure 12, we show our algorithms actually generates all possible representations. We have\\ntwo settings: (1) fast decoder (η1,η2) = (10−3,10−2)(Figure 12 left), and (2) relatively slow\\ndecoder (η1,η2) = (10−2,10−3)(Figure 12) right). The relatively slow decoder produces better\\nrepresentations (in the sense of higher RQI) than a fast decoder, agreeing with our observation in\\nSection 4.\\n17Figure 11:p= 4 case. Equation set A(or geometrically, representation) has a hierarchy: a→b\\nmeansais a parent of b, andbis a child ofa. A realistic model can only generate representations that\\nare descendants of the representation generated by an ideal model.\\nFigure 12:p= 4case. Representations obtained from training neural networks are displayed. η1and\\nη2are learning rates of the representation and the decoder, respectively. As described in the main text,\\n(η1,η2) = (10−2,10−3)(right) is more ideal than (η1,η2) = (10−3,10−2)(left), thus producing\\nrepresentations containing more parallelograms.\\nF Conservation laws of the effective theory\\nRecall that the effective loss function\\nℓeff=ℓ0\\nZ0, ℓ 0≡∑\\n(i,j,m,n )∈P0(D)|Ei+Ej−Em−En|2/|P0(D)|, Z 0≡∑\\nk|Ek|2(22)\\nwhereℓ0andZ0are both quadratic functions of R={E0,···,Ep−1}, andℓeff= 0remains zero\\nunder rescaling and translation E′\\ni=aEi+b. We will ignore the 1/|P0(D)|factor inℓ0since\\nhaving it is equivalent to rescaing time, which does not affect conservation laws. The representation\\nvector Eievolves according to the gradient descent\\ndEi\\ndt=−∂ℓeff\\n∂Ei. (23)\\nWe will prove the following two quantities are conserved:\\nC=∑\\nkEk, Z 0=∑\\nk|Ek|2. (24)\\nEq. (22) and Eq. (23) give\\ndEi\\ndt=−ℓeff\\n∂Ei=−∂(ℓ0\\nZ0)\\n∂Ei=−1\\nZ0∂ℓ0\\n∂Ei+ℓ0\\nZ2\\n0∂Z0\\n∂Ei. (25)\\n18Then\\ndZ0\\ndt= 2∑\\niEk·dEk\\ndt(26)\\n=2\\nZ2\\n0∑\\niEi·(−Z0∂ℓ0\\n∂Ek+ 2ℓ0Ek)\\n=2\\nZ0(−∑\\nk∂ℓ0\\n∂Ek·Ek+ 2ℓ0)\\n= 0.\\nwhere the last equation uses the fact that\\n∑\\nk∂ℓ0\\n∂Ek·Ek= 2∑\\nk∑\\n(i,j,m,n )∈P0(D)(Ei+Ej−Em−En)(δik+δjk−δmk−δnk)·Ek\\n= 2∑\\n(i,j,m,n )∈P0(D)(Ei+Ej−Em−En)∑\\nk(δik+δjk−δmk−δnk)·Ek\\n=∑\\n(i,j,m,n )∈P0(D)(Ei+Ej−Em−En)·(Ei+Ej−Em−En)\\n= 2ℓ0\\nThe conservation of Z0prohibits the representation from collapsing to zero. Now that we have\\ndemonstrated that Z0is a conserved quantity, we can also show\\ndC\\ndt=∑\\nkdEk\\ndt(27)\\n=−1\\nZ0∑\\nk∂ℓ0\\n∂Ek\\n=−2\\nZ0∑\\nk∑\\n(i,j,m,n )∈P0(D)(Ei+Ej−Em−En)(δik+δjk−δmk−δnk)\\n=0.\\nThe last equality holds because the two summations can be swapped and∑\\nk(δik+δjk−δmk−δnk) =\\n0.\\nG More phase diagrams of the toy setup\\nWe study another three hyperparameters in the toy setup by showing phase diagrams similar to\\nFigure 6. The toy setup is: (1) addition without modulo ( p= 10 ); (2) training/validation is split into\\n45/10; (3) hard code addition; (4) 1D embedding. In the following experiments, the decoder is an\\nMLP with size 1-200-200-30. The representation and the encoder are optimized with AdamW with\\ndifferent hyperparameters. The learning rate of the representation is 10−3. We sweep the learning\\nrate of the decoder in range [10−4,10−2]as the x axis, and sweep another hyperparameter as the\\ny axis. By default, we use full batch size 45, initialization scale s= 1 and zero weight decay of\\nrepresentation.\\nBatch size controls the amount of noise in the training dynamics. In Figure 13, the grokking region\\nappears at the top left of the phase diagram (small decoder learning rate and small batch size).\\nHowever, large batch size (with small learning rate) leads to comprehension, implying that smaller\\nbatch size seems harmful. This makes sense since to get crystals (good structures) in experiments,\\none needs a freezer which gradually decreases temperature, rather than something perturbing the\\nsystem with noise.\\nInitialization scale controls distances among embedding vectors at initialization. We initialize\\ncomponents of embedding vectors from independent uniform distribution U[−s/2,s/2]wheres\\n191e-4 1e-3 1e-2\\nlearning rate1\\n12\\n45 batch size\\ncomprehensionmemorizationgrokkingconfusionAddition group (regression)\\n1e-4 1e-3 1e-2\\nlearning rate1\\n12\\n45 batch size\\ncomprehensionmemorizationgrokkingconfusionAddition group (classiﬁcation)Figure 13: Phase diagrams of decoder learning rate (x axis) and batch size (y axis) for the addition\\ngroup (left: regression; right: classiﬁcation). Small decoder leanrning rate and large batch size\\n(bottom left) lead to comprehension.\\n1e-4 1e-3 1e-2\\nlearning rate0.01\\n1.0\\n100.0 initialization scalecomprehension\\nmemorization\\nconfusionAddition group (regression)\\n1e-4 1e-3 1e-2\\nlearning rate0.01\\n1.0\\n100.0 initialization scalecomprehension\\nmemorization\\nconfusionAddition group (classiﬁcation)\\nFigure 14: Phase diagrams of decoder learning rate (x axis) and initialization (y axis) for the addition\\ngroup (left: regression; right: classiﬁcation). Small intialization scale (top) leads to comprehension.\\nis called the initialization scale. Shown in Figure 14, it is beneﬁcial to use a smaller initialization\\nscale. This agrees with the physical intuition that closer particles are more likely to interact and form\\nstructures. For example, the distances among molecules in ice are much smaller than distances in gas.\\nRepresentation weight decay controls the magnitude of embedding vectors. Shown in Figure 15,\\nwe see the representation weight decay in general does not affect model performance much.\\nH General groups\\nH.1 Theory\\nWe focused on Abelian groups for the most part of the paper. This is, however, simply due to\\npedagogical reasons. In this section, we show that it is straight-forward to extend deﬁnitions of\\nparallelograms and representation quality index (RQI) to general non-Abelian groups. We will also\\nshow that most (if not all) qualitative results for the addition group also apply to the permutation\\ngroup.\\n201e-4 1e-3 1e-2\\nlearning rate0.0\\n5.0\\n10.0weight decay (representation)comprehension\\nmemorizationgrokkingAddition group (regression)\\n1e-4 1e-3 1e-2\\nlearning rate0.0\\n5.0\\n10.0weight decay (representation)\\ncomprehension\\nmemorizationgrokking\\nconfusionAddition group (classiﬁcation)Figure 15: Phase diagrams of decoder learning rate (x axis) and representation weight decay (y axis)\\nfor the addition group (left: regression; right: classiﬁcation). Representation weight decay does not\\naffect model performance much.\\nMatrix representation for general groups Let us ﬁrst review the deﬁnition of group representation.\\nA representation of a group Gon a vector space Vis a group homomorphism from GtoGL(V), the\\ngeneral linear group on V. That is, a representation is a map ρ:G→GL(V)such that\\nρ(g1g2) =ρ(g1)ρ(g2),∀g1,g2∈G. (28)\\nIn the caseVis of ﬁnite dimension n, it is common to identify GL(V)withnbyninvertible matrices.\\nThe punchline is that: each group element can be represented as a matrix, and the binary operation is\\nrepresented as matrix multiplication.\\nA new architecture for general groups Inspired by the matrix representation, we embed each group\\nelementaas a learnable matrix Ea∈Rd×d(as opposed to a vector), and manually do matrix\\nmultiplication before sending the product to the deocder for regression or classiﬁcation. More\\nconcretly, for a◦b=c, our architecture takes as input two embedding matrices EaandEband\\naims to predict Ycsuch that Yc= Dec( EaEb), where EaEbmeans the matrix multiplication of Ea\\nandEb. The goal of this simplication is to disentangle learning the representation and learning the\\narithmetic operation (i.e, the matrix multiplication). We will show that, even with this simpliﬁcation,\\nwe are still able to reproduce the characteristic grokking behavior and other rich phenomenon.\\nGeneralized parallelograms we deﬁne generalized parallelograms: (a,b,c,d )is a generalized\\nparallelogram in the representation if ||EaEb−EcEd||2\\nF≤δ, whereδ>0is a threshold to tolerate\\nnumerical errors. Before presenting the numerical results for the permutation group, we show an\\nintuitive picture about how new parallelograms can be deduced from old ones for general groups,\\nwhich is the key to generalization.\\nDeduction of parallelograms We ﬁrst recall the case of the Abelian group (e.g., addition group). As\\nshown in Figure 16, when (a,d,b,c )and(c,f,d,e )are two parallelograms, we have\\nEa+Ed=Eb+Ec,\\nEc+Ef=Ed+Ed.(29)\\nWe can derive that Ea+Ef=Eb+Eeimplying that (a,f,b,e )is also a parallelogram. That is, for\\nAbelian groups, two parallelograms are needed to deduce a new parallelogram.\\nFor the non-Abelian group, if we have only two parallelograms such that\\nEaEd=EbEc,\\nEfEc=EeEd,(30)\\nwe have E−1\\nbEa=EcE−1\\nd=E−1\\nfEe, but this does not lead to something like EfEa=EeEb,\\nhence useless for generalization. However, if we have a third parallelogram such that\\nEeEh=EfEg (31)\\n21Figure 16: Deduction of parallelograms\\nwe have E−1\\nbEa=EcE−1\\nd=E−1\\nfEe=EgE−1\\nh, equivalent to EaEh=EbEg, thus establishing a\\nnew parallelogram (a,h,b,g ). That is, for non-Abelian groups, three parallelograms are needed to\\ndeduce a new parallelogram.\\nH.2 Numerical Results\\nIn this section, we conduct numerical experiments on a simple non-abelian group: the permutation\\ngroupS3. The group has 6 group elements, hence the full dataset contains 36 samples. We embed each\\ngroup element ainto a learnable 3×3embedding matrix Ea. We adopt the new architecture described\\nin the above subsection: we hard code matrix multiplication of two input embedding matrices before\\nfeeding to the decoder. After deﬁning the generalized parallelogram in the last subsection, we can\\ncontinue to deﬁne RQI (as in Section 3) and predict accuracy ˆAcc from representation (as in appendix\\nD). We also compute the number of steps needed to reach RQI = 0.95.\\nRepresentation We ﬂatten each embedding matrix into a vector, and apply principal component\\nanalysis (PCA) to the vectors. We show the ﬁrst three principal components of these group elements\\nin Figure 17. On the plane of PC1 andPC3 , the six points are organized as a hexagon.\\nPC 13\\n2\\n1\\n0123PC 2\\n1.5\\n1.0\\n0.5\\n0.00.51.01.5PC 3\\n1.5\\n1.0\\n0.5\\n0.00.51.01.5\\n[0, 1, 2]\\n[0, 2, 1][1, 0, 2]\\n[1, 2, 0][2, 0, 1]\\n[2, 1, 0]\\n3\\n 2\\n 1\\n 0 1 2 3\\nPC11.5\\n1.0\\n0.5\\n0.00.51.01.5PC2\\n[0, 1, 2] [0, 2, 1]\\n[1, 0, 2][1, 2, 0][2, 0, 1]\\n[2, 1, 0]\\n3\\n 2\\n 1\\n 0 1 2 3\\nPC11.5\\n1.0\\n0.5\\n0.00.51.01.5PC3[0, 1, 2]\\n[0, 2, 1][1, 0, 2]\\n[1, 2, 0][2, 0, 1]\\n[2, 1, 0]\\n1.5\\n 1.0\\n 0.5\\n 0.0 0.5 1.0 1.5\\nPC21.5\\n1.0\\n0.5\\n0.00.51.01.5PC3[0, 1, 2]\\n[0, 2, 1][1, 0, 2]\\n[1, 2, 0][2, 0, 1]\\n[2, 1, 0]\\nFigure 17: Permuation group S3. First three principal components of six embedding matrices R3×3.\\nRQI In Figure 18 (a), we show RQI as a function of training data fraction. For each training data\\nfraction, we run 11 random seeds (shown as scatter points), and the blue line corresponds to the\\nhighest RQI.\\nSteps to reach RQI = 0.95In Figure 18 (b), we whow the steps to reach RQI>0.95as a function\\nof training data fraction, and ﬁnd a phase transition at r=rc= 0.5. The blue line corresponds to the\\nbest model (smallest number of steps).\\n220.4 0.6 0.8\\ntraining data fraction0.00.20.40.60.81.0RQI(a)\\n0.4 0.6 0.8\\ntraining data fraction103104steps to RQI >0.95\\nrc= 0.5(b)\\n0.2 0.4 0.6 0.8 1.0\\ntraining data fraction0.20.30.40.50.60.70.80.91.0Acc(c)\\n0.2 0.4 0.6 0.8 1.0\\ntraining data fraction0.20.30.40.50.60.70.80.91.0ˆAcc(d)\\n0.2 0.4 0.6 0.8 1.0\\nˆAcc0.20.30.40.50.60.70.80.91.0Acc(e)Figure 18: Permutation group S3. (a) RQI increases as training set becomes larger. Each scatter point\\nis a random seed, and the blue line is the highest RQI obtained with a ﬁxed training set ratio; (b)\\nsteps to reach RQI>0.95. The blue line is the smallest number of steps required. There is a phase\\ntransition around rc= 0.5. (c) real accuracy Acc; (d) predicted accuracy ˆAcc; (e) comparison of Acc\\nandˆAcc:ˆAcc serves as a lower bound of Acc.\\nAccuracy The real accuracy Acc is shown in Figure 18 (c), while the predicted accuracy ˆAcc\\n(calculated from RQI) is shown in Figure 18 (d). Their comparison is shown in (e): ˆAccis a lower\\nbound of Acc, implying that there must be some generalization mechanism beyond RQI.\\nPhase diagram We investigate how the model performance varies under the change of two knobs:\\ndecoder learning rate and decoder weight decay. We calculate the number of steps to training accuracy\\n≥0.9and validation accuracy ≥0.9, respectively, shown in Figure 6 (d).\\nI Effective theory for image classiﬁcation\\nIn this section, we show our effective theory proposed in Section 3.2 can generalize beyond algorith-\\nmic datasets. In particular, we will apply the effective theory to image classiﬁcations. We ﬁnd that:\\n(i) The effective theory naturally gives rise to a novel self-supervised learning method, which can\\nprovably avoid mode collapse without contrastive pairs. (ii) The effective theory can shed light on the\\nneural collapse phenomenon [ 28], in which same-class representations collapse to their class-means.\\nWe ﬁrst describe how the effective theory applies to image classiﬁcation. The basic idea is again that,\\nsimilar to algorithmic datasets, neural networks try to develop a structured representation of the inputs\\nbased on the relational information between samples (class labels in the case of image classiﬁcation,\\nsum parallelograms in the case of addition, etc.). The effective theory has two ingredients: (i) samples\\nwith the same label are encouraged to have similar representations; (ii) the effective loss function\\nis scale-invariant to avoid all representations collapsing to zero (global collapse). As a result, an\\neffective loss for image classiﬁcation has the form\\nℓeﬀ=ℓ\\nZ, ℓ =∑\\n(x,y)∈P|f(x)−f(y)|2, Z =∑\\nx|f(x)|2(32)\\nwhere xis an image, f(x)is its representation, (x,y)∈Prefers to unique pairs xandythat have\\nthe same label. Scale invariance means the loss function ℓeﬀdoes not change under the linear scaling\\nf(x)→af(x).\\nRelation to neural collapse It was observed in [ 28] that image representations in the penultimate\\nlayer of the model have some interesting features: (i) representations of same-class images collapse\\nto their class-means; (ii) class-means of different classes develop into an equiangular tight frame. Our\\neffective theory is able to predict the same-class collapse, but does not necessarily put class-means\\ninto equiangular tight frames. We conjecture that little explicit repulsion among different classes can\\nhelp class-means develop into an equiangular tight frame, similar to electrons developing into lattice\\nstructures on a sphere under repulsive Coulomb forces (the Thomson problem [ 29]). We would like\\nto investigate this modiﬁcation of the effective theory in the future.\\nExperiment on MNIST We directly apply the effective loss Eq. (32) to the MNIST dataset. Firstly,\\neach image xis randomly encoded to a 2D embedding f(x)via the same encoder MLP whose weights\\nare randomly initialized. We then train these embeddings by minimizing the effective loss ℓeﬀwith\\n23Figure 19: Our effective theory applies to MNIST image classiﬁcations. Same-class images collapse\\nto their class-means, while class-means of different classes stay separable. As such, the effective\\ntheory serves as a novel self-supervised learning method, as well as shed some light on neural collapse.\\nPlease see texts in Appendix I.\\nan Adam optimizer ( 10−3learning rate) for 100 steps. We show the evolution of these embeddings in\\nFigure 19. Images of the same class collapse to their class-means, and different class-means do not\\ncollapse. This means that our effective theory can give rise to a good representation learning method\\nwhich only exploits non-contrastive relational information in datasets.\\nLink to self-supervised learning Note thatℓitself is vulnerable to global collapse, in the context\\nof Siamese learning without contrastive pairs. Various tricks (e.g., decoder with momentum, stop\\ngradient) [ 13,30] have been proposed to avoid global collapse. However, the reasons why these\\ntricks can avoid global collapse are unclear. We argue ℓfails simply because ℓ→a2ℓunder scaling\\nf(x)→af(x)so gradient descent on ℓencouragea→0. Based on this picture, our effective theory\\nprovides another possible ﬁx: make the loss function ℓscale-invariant (by the normalized loss ℓeﬀ),\\nso the gradient ﬂow has no incentive to change representation scales. In fact, we can prove that\\n24the gradient ﬂow on ℓeﬀpreserveZ(variance of representations) so that global collapse is avoided\\nprovably:\\n∂ℓeﬀ\\n∂f(x)=1\\nZ∂ℓ\\n∂f(x)−l\\nZ2∂Z\\n∂f(x)=2\\nZ∑\\ny∼x(f(x)−f(y))−2ℓ\\nZ2f(x),\\ndZ\\ndt= 2∑\\nxf(x)·df(x)\\ndt= 2∑\\nxf(x)·∂ℓeﬀ\\n∂f(x)\\n=4\\nZ∑\\nxf(x)·(∑\\ny∼x(f(x)−f(y))−ℓ\\nZf(x))\\n=4\\nZ[∑\\nxf(x)·∑\\ny∼x(f(x)−f(y))−∑\\nxℓ\\nZ|f(x)|2]\\n= 0.(33)\\nwhere we use the fact that\\n∑\\nxf(x)·∑\\ny∼x(f(x)−f(y)) =∑\\n(x,y)∈P(f(x)−f(y))·(f(x)−f(y)) =ℓ (34)\\nJ Grokking on MNIST\\nTo induce grokking on MNIST, we make two nonstandard decisions: (1) we reduce the size of the\\ntraining set from 50k to 1k samples (by taking a random subset) and (2) we increase the scale of the\\nweight initialization distribution (by multiplying the initial weights, sampled with Kaiming uniform\\ninitialization, by a constant >1).\\nThe choice of large initializations is justiﬁed by [ 31–33] which ﬁnd large initializations overﬁt data\\neasily but prone to poor generalization. Relevant to this, initialization scale is found to regulate\\n“kernel” vs “rich” learning regimes in networks [34].\\nWith these modiﬁcations to training set size and initialization scale, we train a depth-3 width-200\\nMLP with ReLU activations with the AdamW optimizer. We use MSE loss with one-hot targets,\\nrather than cross-entropy. With this setup, we ﬁnd that the network quickly ﬁts the train set, and then\\nmuch later in training validation accuracy improves, as shown in Figure 8a. This closely follows the\\nstereotypical grokking learning, ﬁrst observed in algorithmic datasets.\\nWith this setup, we also compute a phase diagram over the model weight decay and the last layer\\nlearning rate. See Figure 8b. While in MLPs it is less clear what parts of the network to consider\\nthe “encoder” vs the “decoder”, for our purposes here we consider the last layer to be the “decoder”\\nand vary its learning rate relative to the rest of the network. The resulting phase diagram has some\\nsimilarity to Figure 7. We observe a “confusion”phase in the bottom right (high learning rate and\\nhigh weight decay), a “comprehension” phase bordering it, a “grokking” phase as one decreases\\nweight decay and decoder learning rate, and a “memorization“ phase at low weight decay and low\\nlearning rate. Instead of an accuracy threshold of 95%, we use a threshold of 60% here for validation\\naccuracy for runs to count as comprehension or grokking. This phase diagram demonstrates that with\\nsufﬁcient regularization, we can again “de-grok” learning.\\nWe also investigate the effect of training set size on time to generalization on MNIST. We ﬁnd a result\\nsimilar to what Power et al. [ 1] observed, namely that generalization time increases rapidly once one\\ndrops below a certain amount of training data. See Figure 20.\\nK Lottery Ticket Hypothesis Connection\\nIn Figure 21, we show the projection of the learned embeddings after generalization to their ﬁrst\\ntwo principal components. Compared to the projection at initialization, structure clearly emerges in\\nembedding space when the neural network is able to generalize ( >99% validation accuracy). What\\nis intriguing is that the projection of the embeddings at initialization to the principal components\\nof the embeddings at generalization seem to already contain much of that structure. In this sense,\\n250 5000 10000 15000 20000 25000 30000\\nTrain Points104105Steps to Validation Accuracy > 60%Steps until generalization for MNIST (weight decay 5e-3)\\nMean\\nRuns that didn\\'t reach 60% val acc in 10^5 steps\\nRuns that reached 60% val acc in 10^5 stepsFigure 20: Time to generalize as a function of training set size, on MNIST.\\n−4−2 0 2 4\\nGeneralization PCA 1−4−2024Generalization PCA 2After generalization\\n−5.0−2.5 0.0 2.5 5.0 7.5\\nInitialization PCA 1−6−4−20246Initialization PCA 2At initialization\\n−4−2 0 2 4\\nGeneralization PCA 1−3−2−101234Generalization PCA 2At initialization\\nFigure 21: (Left) Input embeddings after generalization projected on their ﬁrst 2 principal compo-\\nnents. (Center) Input embeddings at initialization projected on their ﬁrst 2 principal components.\\n(Right) Input embeddings at initialization projected on the ﬁrst 2 principal components of the\\nembeddings after generalization at the end of training (same PCA as the left ﬁgure).\\nthe structured representation necessary for generalization already existed (partially) at initialization.\\nThe training procedure essentially prunes other unnecessary dimensions and forms the required\\nparallelograms for generalization. This is a nonstandard interpretation of the lottery ticket hypothesis\\nwhere the winning tickets are not weights or subnetworks but instead particular axes or linear\\ncombinations of the weights (the learned embeddings).\\nIn Figure 22, we show the original training curves (dashed lines). In solid lines, we recompute\\naccuracy with models which use embeddings that are projected onto the nprincipal components of\\nthe embeddings at the end of training (and back). Clearly, the ﬁrst few principal components contain\\nenough information to reach 99% accuracy. The ﬁrst few PCs explain the most variance by deﬁnition,\\nhowever, we note that this is not necessarily the main reason for why they can generalize so well. In\\nfact, embeddings reconstructed from the PCA at the end of training (solid lines) perform better than\\ncurrent highest variance axes (dotted line). This behavior is consistent across seeds.\\n26102103104\\nEpoch0.00.20.40.60.81.0Accuracy10 Components\\nTest Reconstructed (Final PCA)\\nTrain Reconstructed (Final PCA)\\nTrain Reconstructed (Current PCA)\\nTest Reconstructed (Current PCA)\\nTest\\nTrain\\n102103104\\nEpoch0.00.20.40.60.81.0Accuracy6 Components\\nTest Reconstructed (Final PCA)\\nTrain Reconstructed (Final PCA)\\nTrain Reconstructed (Current PCA)\\nTest Reconstructed (Current PCA)\\nTest\\nTrain\\n102103104\\nEpoch0.00.20.40.60.81.0Accuracy5 Components\\nTest Reconstructed (Final PCA)\\nTrain Reconstructed (Final PCA)\\nTrain Reconstructed (Current PCA)\\nTest Reconstructed (Current PCA)\\nTest\\nTrain\\n102103104\\nEpoch0.00.20.40.60.81.0Accuracy2 Components\\nTest Reconstructed (Final PCA)\\nTrain Reconstructed (Final PCA)\\nTrain Reconstructed (Current PCA)\\nTest Reconstructed (Current PCA)\\nTest\\nTrainFigure 22: Train and test accuracy computed while using actual embeddings (dashed line) and\\nembeddings projected onto and reconstructed from their ﬁrst nprincipal components (dotted lines)\\nand, ﬁnally, using embeddings projected onto and reconstructed from the ﬁrst nPCs of the embeddings\\nat the end of training (solid lines).\\n27L Derivation of the effective loss\\nIn this section, we will further motivate the use of our effective loss to study the dynamics of\\nrepresentation learning by deriving it from the gradient ﬂow dynamics on the actual MSE loss in\\nlinear regression. The loss landscape of a neural network is in general nonlinear, but the linear case\\nmay shed some light on how the effective loss can be derived from actual loss. For a sample r(which\\nis the sum of two embeddings EiandEj), the prediction of the linear network is D(r) =Ar+b.\\nThe loss function is ( yis its corresponding label):\\nℓ=1\\n2|Ar+b−y|2\\n\\ued19\\ued18\\ued17\\ued1a\\nℓpred+γ\\n2||A||2\\nF\\n\\ued19\\ued18\\ued17\\ued1a\\nℓreg, (35)\\nwhere the ﬁrst and the second term are prediction error and regularization, respectively. Both the\\nmodel (A,b)and the input rare updated via gradient ﬂow, with learning rate ηAandηx, respectively:\\ndA\\ndt=−ηA∂ℓ\\n∂A,db\\ndt=−ηA∂ℓ\\n∂b,dr\\ndt=−ηx∂ℓ\\n∂r. (36)\\nInsertingℓinto the above equations, we obtain the gradient ﬂow:\\ndA\\ndt=−ηA∂ℓ\\n∂A=−ηA[A(rrT+γ) + (b−y)rT],\\ndb\\ndt=−ηA∂ℓ\\n∂b=−ηA(Ar+b−y)\\ndr\\ndt=−ηx∂ℓ\\n∂r=−ηxAT(Ar+b−y).(37)\\nFor thedb/dtequation, after ignoring the Arterm and set the initial condition b(0) = 0, we obtain\\nanalytically b(t) = (1−e−2ηAt)y. Inserting this into the ﬁrst and third equations, we have\\ndA\\ndt=−ηA[A(rrT+γ)−e−2ηAtyrT],\\ndr\\ndt=−ηxATAr\\ued19\\ued18\\ued17\\ued1a\\ninternal interaction+ηxe−2ηAtATy\\ued19\\ued18\\ued17\\ued1a\\nexternal force.(38)\\nFor the second equation on the evolution of dr/dt, we can artiﬁcially decompose the right hand side\\ninto two terms, based on whether they depend on the label y. In this way, we call the ﬁrst term\\n\"internal interaction\" since it does not depend on y, while the second term \"external force\". Note\\nthis distinction seems a bit artiﬁcial from a mathematical perspective, but it can be conceptually\\nhelpful from a physics perspective. We will show below the internal interaction term is important for\\nrepresentations to form. Because we are interested in how two samples interact, we now consider\\nanother sample at r′, and the evolution becomes\\ndA\\ndt=−ηA[A(rrT+r′r′T+ 2γ)−e−2ηAty(r+r′)T],\\ndr\\ndt=−ηxATAr+ηxe−2ηAtATy,\\ndr′\\ndt=−ηxATAr′+ηxe−2ηAtATy.(39)\\nSubtractingdr/dtbydr′/dtand setting r′=−r, the above equations further simply to\\ndA\\ndt=−2ηAA(rrT+γ),\\ndr\\ndt=−ηxATAr.(40)\\nThe second equation implies that the pair of samples interact via a quadratic potential U(r) =\\n1\\n2rTATAr, leading to a linear attractive force f(r)∝r. We now consider the adiabatic limit where\\nηA→0.\\n28The adiabatic limit Using the standard initialization (e.g., Xavier initialization) of neural networks,\\nwe have AT\\n0A0≈I. As a result, the quadratic potential becomes U(r) =1\\n2rTr, which is time-\\nindependent because ηA→0. We are now in the position to analyze the addition problem. For two\\nsamples x(1)=Ei+Ejandx(2)=Em+Enwith the same label ( i+j=m+n), they contribute\\nto an interaction term\\nU(i,j,m,n ) =1\\n2|Ei+Ej−Em−En|2\\n2. (41)\\nAveraging over all possible quadruples in the training dataset D, the total energy of the system is\\nℓ0=∑\\n(i,j,m,n )∈P0(D)1\\n2|Ei+Ej−Em−En|2\\n2/|P0(D)|, (42)\\nwhereP0(D) ={(i,j,m,n )|i+j=m+n,(i,j)∈D,(m,n)∈D}. To make it scale-invariant,\\nwe deﬁne the normalized Hamiltonian Eq. (42) as\\nℓeﬀ=ℓ0\\nZ0, Z 0=∑\\ni|Ei|2\\n2 (43)\\nwhich is the effective loss we used in Section 3.2.\\n29'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_contents[\"6 Conclusion\"] # notice References not removed yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract\n",
      "1569\n",
      "\n",
      "1 Introduction\n",
      "2891\n",
      "\n",
      "2 Problem Setting\n",
      "3168\n",
      "\n",
      "3 Why Generalization Occurs: Representations and Dynamics\n",
      "11692\n",
      "\n",
      "4 Delayed Generalization: A Phase Diagram\n",
      "10650\n",
      "\n",
      "5 Related work\n",
      "2673\n",
      "\n",
      "6 Conclusion\n",
      "47991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for title, content in section_contents.items():\n",
    "    print(title)\n",
    "    print(len(content))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.\\n3.1 Representation quality predicts generalization for the toy model\\nA rigorous deﬁnition for structure in the learned representation is necessary. We propose the following\\ndeﬁnition,\\nDeﬁnition 1. (i,j,m,n )is aδ-parallelogram in the representation R≡[E0,···,Ep−1]if\\n|(Ei+Ej)−(Em+En)|≤δ.\\nIn the following derivations, we can take δ, which is a small threshold to tolerate numerical errors, to\\nbe zero.\\nProposition 1. When the training loss is zero, any parallelogram (i,j,m,n )in representation R\\nsatisﬁesi+j=m+n.\\nProof. Suppose that this is not the case, i.e., suppose Ei+Ej=Em+Enbuti+j̸=m+n, then\\nYi+j= Dec( Ei+Ej) = Dec( Em+En) =Ym+nwhere the ﬁrst and last equalities come from\\nthe zero training loss assumption. However, since i+j̸=m+n, we have Yi+j̸=Yn+m(almost\\nsurely in the regression task), a contradiction.\\n30.0 0.2 0.4 0.6 0.8 1.0\\ntraining data fraction0.00.20.40.60.81.0Acc\\nMemorization(a)\\n0.0 0.2 0.4 0.6 0.8 1.0\\ntraining set fraction0.00.20.40.60.81.0ˆAcc(b)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nˆAcc0.00.20.40.60.81.0Acc(c)Figure 3: We compute accuracy (of the full dataset) either measured empirically Acc, or predicted\\nfrom the representation of the embeddings ˆAcc. These two accuracies as a function of training data\\nfraction are plotted in (a)(b), and their agreement is shown in (c).\\nIt is convenient to deﬁne the permissible parallelogram set associated with a training dataset D\\n(“permissible” means consistent with 100% training accuracy) as\\nP0(D) ={(i,j,m,n )|(i,j)∈D,(m,n)∈D, i+j=m+n}. (1)\\nFor simplicity, we denote P0≡P0(D0). Given a representation R, we can check how many\\npermissible parallelograms actually exist in Rwithin error δ, so we deﬁne the parallelogram set\\ncorresponding to Ras\\nP(R,δ) ={(i,j,m,n )|(i,j,m,n )∈P0,|(Ei+Ej)−(Em+En)|≤δ}. (2)\\nFor brevity we will write P(R), suppressing the dependence on δ. We deﬁne the representation\\nquality index (RQI) as\\nRQI(R) =|P(R)|\\n|P0|∈[0,1]. (3)\\nWe will use the term linear representation orlinear structure to refer to a representation whose\\nembeddings are of the form Ek=a+kb(k= 0,···,p−1;a,b∈Rdin). A linear representation\\nhasRQI = 1 , while a random representation (sampled from, say, a normal dstribution) has RQI = 0\\nwith high probability.\\nQuantitatively, we denote the “predicted accuracy” ˆAcc as the accuracy achievable on the whole\\ndataset given the representation R(see Appendix D for the full details). In Figure 3, we see that\\nthe predictedˆAcc aligns well with the true accuracy Acc, establishing good evidence that structured\\nrepresentation of input embeddings leads to generalization. We use an example to illustrate the origin\\nof generalization here. In the setup of Figure 2 (b), suppose the decoder can achieve zero training\\nloss and E6+E8is a training sample hence Dec(E6+E8) =Y14. At validation time, the decoder\\nis tasked with predicting a validation sample E5+E9. Since (5,9,6,8)forms a parallelogram\\nsuch that E5+E9=E6+E8, the decoder can predict the validation sample correctly because\\nDec(E5+E9) = Dec( E6+E8) =Y14.\\n3.2 The dynamics of embedding vectors\\nSuppose that we have an ideal model M∗= (Dec∗,R∗)such that:2\\n• (1)M∗can achieve zero training loss;\\n• (2)M∗has an injective decoder, i.e., Dec∗(x1)̸= Dec∗(x2)for any x1̸=x2.\\nThen Proposition 2 provides a mechanism for the formation of parallelograms.\\n2One can verify a posteriori if a trained model Mis close to being an ideal model M∗. Please refer to\\nAppendix E for details.\\n40.00.20.40.60.81.0\\ntraining data fraction0.00.20.40.60.81.0Probability(linear structure) rc= 0.4(a) Theory: phase transition\\n0.4 0.6 0.8 1.0\\ntraining data fraction102103104Steps to RQI >0.95\\nrc= 0.4(b) Empirical: phase transition\\nRuns that reached\\nRQI>0.95 in 104steps\\nRuns that didn’t reach\\nRQI>0.95 in 104steps\\n0 500 1000 1500 2000\\nstep−1.5−1.0−0.50.00.51.01.51D representation\\n0123456789\\n3nh(c) Theory: trajectory\\n0 500 1000 1500 2000\\nstep−1.5−1.0−0.50.00.51.01.51D normalized representation 0123456789\\n3nh(d) Empirical: trajectoryFigure 4: (a) The effective theory predicts a phase transition in the probability of obtaining a linear\\nrepresentation around rc= 0.4. (b) Empirical results display a phase transition of RQI around\\nrc= 0.4, in agreement with the theory (the blue line shows the median of multiple random seeds).\\nThe evolution of 1D representations predicted by the effective theory or obtained from neural network\\ntraining (shown in (c) and (d) respectively) agree creditably well.\\nProposition 2. If a training set Dcontains two samples (i,j)and(m,n)withi+j=m+n,\\nthenM∗learns a representation R∗such that Ei+Ej=Em+En, i.e., (i,j,m,n )forms a\\nparallelogram.\\nProof. Due to the zero training loss assumption, we have Dec∗(Ei+Ej) =Yi+j=Ym+n=\\nDec∗(Em+En). Then the injectivity of Dec∗implies Ei+Ej=Em+En.\\nThe dynamics of the trained embedding vectors are determined by various factors interacting in\\ncomplex ways, for instance: the details of the decoder architecture, the optimizer hyperparameters,\\nand the various kinds of implicit regularization induced by the training procedure. We will see that\\nthe dynamics of normalized quantities, namely, the normalized embeddings at time t, deﬁned as\\n˜E(t)\\nk=E(t)\\nk−µt\\nσt, whereµt=1\\np∑\\nkE(t)\\nkandσt=1\\np∑\\nk|E(t)\\nk−µt|2, can be qualitatively described\\nby a simple effective loss (in the physics effective theory sense). We will assume that the normalized\\nembedding vectors obey a gradient ﬂow for an effective loss function of the form\\nd˜Ei\\ndt=−∂ℓeff\\n∂˜Ei, (4)\\nℓeff=ℓ0\\nZ0, ℓ 0≡∑\\n(i,j,m,n )∈P0(D)|˜Ei+˜Ej−˜Em−˜En|2/|P0(D)|, Z 0≡∑\\nk|˜Ek|2, (5)\\nwhere|·|denotes Euclidean vector norm. Note that the embeddings do not collapse to the trivial\\nsolution E0=···=Ep−1= 0unless initialized as such, because two conserved quantities exist, as\\nproven in Appendix F:\\nC=∑\\nkEk, Z 0=∑\\nk|Ek|2. (6)\\nWe shall now use the effective dynamics to explain empirical observations such as the existence of a\\ncritical training set size for generalization.\\nDegeneracy of ground states (loss optima) We deﬁne ground states as those representations satis-\\nfyingℓeff= 0, which requires the following linear equations to hold:\\nA(P) ={Ei+Ej=Em+En|(i,j,m,n )∈P}. (7)\\nSince each embedding dimension obeys the same set of linear equations, we will assume, without loss\\nof generality, that din= 1. The dimension of the null space of A(P), denoted as n0, is the number of\\ndegrees of freedom of the ground states. Given a set of parallelograms implied by a training dataset\\nD, the nullity of A(P(D))could be obtained by computing the singular values 0≤σ1≤···≤σp.\\nWe always have n0≥2, i.e.,σ1=σ2= 0because the nullity of A(P0), the set of linear equations\\ngiven by all possible parallelograms, is Nullity(A(P0)) = 2 which can be attributed to two degrees\\n5of freedom (translation and scaling). If n0= 2, the representation is unique up to translations and\\nscaling factors, and the embeddings have the form Ek=a+kb. Otherwise, when n0>2, the\\nrepresentation is not constrained enough such that all the embeddings lie on a line.\\nWe present theoretical predictions alongside empirical results for addition ( p= 10 ) in Figure 4. As\\nshown in Figure 4 (a), our effective theory predicts that the probability that the training set implies a\\nunique linear structure (which would result in perfect generalization) depends on the training data\\nfraction and has a phase transition around rc= 0.4. Empirical results from training different models\\nare shown in Figure 4 (b). The number of steps to reach RQI>0.95is seen to have a phase transition\\natrc= 0.4, agreeing with the proposed effective theory and with the empirical ﬁndings in [1].\\nTime towards the linear structure We deﬁne the Hessian matrix of ℓ0as\\nHij=1\\nZ0∂2ℓ0\\n∂Ei∂Ej, (8)\\nNote thatℓeﬀ=1\\n2RTHR,R= [E0,E1,···,Ep−1], so the gradient descent is linear, i.e.,\\ndR\\ndt=−HR. (9)\\nIfHhas eigenvalues λi=σ2\\ni(sorted in increasing order) and eigenvectors ¯vi, and we have the initial\\ncondition R(t= 0) =∑\\niai¯vi, then we have R(t) =∑\\niai¯vie−λit. The ﬁrst two eigenvalues\\nvanish andth= 1/λ3determines the timescale for the slowest component to decrease by a factor\\nofe. We callλ3thegrokking rate . When the step size is η, the corresponding number of steps is\\nnh=th/η= 1/(λ3η).\\nWe verify the above analysis with empirical results. Figure 4 (c)(d) show the trajectories obtained\\nfrom the effective theory and from neural network training, respectively. The 1D neural representation\\nin Figure 4 (d) are manually normalized to zero mean and unit variance. The two trajectories agree\\nqualitatively, and it takes about 3nhsteps for two trajectories to converge to the linear structure. The\\nquantitative differences might be due to the absence of the decoder in the effective theory, which\\nassumes the decoder to take inﬁnitesimal step sizes.\\nDependence of grokking on data size Note thatℓeﬀinvolves averaging over parallelograms in the\\ntraining set, it is dependent on training data size, so is λ3. In Figure 5 (a), we plot the dependence of\\nλ3on training data fraction. There are many datasets with the same data size, so λ3is a probabilistic\\nfunction of data size.\\nTwo insights on grokking can be extracted from this plot: (i) When the data fraction is below some\\nthreshold (around 0.4), λ3is zero with high probability, corresponding to no generalization. This\\nagain veriﬁes our critical point in Figure 4. (ii) When data size is above the threshold, λ3(on average)\\nis an increasing function of data size. This implies that grokking time t∼1/λ3decreases as training\\ndata size becomes larger, an important observation from [1].\\nTo verify our effective theory, we compare the grokking steps obtained from real neural network\\ntraining (deﬁned as steps to RQI>0.95), and those predicted by our theory tth∼1\\nλ3η(ηis the\\nembedding learning rate), shown in Figure 5 (b). The theory agrees qualitatively with neural networks,\\nshowing the trend of decreasing grokking steps as increasing data size. The quantitative differences\\nmight be explained as the gap between our effective loss and actual loss.\\nLimitations of the effective theory While our theory deﬁnes an effective loss based on the Euclidean\\ndistance between embeddings Ei+EjandEn+Em, one could imagine generalizing the theory to\\ndeﬁne a broader notion of parallogram given by some other metric on the representation space. For\\ninstance, if we have a decoder like in Figure 2 (d) then the distance between distinct representations\\nwithin the same “pizza slice” is low, meaning that representations arranged not in parallelograms\\nw.r.t. the Euclidean metric may be parallelograms with respect to the metric deﬁned by the decoder.\\n4 Delayed Generalization: A Phase Diagramw.r.t. the Euclidean metric may be parallelograms with respect to the metric deﬁned by the decoder.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsection = section_contents[\"3 Why Generalization Occurs: Representations and Dynamics\"]\n",
    "subsection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3.1 Representation quality predicts generalization for the toy model\n",
      "\n",
      "3.2 The dynamics of embedding vectors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# then extract subsections from sections\n",
    "pattern = r\"\\n\\d+\\.\\d+ [A-Z][a-zA-Z\\s:]+\\n\"\n",
    "\n",
    "subsections = re.findall(pattern, subsection)\n",
    "print(len(subsections))\n",
    "for match in subsections:\n",
    "    print(match.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract references from conclusion\n",
    "abstract = section_contents[\"Abstract\"]\n",
    "section_contents[\"Abstract\"] = abstract.split(\"Abstract\")[1]\n",
    "\n",
    "section_contents[\"Abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We have shown how, in both toy models and general settings, that representation enables generalization\\nwhen it reﬂects structure in the data. We developed an effective theory of representation learning\\ndynamics (in a toy setting) which predicts the critical dependence of learning on the training data\\nfraction. We then presented four learning phases (comprehension, grokking, memorization and\\nconfusion) which depend on the decoder capacity and learning speed (given by, among other things,\\nlearning rate and weight decay) in decoder-only architectures. While we have mostly focused on a\\ntoy model, we ﬁnd preliminary evidence that our results generalize to the setting of [1].\\nOur work can be viewed as a step towards a statistical physics of deep learning , connecting the\\n“microphysics” of low-level network dynamics with the “thermodynamics” of high-level model\\nbehavior. We view the application of theoretical tools from physics, such as effective theories [ 24], to\\nbe a rich area for further work. The broader impact of such work, if successful, could be to make\\nmodels more transparent and predictable [ 23,25,26], crucial to the task of ensuring the safety of\\nadvanced AI systems.\\n10'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract references from conclusion\n",
    "conclusion = section_contents[\"6 Conclusion\"]\n",
    "section_contents[\"6 Conclusion\"] = conclusion.split(\"References\")[0]\n",
    "# conclusion.split(\"References\")[1]\n",
    "\n",
    "section_contents[\"6 Conclusion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['3 Why Generalization Occurs: Representations and Dynamics', '3.1 Representation quality predicts generalization for the toy model', '3.2 The dynamics of embedding vectors'])\n",
      "dict_keys(['4 Delayed Generalization: A Phase Diagram', '4.1 Phase diagram of a toy model', '4.2 Beyond the toy model\\nWe conjecture that many of the principles which we saw dictate the training dynamics in the toy', '4.3 Grokking Experiment on MNIST'])\n"
     ]
    }
   ],
   "source": [
    "# run subsection splitting algo\n",
    "\n",
    "for title, section in section_contents.items():\n",
    "  pattern = r\"\\n\\d+\\.\\d+ [A-Z][a-zA-Z\\s:]+\\n\"\n",
    "  subsections_headers = re.findall(pattern, section)\n",
    "  \n",
    "  if len(subsections_headers) > 0:\n",
    "    subsection_map = {}\n",
    "    subsections_content = re.split(pattern, section)\n",
    "\n",
    "    subsections_headers.insert(0, title)\n",
    "    for subsection_header, subsection in zip(subsections_headers, subsections_content):\n",
    "      subsection_title = subsection_header.strip()\n",
    "      content = subsection.strip()\n",
    "      subsection_map[subsection_title] = content\n",
    "\n",
    "    print(subsection_map.keys())\n",
    "    section_contents[title] = subsection_map\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(section_contents[\"3 Why Generalization Occurs: Representations and Dynamics\"])\n",
    "len(section_contents[\"4 Delayed Generalization: A Phase Diagram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training details We update the representation and the decoder with different optimizers. For the\\n1D embeddings, we use the Adam optimizer with learning rate [10−5,10−2]and zero weight decay.\\nFor the decoder, we use an AdamW optimizer with the learning rate in [10−5,10−2]and the weight\\ndecay in [0,10](regression) or [0,20](classiﬁcation). For training/validation spliting, we choose\\n45/10 for non-modular addition ( p= 10 ) and 24/12 for the permutation group S3. We hard-code\\naddition or matrix multiplication (details in Appendix H) in the decoder for the addition group and\\nthe permutation group, respectively.\\nFor each choice of learning rate and weight decay, we compute the number of steps to reach high\\n(90%) training/validation accuracy. The 2D plane is split into four phases: comprehension ,grokking ,\\nmemorization andconfusion , deﬁned in Table 1 in Appendix A. Both comprehension and grokking are\\nable to generalize (in the “Goldilocks zone”), although the grokking phase has delayed generalization.\\nMemorization is also called overﬁtting, and confusion means failure to even memorize training data.\\nFigure 6 shows the phase diagrams for the addition group and the permutation group. They display\\nquite rich phenomena.\\nCompetition between representation learning and decoder overﬁtting In the regression setup of\\nthe addition dataset, we show how the competition between representation learning and decoder\\nlearning (which depend on both learning rate and weight decay, among other things) lead to different\\nlearning phases in Figure 6 (a). As expected, a fast decoder coupled with slow representation learning\\n(bottom right) lead to memorization. In the opposite extreme, although an extremely slow decoder\\ncoupled with fast representation learning (top left) will generalize in the end, the generalization time is\\nlong due to the inefﬁcient decoder training. The ideal phase (comprehension) requires representation\\nlearning to be faster, but not too much, than the decoder.\\nDrawing from an analogy to physical systems, one can think of embedding vectors as a group of\\nparticles. In our effective theory from Section 3.2, the dynamics of the particles are described only\\nby their relative positions, in that sense, structure forms mainly due to inter-particle interactions (in\\nreality, these interactions are mediated by the decoder and the loss). The decoder plays the role of an\\nenvironment exerting external forces on the embeddings. If the magnitude of the external forces are\\nsmall/large one can expect better/worse representations.\\n71e-5 1e-4 1e-3 1e-2\\ndecoder learning rate1e-2\\n1e-3\\n1e-4\\n1e-5representation learning ratecomprehension\\nmemorizationgrokking(a) Addition, regression\\n1e-5 1e-4 1e-3 1e-2\\ndecoder learning rate0\\n5\\n10 weight decay\\ncomprehensionmemorization\\ngrokking\\nconfusion (b) Addition, regression\\n1e-5 1e-4 1e-3 1e-2\\ndecoder learning rate0\\n10\\n20 weight decaycomprehensionmemorization\\ngrokking\\nconfusion\\n(c) Addition, classiﬁcation\\n1e-5 1e-4 1e-3 1e-2\\ndecoder learning rate0\\n10\\n200 weight decaycomprehensionmemorization\\ngrokking\\nconfusion (d) Permutation, regression\\nFigure 6: Phase diagrams of learning for the addition group and the permutation group. (a) shows the\\ncompetition between representation and decoder. (b)(c)(d): each phase diagram contains four phases:\\ncomprehension, grokking, memorization and confusion, deﬁned in Table 1. In (b)(c)(d), grokking is\\nsandwiched between comprehension and memorization.\\nUniversality of phase diagrams We ﬁx the embedding learning rate to be 10−3and sweep instead\\ndecoder weight decay in Figure 6 (b)(c)(d). The phase diagrams correspond to addition regression (b),\\naddition classiﬁcation (c) and permutation regression (d), respectively. Common phenomena emerge\\nfrom these different tasks: (i) they all include four phases; (ii) The top right corner (a fast and capable\\ndecoder) is the memorization phase; (iii) the bottom right corner (a fast and simple decoder) is the\\nconfusion phase; (iv) grokking is sandwiched between comprehension and memorization, which\\nseems to imply that it is an undesirable phase that stems from improperly tuned hyperparameters.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use this for section title-based splitting\n",
    "section_contents[\"4 Delayed Generalization: A Phase Diagram\"]['4.1 Phase diagram of a toy model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abstract': '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.',\n",
       " '1 Introduction': 'Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       " '2 Problem Setting': 'Power et al. [ 1] observe grokking on a less common task – learning “algorithmic” binary operations.\\nGiven some binary operation ◦, a network is tasked with learning the map (a,b)↦→cwherec=a◦b.\\nThey use a decoder-only transformer to predict the second to last token in a tokenized equation of the\\nform “<lhs> <op> <rhs> <eq> <result> <eos>”. Each token is represented as a 256-dimensional\\nembedding vector. The embeddings are learnable and initialized randomly. After the transformer, a\\nﬁnal linear layer maps the output to class logits for each token.\\nToy Model We primarily study grokking in a simpler toy model, which still retains the key behaviors\\nfrom the setup of [ 1]. Although [ 1] treated this as a classiﬁcation task, we study both regression\\n(mean-squared error) and classiﬁcation (cross-entropy). The basic setup is as follows: our model\\ntakes as input the symbols a,band maps them to trainable embedding vectors Ea,Eb∈Rdin. It\\nthen sums Ea,Eband sends the resulting vector through a “decoder” MLP. The target output vector,\\ndenoted Yc∈Rdoutis a ﬁxed random vector (regression task) or a one-hot vector (classiﬁcation\\ntask). Our model architecture can therefore be compactly described as (a,b)↦→Dec(Ea+Eb),\\nwhere the embeddings E∗and the decoder are trainable. Despite its simplicity, this toy model can\\ngeneralize to all abelian groups (discussed in Appendix B). In sections 3-4.1, we consider only the\\nbinary operation of addition. We consider modular addition in Section 4.2 to generalize some of our\\nresults to a transformer architecture and study general non-abelian operations in Appendix H.\\nDataset In our toy setting, we are concerned with learning the addition operation. A data sample\\ncorresponding to i+jis denoted as (i,j)for simplicity. If i,j∈{0,...,p−1}, there are in total\\np(p+ 1)/2different samples since we consider i+jandj+ito be the same sample. A dataset D\\nis a set of non-repeating data samples. We denote the full dataset as D0and split it into a training\\ndatasetDand a validation dataset D′, i.e.,D⋃D′=D0,D⋂D′=∅. We deﬁne training data\\nfraction =|D|/|D0|where|·|denotes the cardinality of the set.\\n1Project code can be found at: https://github.com/ejmichaud/grokking-squared\\n2RQI: 0.0 — Accuracy - train: 1.0, validation: 0.1\\n02468101214161820(a) Memorization in toy addition\\nRQI: 0.6 — Accuracy - train: 1.0, validation: 0.9\\n02468101214161820 (b) Generalization in toy addition\\nAccuracy - train: 1.0, validation: 0.1\\n0246810\\n(c) Memorization in toy modular addition\\nAccuracy - train: 1.0, validation: 0.9\\n0246810 (d) Generalization in toy modular addition\\nFigure 2: Visualization of the learned set of embeddings ( p= 11 ) and the decoder function associated\\nwith it for the case of 2D embeddings. Axes refer to each dimension of the learned embeddings. The\\ndecoder is evaluated on a grid of points in embedding-space and the color at each point represents\\nthe highest probability class. For visualization purposes, the decoder is trained on inputs of the form\\n(Ei+Ej)/2. One can read off the output of the decoder when fed the operation i◦jfrom this ﬁgure\\nsimply by taking the midpoint between the respective embeddings of iandj.',\n",
       " '3 Why Generalization Occurs: Representations and Dynamics': {'3 Why Generalization Occurs: Representations and Dynamics': 'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.',\n",
       "  '3.1 Representation quality predicts generalization for the toy model': 'A rigorous deﬁnition for structure in the learned representation is necessary. We propose the following\\ndeﬁnition,\\nDeﬁnition 1. (i,j,m,n )is aδ-parallelogram in the representation R≡[E0,···,Ep−1]if\\n|(Ei+Ej)−(Em+En)|≤δ.\\nIn the following derivations, we can take δ, which is a small threshold to tolerate numerical errors, to\\nbe zero.\\nProposition 1. When the training loss is zero, any parallelogram (i,j,m,n )in representation R\\nsatisﬁesi+j=m+n.\\nProof. Suppose that this is not the case, i.e., suppose Ei+Ej=Em+Enbuti+j̸=m+n, then\\nYi+j= Dec( Ei+Ej) = Dec( Em+En) =Ym+nwhere the ﬁrst and last equalities come from\\nthe zero training loss assumption. However, since i+j̸=m+n, we have Yi+j̸=Yn+m(almost\\nsurely in the regression task), a contradiction.\\n30.0 0.2 0.4 0.6 0.8 1.0\\ntraining data fraction0.00.20.40.60.81.0Acc\\nMemorization(a)\\n0.0 0.2 0.4 0.6 0.8 1.0\\ntraining set fraction0.00.20.40.60.81.0ˆAcc(b)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nˆAcc0.00.20.40.60.81.0Acc(c)Figure 3: We compute accuracy (of the full dataset) either measured empirically Acc, or predicted\\nfrom the representation of the embeddings ˆAcc. These two accuracies as a function of training data\\nfraction are plotted in (a)(b), and their agreement is shown in (c).\\nIt is convenient to deﬁne the permissible parallelogram set associated with a training dataset D\\n(“permissible” means consistent with 100% training accuracy) as\\nP0(D) ={(i,j,m,n )|(i,j)∈D,(m,n)∈D, i+j=m+n}. (1)\\nFor simplicity, we denote P0≡P0(D0). Given a representation R, we can check how many\\npermissible parallelograms actually exist in Rwithin error δ, so we deﬁne the parallelogram set\\ncorresponding to Ras\\nP(R,δ) ={(i,j,m,n )|(i,j,m,n )∈P0,|(Ei+Ej)−(Em+En)|≤δ}. (2)\\nFor brevity we will write P(R), suppressing the dependence on δ. We deﬁne the representation\\nquality index (RQI) as\\nRQI(R) =|P(R)|\\n|P0|∈[0,1]. (3)\\nWe will use the term linear representation orlinear structure to refer to a representation whose\\nembeddings are of the form Ek=a+kb(k= 0,···,p−1;a,b∈Rdin). A linear representation\\nhasRQI = 1 , while a random representation (sampled from, say, a normal dstribution) has RQI = 0\\nwith high probability.\\nQuantitatively, we denote the “predicted accuracy” ˆAcc as the accuracy achievable on the whole\\ndataset given the representation R(see Appendix D for the full details). In Figure 3, we see that\\nthe predictedˆAcc aligns well with the true accuracy Acc, establishing good evidence that structured\\nrepresentation of input embeddings leads to generalization. We use an example to illustrate the origin\\nof generalization here. In the setup of Figure 2 (b), suppose the decoder can achieve zero training\\nloss and E6+E8is a training sample hence Dec(E6+E8) =Y14. At validation time, the decoder\\nis tasked with predicting a validation sample E5+E9. Since (5,9,6,8)forms a parallelogram\\nsuch that E5+E9=E6+E8, the decoder can predict the validation sample correctly because\\nDec(E5+E9) = Dec( E6+E8) =Y14.',\n",
       "  '3.2 The dynamics of embedding vectors': 'Suppose that we have an ideal model M∗= (Dec∗,R∗)such that:2\\n• (1)M∗can achieve zero training loss;\\n• (2)M∗has an injective decoder, i.e., Dec∗(x1)̸= Dec∗(x2)for any x1̸=x2.\\nThen Proposition 2 provides a mechanism for the formation of parallelograms.\\n2One can verify a posteriori if a trained model Mis close to being an ideal model M∗. Please refer to\\nAppendix E for details.\\n40.00.20.40.60.81.0\\ntraining data fraction0.00.20.40.60.81.0Probability(linear structure) rc= 0.4(a) Theory: phase transition\\n0.4 0.6 0.8 1.0\\ntraining data fraction102103104Steps to RQI >0.95\\nrc= 0.4(b) Empirical: phase transition\\nRuns that reached\\nRQI>0.95 in 104steps\\nRuns that didn’t reach\\nRQI>0.95 in 104steps\\n0 500 1000 1500 2000\\nstep−1.5−1.0−0.50.00.51.01.51D representation\\n0123456789\\n3nh(c) Theory: trajectory\\n0 500 1000 1500 2000\\nstep−1.5−1.0−0.50.00.51.01.51D normalized representation 0123456789\\n3nh(d) Empirical: trajectoryFigure 4: (a) The effective theory predicts a phase transition in the probability of obtaining a linear\\nrepresentation around rc= 0.4. (b) Empirical results display a phase transition of RQI around\\nrc= 0.4, in agreement with the theory (the blue line shows the median of multiple random seeds).\\nThe evolution of 1D representations predicted by the effective theory or obtained from neural network\\ntraining (shown in (c) and (d) respectively) agree creditably well.\\nProposition 2. If a training set Dcontains two samples (i,j)and(m,n)withi+j=m+n,\\nthenM∗learns a representation R∗such that Ei+Ej=Em+En, i.e., (i,j,m,n )forms a\\nparallelogram.\\nProof. Due to the zero training loss assumption, we have Dec∗(Ei+Ej) =Yi+j=Ym+n=\\nDec∗(Em+En). Then the injectivity of Dec∗implies Ei+Ej=Em+En.\\nThe dynamics of the trained embedding vectors are determined by various factors interacting in\\ncomplex ways, for instance: the details of the decoder architecture, the optimizer hyperparameters,\\nand the various kinds of implicit regularization induced by the training procedure. We will see that\\nthe dynamics of normalized quantities, namely, the normalized embeddings at time t, deﬁned as\\n˜E(t)\\nk=E(t)\\nk−µt\\nσt, whereµt=1\\np∑\\nkE(t)\\nkandσt=1\\np∑\\nk|E(t)\\nk−µt|2, can be qualitatively described\\nby a simple effective loss (in the physics effective theory sense). We will assume that the normalized\\nembedding vectors obey a gradient ﬂow for an effective loss function of the form\\nd˜Ei\\ndt=−∂ℓeff\\n∂˜Ei, (4)\\nℓeff=ℓ0\\nZ0, ℓ 0≡∑\\n(i,j,m,n )∈P0(D)|˜Ei+˜Ej−˜Em−˜En|2/|P0(D)|, Z 0≡∑\\nk|˜Ek|2, (5)\\nwhere|·|denotes Euclidean vector norm. Note that the embeddings do not collapse to the trivial\\nsolution E0=···=Ep−1= 0unless initialized as such, because two conserved quantities exist, as\\nproven in Appendix F:\\nC=∑\\nkEk, Z 0=∑\\nk|Ek|2. (6)\\nWe shall now use the effective dynamics to explain empirical observations such as the existence of a\\ncritical training set size for generalization.\\nDegeneracy of ground states (loss optima) We deﬁne ground states as those representations satis-\\nfyingℓeff= 0, which requires the following linear equations to hold:\\nA(P) ={Ei+Ej=Em+En|(i,j,m,n )∈P}. (7)\\nSince each embedding dimension obeys the same set of linear equations, we will assume, without loss\\nof generality, that din= 1. The dimension of the null space of A(P), denoted as n0, is the number of\\ndegrees of freedom of the ground states. Given a set of parallelograms implied by a training dataset\\nD, the nullity of A(P(D))could be obtained by computing the singular values 0≤σ1≤···≤σp.\\nWe always have n0≥2, i.e.,σ1=σ2= 0because the nullity of A(P0), the set of linear equations\\ngiven by all possible parallelograms, is Nullity(A(P0)) = 2 which can be attributed to two degrees\\n5of freedom (translation and scaling). If n0= 2, the representation is unique up to translations and\\nscaling factors, and the embeddings have the form Ek=a+kb. Otherwise, when n0>2, the\\nrepresentation is not constrained enough such that all the embeddings lie on a line.\\nWe present theoretical predictions alongside empirical results for addition ( p= 10 ) in Figure 4. As\\nshown in Figure 4 (a), our effective theory predicts that the probability that the training set implies a\\nunique linear structure (which would result in perfect generalization) depends on the training data\\nfraction and has a phase transition around rc= 0.4. Empirical results from training different models\\nare shown in Figure 4 (b). The number of steps to reach RQI>0.95is seen to have a phase transition\\natrc= 0.4, agreeing with the proposed effective theory and with the empirical ﬁndings in [1].\\nTime towards the linear structure We deﬁne the Hessian matrix of ℓ0as\\nHij=1\\nZ0∂2ℓ0\\n∂Ei∂Ej, (8)\\nNote thatℓeﬀ=1\\n2RTHR,R= [E0,E1,···,Ep−1], so the gradient descent is linear, i.e.,\\ndR\\ndt=−HR. (9)\\nIfHhas eigenvalues λi=σ2\\ni(sorted in increasing order) and eigenvectors ¯vi, and we have the initial\\ncondition R(t= 0) =∑\\niai¯vi, then we have R(t) =∑\\niai¯vie−λit. The ﬁrst two eigenvalues\\nvanish andth= 1/λ3determines the timescale for the slowest component to decrease by a factor\\nofe. We callλ3thegrokking rate . When the step size is η, the corresponding number of steps is\\nnh=th/η= 1/(λ3η).\\nWe verify the above analysis with empirical results. Figure 4 (c)(d) show the trajectories obtained\\nfrom the effective theory and from neural network training, respectively. The 1D neural representation\\nin Figure 4 (d) are manually normalized to zero mean and unit variance. The two trajectories agree\\nqualitatively, and it takes about 3nhsteps for two trajectories to converge to the linear structure. The\\nquantitative differences might be due to the absence of the decoder in the effective theory, which\\nassumes the decoder to take inﬁnitesimal step sizes.\\nDependence of grokking on data size Note thatℓeﬀinvolves averaging over parallelograms in the\\ntraining set, it is dependent on training data size, so is λ3. In Figure 5 (a), we plot the dependence of\\nλ3on training data fraction. There are many datasets with the same data size, so λ3is a probabilistic\\nfunction of data size.\\nTwo insights on grokking can be extracted from this plot: (i) When the data fraction is below some\\nthreshold (around 0.4), λ3is zero with high probability, corresponding to no generalization. This\\nagain veriﬁes our critical point in Figure 4. (ii) When data size is above the threshold, λ3(on average)\\nis an increasing function of data size. This implies that grokking time t∼1/λ3decreases as training\\ndata size becomes larger, an important observation from [1].\\nTo verify our effective theory, we compare the grokking steps obtained from real neural network\\ntraining (deﬁned as steps to RQI>0.95), and those predicted by our theory tth∼1\\nλ3η(ηis the\\nembedding learning rate), shown in Figure 5 (b). The theory agrees qualitatively with neural networks,\\nshowing the trend of decreasing grokking steps as increasing data size. The quantitative differences\\nmight be explained as the gap between our effective loss and actual loss.\\nLimitations of the effective theory While our theory deﬁnes an effective loss based on the Euclidean\\ndistance between embeddings Ei+EjandEn+Em, one could imagine generalizing the theory to\\ndeﬁne a broader notion of parallogram given by some other metric on the representation space. For\\ninstance, if we have a decoder like in Figure 2 (d) then the distance between distinct representations\\nwithin the same “pizza slice” is low, meaning that representations arranged not in parallelograms\\nw.r.t. the Euclidean metric may be parallelograms with respect to the metric deﬁned by the decoder.\\n4 Delayed Generalization: A Phase Diagramw.r.t. the Euclidean metric may be parallelograms with respect to the metric deﬁned by the decoder.'},\n",
       " '4 Delayed Generalization: A Phase Diagram': {'4 Delayed Generalization: A Phase Diagram': 'So far, we have (1) observed empirically that generalization on algorithmic datasets corresponds with\\nthe emergence of well-structured representations, (2) deﬁned a notion of representation quality in a\\ntoy setting and shown that it predicts generalization, and (3) developed an effective theory to describe\\n60.0 0.2 0.4 0.6 0.8 1.0\\ntraining data fraction0.00.10.20.30.4grokking rate λ3(a)\\n10−210−1100\\ngrokking rate λ3103104Steps to RQI >0.95tth= 1\\n/(2λ3η)\\nRuns that didn’t reach RQI >0.95 in 104steps\\nRuns that reached RQI >0.95 in 104steps (b)\\nFigure 5: Effective theory explains the dependence of grokking time on data size, for the addition\\ntask. (a) Dependence of λ3on training data fraction. Above the critical data fraction (around 0.4),\\nas data size becomes larger, λ3increases hence grokking time t∼1/λ3(predicted by our effective\\ntheory) decreases. (b) Comparing grokking steps (deﬁned as RQI>0.95) predicted by the effective\\ntheory with real neural network results. η= 10−3is the learning rate of the embeddings.\\nthe learning dynamics of the representations in the same toy setting. We now study how optimizer\\nhyperparameters affect high-level learning performance. In particular, we develop phase diagrams for\\nhow learning performance depends on the representation learning rate, decoder learning rate and the\\ndecoder weight decay. These parameters are of interest since they most explicitly regulate a kind of\\ncompetition between the encoder and decoder, as we elaborate below.',\n",
       "  '4.1 Phase diagram of a toy model': 'Training details We update the representation and the decoder with different optimizers. For the\\n1D embeddings, we use the Adam optimizer with learning rate [10−5,10−2]and zero weight decay.\\nFor the decoder, we use an AdamW optimizer with the learning rate in [10−5,10−2]and the weight\\ndecay in [0,10](regression) or [0,20](classiﬁcation). For training/validation spliting, we choose\\n45/10 for non-modular addition ( p= 10 ) and 24/12 for the permutation group S3. We hard-code\\naddition or matrix multiplication (details in Appendix H) in the decoder for the addition group and\\nthe permutation group, respectively.\\nFor each choice of learning rate and weight decay, we compute the number of steps to reach high\\n(90%) training/validation accuracy. The 2D plane is split into four phases: comprehension ,grokking ,\\nmemorization andconfusion , deﬁned in Table 1 in Appendix A. Both comprehension and grokking are\\nable to generalize (in the “Goldilocks zone”), although the grokking phase has delayed generalization.\\nMemorization is also called overﬁtting, and confusion means failure to even memorize training data.\\nFigure 6 shows the phase diagrams for the addition group and the permutation group. They display\\nquite rich phenomena.\\nCompetition between representation learning and decoder overﬁtting In the regression setup of\\nthe addition dataset, we show how the competition between representation learning and decoder\\nlearning (which depend on both learning rate and weight decay, among other things) lead to different\\nlearning phases in Figure 6 (a). As expected, a fast decoder coupled with slow representation learning\\n(bottom right) lead to memorization. In the opposite extreme, although an extremely slow decoder\\ncoupled with fast representation learning (top left) will generalize in the end, the generalization time is\\nlong due to the inefﬁcient decoder training. The ideal phase (comprehension) requires representation\\nlearning to be faster, but not too much, than the decoder.\\nDrawing from an analogy to physical systems, one can think of embedding vectors as a group of\\nparticles. In our effective theory from Section 3.2, the dynamics of the particles are described only\\nby their relative positions, in that sense, structure forms mainly due to inter-particle interactions (in\\nreality, these interactions are mediated by the decoder and the loss). The decoder plays the role of an\\nenvironment exerting external forces on the embeddings. If the magnitude of the external forces are\\nsmall/large one can expect better/worse representations.\\n71e-5 1e-4 1e-3 1e-2\\ndecoder learning rate1e-2\\n1e-3\\n1e-4\\n1e-5representation learning ratecomprehension\\nmemorizationgrokking(a) Addition, regression\\n1e-5 1e-4 1e-3 1e-2\\ndecoder learning rate0\\n5\\n10 weight decay\\ncomprehensionmemorization\\ngrokking\\nconfusion (b) Addition, regression\\n1e-5 1e-4 1e-3 1e-2\\ndecoder learning rate0\\n10\\n20 weight decaycomprehensionmemorization\\ngrokking\\nconfusion\\n(c) Addition, classiﬁcation\\n1e-5 1e-4 1e-3 1e-2\\ndecoder learning rate0\\n10\\n200 weight decaycomprehensionmemorization\\ngrokking\\nconfusion (d) Permutation, regression\\nFigure 6: Phase diagrams of learning for the addition group and the permutation group. (a) shows the\\ncompetition between representation and decoder. (b)(c)(d): each phase diagram contains four phases:\\ncomprehension, grokking, memorization and confusion, deﬁned in Table 1. In (b)(c)(d), grokking is\\nsandwiched between comprehension and memorization.\\nUniversality of phase diagrams We ﬁx the embedding learning rate to be 10−3and sweep instead\\ndecoder weight decay in Figure 6 (b)(c)(d). The phase diagrams correspond to addition regression (b),\\naddition classiﬁcation (c) and permutation regression (d), respectively. Common phenomena emerge\\nfrom these different tasks: (i) they all include four phases; (ii) The top right corner (a fast and capable\\ndecoder) is the memorization phase; (iii) the bottom right corner (a fast and simple decoder) is the\\nconfusion phase; (iv) grokking is sandwiched between comprehension and memorization, which\\nseems to imply that it is an undesirable phase that stems from improperly tuned hyperparameters.',\n",
       "  '4.2 Beyond the toy model\\nWe conjecture that many of the principles which we saw dictate the training dynamics in the toy': 'model also apply more generally. Below, we will see how our framework generalizes to transformer\\narchitectures for the task of addition modulo p, a minimal reproducible example of the original\\ngrokking paper [1].\\nWe ﬁrst encode p= 53 integers into 256D learnable embeddings, then pass two integers to a decoder-\\nonly transformer architecture. For simplicity, we do not encode the operation symbols here. The\\noutputs from the last layer are concatenated and passed to a linear layer for classiﬁcation. Training\\nboth the encoder and the decoder with the same optimizer (i.e., with the same hyperparameters)\\nleads to the grokking phenomenon. Generalization appears much earlier once we lower the effective\\ndecoder capacity with weight decay (full phase diagram in Figure 7).\\nEarly on, the model is able to perfectly ﬁt the training set while having no generalization. We study\\nthe embeddings at different training times and ﬁnd that neither PCA (shown in Figure 1) nor t-SNE\\n(not shown here) reveal any structure. Eventually, validation accuracy starts to increase, and perfect\\ngeneralization coincides with the PCA projecting the embeddings into a circle in 2D. Of course, no\\n8100102104\\nstep303540455055eﬀective dimension eS\\neS\\ngeneralization\\nmemorization\\n0.0 0.2 0.4\\ndropout rate101103105epochs to 90% accuracyvalidation\\ntrain\\nvalidation - train\\n3e-7 1e-4 4e-2\\nlearning rate1e-2\\n2e-1\\n2e+0\\n3e+1weight decay\\ncomprehensionmemorization\\ngrokking\\nconfusionFigure 7: Left: Evolution of the effective dimension of the embeddings (deﬁned as the exponential of\\nthe entropy) during training and evaluated over 100 seeds. Center: Effect of dropout on speeding up\\ngeneralization. Right: Phase diagram of the transformer architecture. A scan is performed over the\\nweight decay and learning rate of the decoder while the learning rate of the embeddings is kept ﬁxed\\nat10−3(with zero weight decay).\\nchoice of dimensionality reduction is guaranteed to ﬁnd any structure, and thus, it is challenging\\nto show explicitly that generalization only occurs when a structure exists. Nevertheless, the fact\\nthat, when coupled with the implicit regularization of the optimizer for sparse solutions, such a clear\\nstructure appears in a simple PCA so quickly at generalization time suggests that our analysis in\\nthe toy setting is applicable here as well. This is also seen in the evolution of the entropy of the\\nexplained variance ratio in the PCA of the embeddings (deﬁned as S=−∑\\niσilogσiwhereσiis\\nthe fractional variance explained by the ith principal component). As seen in Figure 7, the entropy\\nincreases up to generalization time then decreases drastically afterwards which would be consistent\\nwith the conjecture that generalization occurs when a low-dimensional structure is discovered. The\\ndecoder then primarily relies on the information in this low-dimensional manifold and essentially\\n“prunes” the rest of the high-dimensional embedding space. Another interesting insight appears when\\nwe project the embeddings at initialization onto the principal axes at the end of training. Some of the\\nstructure required for generalization exists before training hinting at a connection with the Lottery\\nTicket Hypothesis. See Appendix K for more details.\\nIn Figure 7 (right), we show a comparable phase diagram to Figure 6 evaluated now in the transformer\\nsetting. Note that, as opposed to the setting in [ 1], weight decay has only been applied to the decoder\\nand not to the embedding layer. Contrary to the toy model, a certain amount of weight decay proves\\nbeneﬁcial to generalization and speeds it up signiﬁcantly. We conjecture that this difference comes\\nfrom the different embedding dimensions. With a highly over-parameterized setting, a non-zero\\nweight decay gives a crucial incentive to reduce complexity in the decoder and help generalize in\\nfewer steps. This is subject to further investigation. We also explore the effect of dropout layers\\nin the decoder blocks of the transformer. With a signiﬁcant dropout rate, the generalization time\\ncan be brought down to under 103steps and the grokking phenomenon vanishes completely. The\\noverall trend suggests that constraining the decoder with the same tools used to avoid overﬁtting\\nreduces generalization time and can avoid the grokking phenomenon. This is also observed in an\\nimage classiﬁcation task where we were able to induce grokking. See Appendix J for more details.',\n",
       "  '4.3 Grokking Experiment on MNIST': 'We now demonstrate, for the ﬁrst time, that grokking (signiﬁcantly delayed generalization) is a more\\ngeneral phenomenon in machine learning that can occur not only on algorithmic datasets, but also\\non mainstream benchmark datasets. In particular, we exhibit grokking on MNIST in Figure 8 and\\ndemonstrate that we can control grokking by varying optimization hyperparameters. More details on\\nthe experimental setup are in Appendix J.'},\n",
       " '5 Related work': 'Relatively few works have analyzed the phenomenon of grokking. [ 2] describe the circuit that\\ntransformers use to perform modular addition, track its formation over training, and broadly suggest\\nthat grokking is related to the phenomenon of “phase changes” in neural network training. [ 3,4]\\n9102103104105\\nOptimization Steps0.20.40.60.81.0AccuracyTrain Points: 1000 | Initialization Scale: 9.0\\ntrain\\nval(a)\\n1.00e-06 2.98e-05 8.86e-04 2.64e-02 7.85e-01\\nLast Layer Learning Rate1.00e-05\\n2.98e-04\\n8.86e-03\\n2.64e-01\\n7.85e+00Weight DecayMemorization\\nGrokking\\nComprehension\\nConfusion (b)\\nFigure 8: Left: Training curves for a run on MNIST, in the setting where we observe grokking. Right:\\nPhase diagram with the four phases of learning dynamics on MNIST.\\nprovided earlier speculative, informal conjectures on grokking [ 3,4]. Our work is related to the\\nfollowing broad research directions:\\nLearning mathematical structures [5] trains a neural network to learn arithmetic operation from\\npictures of digits, but they do not observe grokking due to their abundant training data. Beyond\\narithmetic relations, machine learning has been applied to learn other mathematical structures,\\nincluding geometry [6], knot theory [7] and group theory [8].\\nDouble descent Grokking is somewhat reminiscent of the phenomena of “epoch-wise” double\\ndescent [9], where generalization can improve after a period of overﬁtting. [ 10] ﬁnd that regularization\\ncan mitigate double descent, similar perhaps to how weight decay inﬂuences grokking.\\nRepresentation learning Representation learning lies at the core of machine learning [ 11–14].\\nRepresentation quality is usually measured by (perhaps vague) semantic meanings or performance on\\ndownstream tasks. In our study, the simplicity of arithmetic datasets allows us to deﬁne representation\\nquality and study evolution of representations in a quantitative way.\\nPhysics of learning Physics-inspired tools have proved to be useful in understanding deep learning\\nfrom a theoretical perspective. These tools include effective theories [ 15,16], conservation laws [ 17]\\nand free energy principle [ 18]. In addition, statistical physics has been identiﬁed as a powerful tool in\\nstudying generalization in neural networks [ 19–22]. Our work connects a low-level understanding of\\nmodels with their high-level performance. In a recent work, researchers at Anthropic [ 23], connect a\\nsudden decrease in loss during training with the emergence of induction heads within their models.\\nThey analogize their work to statistical physics , since it bridges a “microscopic”, mechanistic\\nunderstanding of networks with “macroscopic” facts about overall model performance.',\n",
       " '6 Conclusion': 'We have shown how, in both toy models and general settings, that representation enables generalization\\nwhen it reﬂects structure in the data. We developed an effective theory of representation learning\\ndynamics (in a toy setting) which predicts the critical dependence of learning on the training data\\nfraction. We then presented four learning phases (comprehension, grokking, memorization and\\nconfusion) which depend on the decoder capacity and learning speed (given by, among other things,\\nlearning rate and weight decay) in decoder-only architectures. While we have mostly focused on a\\ntoy model, we ﬁnd preliminary evidence that our results generalize to the setting of [1].\\nOur work can be viewed as a step towards a statistical physics of deep learning , connecting the\\n“microphysics” of low-level network dynamics with the “thermodynamics” of high-level model\\nbehavior. We view the application of theoretical tools from physics, such as effective theories [ 24], to\\nbe a rich area for further work. The broader impact of such work, if successful, could be to make\\nmodels more transparent and predictable [ 23,25,26], crucial to the task of ensuring the safety of\\nadvanced AI systems.\\n10'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Abstract', '1 Introduction', '2 Problem Setting', '3 Why Generalization Occurs: Representations and Dynamics', '4 Delayed Generalization: A Phase Diagram', '5 Related work', '6 Conclusion'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_contents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "# 1. Extract AIC for model to generate 10 questions to answer\n",
    "#  -> human intervention\n",
    "\n",
    "aic_dict = {}\n",
    "contents_arr = []\n",
    "for title, content in section_contents.items():\n",
    "      if title in [\"Abstract\", \"1 Introduction\", \"6 Conclusion\"]:\n",
    "            # aic_arr.append(title + \": \" + content)\n",
    "            aic_dict[title] = content\n",
    "      else:\n",
    "            # 2. Output array of other sections where LLM will \"look through\" to summarise and fetch facts based on questions guidance\n",
    "            if isinstance(content, dict):\n",
    "                  section_paragraphs = \"\"\n",
    "                  \n",
    "                  for sub_title, sub_content in content.items():\n",
    "                    section_paragraphs += sub_title + \": \" + sub_content + \"\\n\"\n",
    "                  contents_arr.append(section_paragraphs)\n",
    "            else: \n",
    "                 contents_arr.append(title + \": \" + content) \n",
    "            \n",
    "aic_dict[\"Abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text, model=\"ada_gcal\"): # model = \"deployment_name\"\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "# df_bills['ada_v2'] = df_bills[\"text\"].apply(lambda x : generate_embeddings (x, model = 'text-embedding-ada-002'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "azure_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "                api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "                api_type=\"azure\",\n",
    "                api_version = \"2024-02-01\",\n",
    "                model_name=\"ada_gcal\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "# Example setup of the client to connect to your chroma server\n",
    "client = chromadb.HttpClient(host='localhost', port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_collection(name=\"test_pdf_2\") \n",
    "# Delete a collection and all associated embeddings, documents, and metadata. ⚠️ This is destructive and not reversible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection = client.create_collection(\n",
    "#         embedding_function=azure_ef,\n",
    "#         name=\"test_pdf_2\",\n",
    "#         metadata={\"hnsw:space\": \"l2\"} # l2 is the default\n",
    "#     )\n",
    "collection = client.get_collection(name=\"test_pdf_2\", embedding_function=azure_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['06b4e998-2155-4875-a2dc-f449ab7ed19e',\n",
       "  '32c9fd9b-fbd9-4e36-b85f-e10b5c6938c1',\n",
       "  '3df1fa1f-b1a7-40b8-8fcb-51a0dab998ea',\n",
       "  '502463a7-4688-4dd0-9d6f-9a54b0add4b7',\n",
       "  'c62d5a19-2510-4bd3-94cc-94f1bd268fbc',\n",
       "  'db9b51c0-a91e-4d1a-8042-4f5e21d308d5',\n",
       "  'e4450671-f54e-4bf8-bf90-908eeefcf1b9'],\n",
       " 'embeddings': [[-0.02222548983991146,\n",
       "   -0.0005509211914613843,\n",
       "   0.04491744190454483,\n",
       "   -0.01904258131980896,\n",
       "   -0.009329218417406082,\n",
       "   -0.001376231200993061,\n",
       "   0.01250526774674654,\n",
       "   -0.012532707303762436,\n",
       "   -0.013966388069093227,\n",
       "   -0.048731446266174316,\n",
       "   0.02083982713520527,\n",
       "   0.032789457589387894,\n",
       "   -0.008183645084500313,\n",
       "   0.007895536720752716,\n",
       "   -0.014377971179783344,\n",
       "   0.006897447630763054,\n",
       "   0.03424371778964996,\n",
       "   0.019152335822582245,\n",
       "   -0.0026324172504246235,\n",
       "   -0.016024304553866386,\n",
       "   -0.01476211566478014,\n",
       "   0.03665833920240402,\n",
       "   -0.0021762459073215723,\n",
       "   -0.0278916172683239,\n",
       "   -0.0037762753199785948,\n",
       "   0.005604390520602465,\n",
       "   0.01585967093706131,\n",
       "   -0.030896175652742386,\n",
       "   -0.018095938488841057,\n",
       "   0.01978342980146408,\n",
       "   0.020208733156323433,\n",
       "   0.0022174043115228415,\n",
       "   -0.004403939936310053,\n",
       "   -0.0030885885935276747,\n",
       "   -0.01852124184370041,\n",
       "   -0.015585281886160374,\n",
       "   0.026231566444039345,\n",
       "   -0.01462492160499096,\n",
       "   -0.0031434663105756044,\n",
       "   -0.0053231422789394855,\n",
       "   0.034298595041036606,\n",
       "   0.011524328030645847,\n",
       "   0.0022294088266789913,\n",
       "   -0.02197854034602642,\n",
       "   -0.005220246501266956,\n",
       "   0.0033766967244446278,\n",
       "   0.01122250035405159,\n",
       "   -0.015393209643661976,\n",
       "   -0.010591406375169754,\n",
       "   0.049911316484212875,\n",
       "   0.017382528632879257,\n",
       "   0.02547699771821499,\n",
       "   -0.01817825622856617,\n",
       "   -0.0015288599533960223,\n",
       "   -0.015063943341374397,\n",
       "   -0.004726346582174301,\n",
       "   0.009946593083441257,\n",
       "   0.002119653159752488,\n",
       "   0.039429664611816406,\n",
       "   -0.02281542681157589,\n",
       "   0.0035773436538875103,\n",
       "   -0.003364692209288478,\n",
       "   0.005065902601927519,\n",
       "   0.01911117695271969,\n",
       "   0.002649566624313593,\n",
       "   0.0005234823329374194,\n",
       "   -0.03569797798991203,\n",
       "   0.0005059043178334832,\n",
       "   -0.020661473274230957,\n",
       "   -0.0071752662770450115,\n",
       "   0.0200029406696558,\n",
       "   0.023240728303790092,\n",
       "   -0.030210202559828758,\n",
       "   -0.014350532554090023,\n",
       "   0.03166446462273598,\n",
       "   -0.015475526452064514,\n",
       "   -0.006821990944445133,\n",
       "   -0.004681758116930723,\n",
       "   -0.009980891831219196,\n",
       "   0.017972463741898537,\n",
       "   -0.003011416643857956,\n",
       "   -0.021621834486722946,\n",
       "   0.011455731466412544,\n",
       "   0.028207166120409966,\n",
       "   -0.006410407368093729,\n",
       "   -0.015516684390604496,\n",
       "   0.00859522819519043,\n",
       "   -0.014172179624438286,\n",
       "   0.0007438507745973766,\n",
       "   0.005552942864596844,\n",
       "   0.004218727350234985,\n",
       "   0.0008943359134718776,\n",
       "   0.028975453227758408,\n",
       "   0.01488559041172266,\n",
       "   -0.005148219410330057,\n",
       "   0.005460336338728666,\n",
       "   -0.01795874536037445,\n",
       "   0.01197707001119852,\n",
       "   -0.03125287964940071,\n",
       "   -0.021320007741451263,\n",
       "   -0.03388701379299164,\n",
       "   0.009679064154624939,\n",
       "   0.0014491156907752156,\n",
       "   -0.026382479816675186,\n",
       "   -0.032322995364665985,\n",
       "   -0.002376892603933811,\n",
       "   0.017629478126764297,\n",
       "   -0.005552942864596844,\n",
       "   0.0022259787656366825,\n",
       "   -0.017149297520518303,\n",
       "   -0.0078063602559268475,\n",
       "   0.03561566025018692,\n",
       "   -0.012635602615773678,\n",
       "   -0.020702632144093513,\n",
       "   0.007346759084612131,\n",
       "   0.006763683166354895,\n",
       "   0.013383312150835991,\n",
       "   -0.01832916960120201,\n",
       "   -0.004897839389741421,\n",
       "   -0.03246019035577774,\n",
       "   0.02710960991680622,\n",
       "   -0.010207261890172958,\n",
       "   0.00972022209316492,\n",
       "   0.0017097849631682038,\n",
       "   0.008094469085335732,\n",
       "   -0.004661179147660732,\n",
       "   -0.010118085891008377,\n",
       "   -0.013054045848548412,\n",
       "   -0.012745358049869537,\n",
       "   -0.015050224028527737,\n",
       "   -0.006430986803025007,\n",
       "   0.012731638737022877,\n",
       "   0.004523984622210264,\n",
       "   0.01778039149940014,\n",
       "   -0.013918370008468628,\n",
       "   0.025092853233218193,\n",
       "   0.00981625821441412,\n",
       "   -0.003371551865711808,\n",
       "   -0.022760547697544098,\n",
       "   -0.03635651245713234,\n",
       "   0.003858592128381133,\n",
       "   -0.002906806068494916,\n",
       "   -0.010461071506142616,\n",
       "   -0.014268215745687485,\n",
       "   -0.012251458130776882,\n",
       "   0.028646187856793404,\n",
       "   0.004849821329116821,\n",
       "   0.0463717021048069,\n",
       "   0.004472536966204643,\n",
       "   -0.021292569115757942,\n",
       "   0.0009775100043043494,\n",
       "   -0.0008184502366930246,\n",
       "   0.0008741854690015316,\n",
       "   0.0023185850586742163,\n",
       "   0.02732912078499794,\n",
       "   0.012182861566543579,\n",
       "   0.029661426320672035,\n",
       "   0.04019795358181,\n",
       "   -0.01765691675245762,\n",
       "   0.009377236478030682,\n",
       "   -0.007861237972974777,\n",
       "   -0.0063623893074691296,\n",
       "   -0.01712185889482498,\n",
       "   -0.004572002682834864,\n",
       "   0.010220981203019619,\n",
       "   0.026794062927365303,\n",
       "   0.01907001994550228,\n",
       "   -0.0010881229536607862,\n",
       "   0.011586065404117107,\n",
       "   -0.020935863256454468,\n",
       "   -0.0006045127520337701,\n",
       "   0.027740703895688057,\n",
       "   -0.008368857204914093,\n",
       "   0.024009017273783684,\n",
       "   0.0038620219565927982,\n",
       "   0.010776618495583534,\n",
       "   0.028646187856793404,\n",
       "   -0.0029462494421750307,\n",
       "   -0.034792494028806686,\n",
       "   -0.017259053885936737,\n",
       "   0.004403939936310053,\n",
       "   0.011860454455018044,\n",
       "   -0.0035842033103108406,\n",
       "   -0.003374981926754117,\n",
       "   -0.013273556716740131,\n",
       "   -0.005755304358899593,\n",
       "   0.030127886682748795,\n",
       "   0.005841050762683153,\n",
       "   -0.008787300437688828,\n",
       "   0.015996865928173065,\n",
       "   -0.01830173097550869,\n",
       "   0.009747660718858242,\n",
       "   -0.026574552059173584,\n",
       "   0.03053946979343891,\n",
       "   -0.6273624897003174,\n",
       "   -0.010714881122112274,\n",
       "   0.015269734896719456,\n",
       "   -0.03539615124464035,\n",
       "   -0.0043422020971775055,\n",
       "   -0.003460728330537677,\n",
       "   0.01573619619011879,\n",
       "   0.0022911462001502514,\n",
       "   -0.008478612639009953,\n",
       "   0.037454064935445786,\n",
       "   -0.010797197930514812,\n",
       "   -0.012045666575431824,\n",
       "   0.0011695821303874254,\n",
       "   0.0037419768050312996,\n",
       "   -0.01931696943938732,\n",
       "   -0.02828948199748993,\n",
       "   0.0035739135928452015,\n",
       "   -0.007072370499372482,\n",
       "   0.03358518332242966,\n",
       "   0.02269195020198822,\n",
       "   -0.009171444922685623,\n",
       "   0.009967171587049961,\n",
       "   0.0015863100998103619,\n",
       "   0.008938213810324669,\n",
       "   -0.00036720934440381825,\n",
       "   0.002891371725127101,\n",
       "   0.009836837649345398,\n",
       "   -0.002971973270177841,\n",
       "   0.017574600875377655,\n",
       "   0.009150865487754345,\n",
       "   -0.04469792917370796,\n",
       "   -0.0014868441503494978,\n",
       "   0.017053261399269104,\n",
       "   0.013609683141112328,\n",
       "   0.04489000141620636,\n",
       "   0.0016506199026480317,\n",
       "   -0.011881033889949322,\n",
       "   0.04052722081542015,\n",
       "   0.0054500470869243145,\n",
       "   0.031362637877464294,\n",
       "   -0.01595570705831051,\n",
       "   -0.027960214763879776,\n",
       "   0.016888627782464027,\n",
       "   0.015310892835259438,\n",
       "   -0.02241756208240986,\n",
       "   0.013657701201736927,\n",
       "   0.0007794355624355376,\n",
       "   0.007202704902738333,\n",
       "   -0.022554757073521614,\n",
       "   -0.009075408801436424,\n",
       "   0.002268852200359106,\n",
       "   0.008019011467695236,\n",
       "   -0.0036528005730360746,\n",
       "   0.003349257865920663,\n",
       "   0.011490029282867908,\n",
       "   0.014062424190342426,\n",
       "   0.013884071260690689,\n",
       "   -0.016861189156770706,\n",
       "   0.012937430292367935,\n",
       "   -0.01765691675245762,\n",
       "   0.0024437750689685345,\n",
       "   -0.010426772758364677,\n",
       "   -0.008053310215473175,\n",
       "   0.004256455693393946,\n",
       "   -0.04645401984453201,\n",
       "   0.022582195699214935,\n",
       "   -0.019152335822582245,\n",
       "   -0.010735460557043552,\n",
       "   0.005762164015322924,\n",
       "   -0.030100448057055473,\n",
       "   0.016984663903713226,\n",
       "   0.041075997054576874,\n",
       "   0.005676417611539364,\n",
       "   -0.002385467290878296,\n",
       "   -0.004486256279051304,\n",
       "   0.02828948199748993,\n",
       "   0.011723260395228863,\n",
       "   -0.008286540396511555,\n",
       "   0.019948063418269157,\n",
       "   0.022403843700885773,\n",
       "   -0.003971777390688658,\n",
       "   -0.008286540396511555,\n",
       "   -0.005535793490707874,\n",
       "   -0.0017063551349565387,\n",
       "   0.04049978032708168,\n",
       "   -0.020510559901595116,\n",
       "   0.0014645500341430306,\n",
       "   0.005930227227509022,\n",
       "   -0.015599001199007034,\n",
       "   -0.007820080034434795,\n",
       "   -0.01585967093706131,\n",
       "   0.007820080034434795,\n",
       "   -0.029963253065943718,\n",
       "   -0.052572887390851974,\n",
       "   0.004225587006658316,\n",
       "   0.02142976224422455,\n",
       "   -0.015969425439834595,\n",
       "   0.008862757124006748,\n",
       "   0.023007497191429138,\n",
       "   -0.027754424139857292,\n",
       "   -0.0016772013623267412,\n",
       "   -0.00732618011534214,\n",
       "   0.015900829806923866,\n",
       "   -0.0008313122089020908,\n",
       "   0.012169142253696918,\n",
       "   0.008965653367340565,\n",
       "   -0.012416091747581959,\n",
       "   0.005573521833866835,\n",
       "   0.04099368304014206,\n",
       "   -0.022307807579636574,\n",
       "   -0.006132588721811771,\n",
       "   -0.0030405705329030752,\n",
       "   -0.011538047343492508,\n",
       "   0.0007678598049096763,\n",
       "   -0.009356657043099403,\n",
       "   -0.02797393500804901,\n",
       "   0.006084571126848459,\n",
       "   -0.002737028058618307,\n",
       "   -0.0009843696607276797,\n",
       "   -0.030127886682748795,\n",
       "   0.018164535984396935,\n",
       "   -0.005727865733206272,\n",
       "   -0.0024094763211905956,\n",
       "   -0.012807095423340797,\n",
       "   -0.004297614097595215,\n",
       "   -0.021470921114087105,\n",
       "   0.006149738095700741,\n",
       "   0.002507227472960949,\n",
       "   -0.005535793490707874,\n",
       "   0.007346759084612131,\n",
       "   0.0058273314498364925,\n",
       "   -0.009089128114283085,\n",
       "   0.01610662043094635,\n",
       "   -0.01712185889482498,\n",
       "   0.0168200321495533,\n",
       "   0.011599784716963768,\n",
       "   0.0065956199541687965,\n",
       "   -0.006328091025352478,\n",
       "   -0.004866970703005791,\n",
       "   -0.04387476295232773,\n",
       "   -0.02259591408073902,\n",
       "   -0.0006581043126061559,\n",
       "   0.019097458571195602,\n",
       "   0.006211475934833288,\n",
       "   -0.014213338494300842,\n",
       "   -0.0069557554088532925,\n",
       "   -0.024708708748221397,\n",
       "   0.0006478147115558386,\n",
       "   -0.0024986527860164642,\n",
       "   0.011380273848772049,\n",
       "   0.02535352297127247,\n",
       "   -0.0067533934488892555,\n",
       "   -0.015379490330815315,\n",
       "   0.017108140513300896,\n",
       "   -0.005676417611539364,\n",
       "   5.33771890331991e-05,\n",
       "   -0.008622666820883751,\n",
       "   -0.024214807897806168,\n",
       "   -0.010118085891008377,\n",
       "   -0.026890099048614502,\n",
       "   -0.010783478617668152,\n",
       "   0.02553187496960163,\n",
       "   -0.019248371943831444,\n",
       "   0.019701113924384117,\n",
       "   -0.016984663903713226,\n",
       "   -0.015516684390604496,\n",
       "   -0.01007692702114582,\n",
       "   0.012374933809041977,\n",
       "   -0.02071635238826275,\n",
       "   -0.016422167420387268,\n",
       "   -0.017725514248013496,\n",
       "   -0.03188397362828255,\n",
       "   -0.002388897119089961,\n",
       "   -0.012772796675562859,\n",
       "   -0.008217943832278252,\n",
       "   0.022774267941713333,\n",
       "   -0.00975452084094286,\n",
       "   0.008382576517760754,\n",
       "   -0.008801019750535488,\n",
       "   -0.0147758349776268,\n",
       "   -0.0029822629876434803,\n",
       "   -0.02101817913353443,\n",
       "   -0.0371522381901741,\n",
       "   0.001034960150718689,\n",
       "   0.024653831496834755,\n",
       "   0.017547162249684334,\n",
       "   0.054767996072769165,\n",
       "   0.017698075622320175,\n",
       "   -0.010392474941909313,\n",
       "   0.02018129453063011,\n",
       "   0.008746141567826271,\n",
       "   0.03396932780742645,\n",
       "   0.008217943832278252,\n",
       "   0.00476750498637557,\n",
       "   0.008272821083664894,\n",
       "   -0.004119261167943478,\n",
       "   0.020935863256454468,\n",
       "   -0.013026606291532516,\n",
       "   -0.006585330236703157,\n",
       "   0.04947229474782944,\n",
       "   0.02160811610519886,\n",
       "   -0.00481552304700017,\n",
       "   0.007772061973810196,\n",
       "   -0.002481503412127495,\n",
       "   0.01620265655219555,\n",
       "   -0.01511882059276104,\n",
       "   0.014240777119994164,\n",
       "   -0.019769711419939995,\n",
       "   0.0077240439131855965,\n",
       "   -0.003717967774719,\n",
       "   0.011791856959462166,\n",
       "   -0.01697094552218914,\n",
       "   0.007470234297215939,\n",
       "   -0.03196629136800766,\n",
       "   0.0014731247210875154,\n",
       "   0.047496695071458817,\n",
       "   -0.010467931628227234,\n",
       "   0.024475477635860443,\n",
       "   -0.021813906729221344,\n",
       "   -0.0014045274583622813,\n",
       "   0.011435152031481266,\n",
       "   0.00015144974167924374,\n",
       "   0.04071929305791855,\n",
       "   0.012313195504248142,\n",
       "   -0.0025603901594877243,\n",
       "   0.044286347925662994,\n",
       "   0.0010744035243988037,\n",
       "   0.022897742688655853,\n",
       "   0.010028909891843796,\n",
       "   -0.026972414925694466,\n",
       "   0.008416875265538692,\n",
       "   0.009109706617891788,\n",
       "   0.021621834486722946,\n",
       "   0.014967907220125198,\n",
       "   -0.00987799558788538,\n",
       "   0.011572346091270447,\n",
       "   0.0020030380692332983,\n",
       "   -0.011819296516478062,\n",
       "   0.060639917850494385,\n",
       "   0.006557891611009836,\n",
       "   0.004781224299222231,\n",
       "   0.007820080034434795,\n",
       "   0.0031246021389961243,\n",
       "   -0.017259053885936737,\n",
       "   0.0026512814220041037,\n",
       "   -0.012779656797647476,\n",
       "   0.027260523289442062,\n",
       "   -0.026231566444039345,\n",
       "   -0.016463326290249825,\n",
       "   0.015091381967067719,\n",
       "   0.0064275567419826984,\n",
       "   0.006701945792883635,\n",
       "   -0.013609683141112328,\n",
       "   -0.00482924235984683,\n",
       "   0.0040266551077365875,\n",
       "   -0.010042629204690456,\n",
       "   0.010461071506142616,\n",
       "   0.019344408065080643,\n",
       "   0.029826058074831963,\n",
       "   0.03926503285765648,\n",
       "   -0.00476750498637557,\n",
       "   -0.002896516351029277,\n",
       "   0.0026358470786362886,\n",
       "   -0.014240777119994164,\n",
       "   0.0003241217345930636,\n",
       "   0.006475574802607298,\n",
       "   0.02498309686779976,\n",
       "   -0.013033466413617134,\n",
       "   -0.01891910471022129,\n",
       "   0.022156892344355583,\n",
       "   -0.01011122576892376,\n",
       "   -0.005103630945086479,\n",
       "   0.002841638633981347,\n",
       "   -0.03073154203593731,\n",
       "   0.011092165485024452,\n",
       "   -0.003268656088039279,\n",
       "   0.029579108580946922,\n",
       "   0.021251410245895386,\n",
       "   0.0009500710875727236,\n",
       "   0.02927728183567524,\n",
       "   -0.02772698551416397,\n",
       "   -0.027219366282224655,\n",
       "   0.00607428140938282,\n",
       "   0.007751482538878918,\n",
       "   0.011983929201960564,\n",
       "   -0.02525748685002327,\n",
       "   -0.023912981152534485,\n",
       "   -0.005014454945921898,\n",
       "   0.000310616655042395,\n",
       "   -0.01607918180525303,\n",
       "   -0.004438238218426704,\n",
       "   0.005522073712199926,\n",
       "   -0.02142976224422455,\n",
       "   -0.02229408733546734,\n",
       "   -0.03701504319906235,\n",
       "   0.00014169607311487198,\n",
       "   0.015269734896719456,\n",
       "   -0.019330687820911407,\n",
       "   -0.022252928465604782,\n",
       "   0.004678328521549702,\n",
       "   -0.022390123456716537,\n",
       "   0.02155323699116707,\n",
       "   -0.028893137350678444,\n",
       "   -0.014281935058534145,\n",
       "   0.017643198370933533,\n",
       "   0.00972022209316492,\n",
       "   -0.014789554290473461,\n",
       "   -0.009898575022816658,\n",
       "   -0.011805576272308826,\n",
       "   -0.019495321437716484,\n",
       "   0.03229555860161781,\n",
       "   0.007168406620621681,\n",
       "   0.018713314086198807,\n",
       "   -0.01017296314239502,\n",
       "   -0.0007442795322276652,\n",
       "   0.013300995342433453,\n",
       "   0.0007755769765935838,\n",
       "   0.016284972429275513,\n",
       "   0.028920575976371765,\n",
       "   0.017986183986067772,\n",
       "   -0.004146700259298086,\n",
       "   -0.014734677039086819,\n",
       "   -0.00013773029786534607,\n",
       "   0.009699642658233643,\n",
       "   0.057237498462200165,\n",
       "   0.02510657161474228,\n",
       "   -0.02121025137603283,\n",
       "   0.007765202317386866,\n",
       "   -0.002695869654417038,\n",
       "   -0.01820569485425949,\n",
       "   -0.019563918933272362,\n",
       "   -0.017355090007185936,\n",
       "   0.018562400713562965,\n",
       "   0.009871135465800762,\n",
       "   0.002553530503064394,\n",
       "   0.0038928906433284283,\n",
       "   0.011092165485024452,\n",
       "   -0.009013671427965164,\n",
       "   0.011153903789818287,\n",
       "   0.009761380031704903,\n",
       "   0.002980547957122326,\n",
       "   -0.01465236023068428,\n",
       "   0.005206526722759008,\n",
       "   -0.013300995342433453,\n",
       "   -0.0005136214895173907,\n",
       "   -0.021223971620202065,\n",
       "   0.0001295843831030652,\n",
       "   0.03457298502326012,\n",
       "   -0.0002928242611233145,\n",
       "   0.01069430261850357,\n",
       "   0.01830173097550869,\n",
       "   0.04140526428818703,\n",
       "   -0.013287276029586792,\n",
       "   -0.03122544102370739,\n",
       "   -0.016490764915943146,\n",
       "   0.0069111669436097145,\n",
       "   0.01197707001119852,\n",
       "   -0.0068802982568740845,\n",
       "   -0.009390955790877342,\n",
       "   0.03125287964940071,\n",
       "   0.02979861944913864,\n",
       "   0.015022785402834415,\n",
       "   -0.008931354619562626,\n",
       "   -0.006163457874208689,\n",
       "   0.009068548679351807,\n",
       "   0.015557843260467052,\n",
       "   0.0016308982158079743,\n",
       "   -0.009438973851501942,\n",
       "   0.006499583832919598,\n",
       "   -0.023227009922266006,\n",
       "   0.025435838848352432,\n",
       "   0.029880937188863754,\n",
       "   -0.012855113483965397,\n",
       "   -0.020016660913825035,\n",
       "   -0.001059826579876244,\n",
       "   0.0029702584724873304,\n",
       "   -0.028975453227758408,\n",
       "   -0.005137929692864418,\n",
       "   0.00738105783239007,\n",
       "   -0.016765153035521507,\n",
       "   0.015722475945949554,\n",
       "   0.0008501764386892319,\n",
       "   0.010632564313709736,\n",
       "   -0.006434416398406029,\n",
       "   -0.02241756208240986,\n",
       "   -0.007689745165407658,\n",
       "   0.009246901609003544,\n",
       "   -0.022239210084080696,\n",
       "   -0.004095252137631178,\n",
       "   -0.021992260590195656,\n",
       "   -0.004043804481625557,\n",
       "   -0.006077711004763842,\n",
       "   -0.034490667283535004,\n",
       "   0.0077857812866568565,\n",
       "   -0.0051550790667533875,\n",
       "   -0.027246804907917976,\n",
       "   -0.04439610242843628,\n",
       "   -0.020304769277572632,\n",
       "   0.021045617759227753,\n",
       "   0.015283454209566116,\n",
       "   0.018589839339256287,\n",
       "   -0.01820569485425949,\n",
       "   0.0035156060475856066,\n",
       "   0.014460287988185883,\n",
       "   -0.005011024884879589,\n",
       "   -0.014144740998744965,\n",
       "   -0.005456906743347645,\n",
       "   -0.02723308466374874,\n",
       "   0.013952668756246567,\n",
       "   -0.020071538165211678,\n",
       "   -0.02022245153784752,\n",
       "   -0.012512127868831158,\n",
       "   -0.011071586981415749,\n",
       "   0.011970209889113903,\n",
       "   0.014734677039086819,\n",
       "   -0.0033201039768755436,\n",
       "   -0.01916605606675148,\n",
       "   0.012992308475077152,\n",
       "   0.002097359159961343,\n",
       "   0.013932089321315289,\n",
       "   0.01987946592271328,\n",
       "   0.0006400975398719311,\n",
       "   0.019961783662438393,\n",
       "   -0.031609587371349335,\n",
       "   0.007360478863120079,\n",
       "   -0.010166103951632977,\n",
       "   -0.012861973606050014,\n",
       "   -0.025819983333349228,\n",
       "   -0.00618403684347868,\n",
       "   0.005676417611539364,\n",
       "   0.005432897713035345,\n",
       "   0.005964525975286961,\n",
       "   -0.011359695345163345,\n",
       "   -0.0010452497517690063,\n",
       "   0.03416140004992485,\n",
       "   -0.019097458571195602,\n",
       "   -0.0013470773119479418,\n",
       "   0.017492283135652542,\n",
       "   0.014089862816035748,\n",
       "   0.008869617246091366,\n",
       "   0.03149982914328575,\n",
       "   0.006767112761735916,\n",
       "   0.008938213810324669,\n",
       "   -0.011476309970021248,\n",
       "   -0.0051104906015098095,\n",
       "   -0.019467882812023163,\n",
       "   0.03226811811327934,\n",
       "   0.007840659469366074,\n",
       "   -0.013499927707016468,\n",
       "   0.031554706394672394,\n",
       "   0.014103583060204983,\n",
       "   -0.008684404194355011,\n",
       "   -0.04192660376429558,\n",
       "   -0.008444313891232014,\n",
       "   -0.006286932621151209,\n",
       "   0.030594347044825554,\n",
       "   -0.004359351471066475,\n",
       "   -0.029963253065943718,\n",
       "   -0.02321328967809677,\n",
       "   0.004438238218426704,\n",
       "   -0.012347494252026081,\n",
       "   -0.012121124193072319,\n",
       "   -0.017643198370933533,\n",
       "   -0.009109706617891788,\n",
       "   0.006115439813584089,\n",
       "   -0.009644765406847,\n",
       "   0.015077662654221058,\n",
       "   -0.018219413235783577,\n",
       "   0.008211083710193634,\n",
       "   -0.034326035529375076,\n",
       "   -0.018219413235783577,\n",
       "   -0.006883728317916393,\n",
       "   -0.006801411509513855,\n",
       "   0.012011367827653885,\n",
       "   -0.0002883225679397583,\n",
       "   0.006304081995040178,\n",
       "   -0.009727082215249538,\n",
       "   -0.020977022126317024,\n",
       "   -0.012724778614938259,\n",
       "   -0.029880937188863754,\n",
       "   -0.00044288061326369643,\n",
       "   -0.007182125933468342,\n",
       "   0.03583517298102379,\n",
       "   0.005580381490290165,\n",
       "   0.05303934961557388,\n",
       "   -0.0003391273785382509,\n",
       "   0.029441915452480316,\n",
       "   0.016216376796364784,\n",
       "   -0.013054045848548412,\n",
       "   -0.006701945792883635,\n",
       "   0.001407099887728691,\n",
       "   0.002776471432298422,\n",
       "   -0.04777108505368233,\n",
       "   -0.015928268432617188,\n",
       "   -0.004129550885409117,\n",
       "   0.005813612136989832,\n",
       "   0.002097359159961343,\n",
       "   0.010344456881284714,\n",
       "   0.0371522381901741,\n",
       "   0.023240728303790092,\n",
       "   0.012224019505083561,\n",
       "   -0.02012641541659832,\n",
       "   -0.015310892835259438,\n",
       "   -0.03824979439377785,\n",
       "   -0.021566957235336304,\n",
       "   0.03317360207438469,\n",
       "   0.0029325298964977264,\n",
       "   0.00885589700192213,\n",
       "   -0.023199569433927536,\n",
       "   -0.002277426654472947,\n",
       "   0.022431282326579094,\n",
       "   -0.0028742223512381315,\n",
       "   0.004246165975928307,\n",
       "   -0.006520163267850876,\n",
       "   0.035725418478250504,\n",
       "   -0.014185898937284946,\n",
       "   0.004537704400718212,\n",
       "   -0.018877947703003883,\n",
       "   0.01827429234981537,\n",
       "   -0.02924984320998192,\n",
       "   0.005950806196779013,\n",
       "   -0.03300897032022476,\n",
       "   -0.005083051975816488,\n",
       "   0.002495222957804799,\n",
       "   -0.023117253556847572,\n",
       "   0.020441962406039238,\n",
       "   -0.006434416398406029,\n",
       "   0.011867314577102661,\n",
       "   -0.000156487338244915,\n",
       "   -0.014076143503189087,\n",
       "   -0.006966044660657644,\n",
       "   -0.005069332662969828,\n",
       "   0.01462492160499096,\n",
       "   -0.04126806929707527,\n",
       "   0.01114704366773367,\n",
       "   -0.029908375814557076,\n",
       "   0.004705767147243023,\n",
       "   0.006708805449306965,\n",
       "   -0.0023631732910871506,\n",
       "   0.014954187907278538,\n",
       "   -0.004613161087036133,\n",
       "   0.013623402453958988,\n",
       "   -0.03885344788432121,\n",
       "   -0.005926797166466713,\n",
       "   0.021169092506170273,\n",
       "   0.013794895261526108,\n",
       "   0.047935716807842255,\n",
       "   -0.0021076486445963383,\n",
       "   0.025929737836122513,\n",
       "   0.0063006519339978695,\n",
       "   0.02651967480778694,\n",
       "   -0.0037008184008300304,\n",
       "   -0.0031314617954194546,\n",
       "   0.0020527709275484085,\n",
       "   -0.005703856702893972,\n",
       "   0.015571562573313713,\n",
       "   0.006804841570556164,\n",
       "   -0.028646187856793404,\n",
       "   -0.018658436834812164,\n",
       "   -0.00014276790898293257,\n",
       "   -0.009013671427965164,\n",
       "   -0.0262727253139019,\n",
       "   -0.04272232949733734,\n",
       "   0.023350484669208527,\n",
       "   -0.0072713023982942104,\n",
       "   0.009384095668792725,\n",
       "   -0.01607918180525303,\n",
       "   0.0011087021557614207,\n",
       "   -0.0173413697630167,\n",
       "   0.001447400776669383,\n",
       "   0.0037762753199785948,\n",
       "   0.003690528916195035,\n",
       "   -0.017368808388710022,\n",
       "   -0.016682837158441544,\n",
       "   -0.02651967480778694,\n",
       "   0.03978637233376503,\n",
       "   0.0003116884909104556,\n",
       "   0.010577687062323093,\n",
       "   -0.004620020743459463,\n",
       "   -0.002116223331540823,\n",
       "   0.02587486058473587,\n",
       "   -0.0038654517848044634,\n",
       "   -0.005930227227509022,\n",
       "   0.025175169110298157,\n",
       "   -0.009123426862061024,\n",
       "   0.03973149508237839,\n",
       "   -0.009740801528096199,\n",
       "   0.008972512558102608,\n",
       "   0.0067979819141328335,\n",
       "   -0.015749914571642876,\n",
       "   0.0017337939934805036,\n",
       "   0.00020268326625227928,\n",
       "   -0.019701113924384117,\n",
       "   0.030374836176633835,\n",
       "   -0.0016420453321188688,\n",
       "   -0.008958793245255947,\n",
       "   -0.008883336558938026,\n",
       "   -0.002330589573830366,\n",
       "   0.01074232067912817,\n",
       "   -0.02634132094681263,\n",
       "   -0.00476750498637557,\n",
       "   -0.026999855414032936,\n",
       "   -0.008650105446577072,\n",
       "   -0.0023974718060344458,\n",
       "   0.010042629204690456,\n",
       "   -0.006389828398823738,\n",
       "   -0.005021314602345228,\n",
       "   0.017012104392051697,\n",
       "   0.004650889430195093,\n",
       "   -0.0006040839944034815,\n",
       "   -0.00876672100275755,\n",
       "   -0.010714881122112274,\n",
       "   0.022705670446157455,\n",
       "   -0.03333823382854462,\n",
       "   -0.012539566494524479,\n",
       "   -0.009857416152954102,\n",
       "   -0.004722916521131992,\n",
       "   0.008128766901791096,\n",
       "   -0.014062424190342426,\n",
       "   -0.0334479920566082,\n",
       "   -0.002273996826261282,\n",
       "   0.025312364101409912,\n",
       "   -0.015050224028527737,\n",
       "   0.0019378706347197294,\n",
       "   0.0064069777727127075,\n",
       "   -0.021402323618531227,\n",
       "   -0.016422167420387268,\n",
       "   -0.000596366822719574,\n",
       "   0.0017097849631682038,\n",
       "   -0.005504924803972244,\n",
       "   0.044752806425094604,\n",
       "   -0.026039494201540947,\n",
       "   -0.01950904168188572,\n",
       "   -0.018507521599531174,\n",
       "   -0.007696604821830988,\n",
       "   0.026780344545841217,\n",
       "   -0.009603606536984444,\n",
       "   0.012820814736187458,\n",
       "   -0.0015768778976053,\n",
       "   0.004842961672693491,\n",
       "   -0.0007815792341716588,\n",
       "   0.00836199801415205,\n",
       "   -0.00847175344824791,\n",
       "   0.001673771534115076,\n",
       "   -0.0002233695995528251,\n",
       "   -0.0002250845282105729,\n",
       "   0.01214856281876564,\n",
       "   -0.03180165961384773,\n",
       "   0.007202704902738333,\n",
       "   0.000787152792327106,\n",
       "   0.0069866240955889225,\n",
       "   -0.02560047246515751,\n",
       "   -0.045960117131471634,\n",
       "   0.0026941546238958836,\n",
       "   0.0010375325800850987,\n",
       "   0.007909256033599377,\n",
       "   -0.019646234810352325,\n",
       "   0.02074379101395607,\n",
       "   -0.0017492283368483186,\n",
       "   -0.019028861075639725,\n",
       "   -0.014830713160336018,\n",
       "   0.015969425439834595,\n",
       "   -0.0050590429455041885,\n",
       "   -0.0004105113330297172,\n",
       "   -0.0007652874337509274,\n",
       "   0.005518644116818905,\n",
       "   0.012772796675562859,\n",
       "   0.02158067561686039,\n",
       "   -0.019769711419939995,\n",
       "   -0.026149248704314232,\n",
       "   0.003364692209288478,\n",
       "   -0.009781959466636181,\n",
       "   -0.03613699972629547,\n",
       "   -0.005916507914662361,\n",
       "   -0.01669655553996563,\n",
       "   0.03122544102370739,\n",
       "   0.003934049047529697,\n",
       "   -0.00867068488150835,\n",
       "   -0.0014525455189868808,\n",
       "   0.00155544129665941,\n",
       "   -0.034271158277988434,\n",
       "   -0.004530844744294882,\n",
       "   -0.0015597286401316524,\n",
       "   0.014954187907278538,\n",
       "   0.005546082742512226,\n",
       "   -0.004081532824784517,\n",
       "   -0.026286443695425987,\n",
       "   0.033777255564928055,\n",
       "   -0.015036504715681076,\n",
       "   0.008444313891232014,\n",
       "   -0.004911558702588081,\n",
       "   0.02318585105240345,\n",
       "   -0.006818560883402824,\n",
       "   0.00020375510212033987,\n",
       "   0.008101328276097775,\n",
       "   -0.015996865928173065,\n",
       "   -0.021745309233665466,\n",
       "   0.005436327308416367,\n",
       "   0.03026508167386055,\n",
       "   -0.012642462737858295,\n",
       "   -0.008540350012481213,\n",
       "   0.01765691675245762,\n",
       "   0.00012443959712982178,\n",
       "   0.01916605606675148,\n",
       "   -0.015214856714010239,\n",
       "   -0.0034401491284370422,\n",
       "   0.021182812750339508,\n",
       "   0.01102356892079115,\n",
       "   0.041844286024570465,\n",
       "   -0.038935765624046326,\n",
       "   -0.00622176518663764,\n",
       "   0.0008167353225871921,\n",
       "   0.0057210056111216545,\n",
       "   -0.03446323052048683,\n",
       "   0.017917586490511894,\n",
       "   0.0038002843502908945,\n",
       "   0.013376452028751373,\n",
       "   0.030649226158857346,\n",
       "   -0.02878338098526001,\n",
       "   -0.005419177934527397,\n",
       "   0.03369494155049324,\n",
       "   -0.0022945760283619165,\n",
       "   0.06173747405409813,\n",
       "   0.020702632144093513,\n",
       "   0.0055769518949091434,\n",
       "   0.0013556519988924265,\n",
       "   0.03413396328687668,\n",
       "   0.011846735142171383,\n",
       "   -0.013733157888054848,\n",
       "   -0.013397031463682652,\n",
       "   -0.026080653071403503,\n",
       "   0.01731393113732338,\n",
       "   0.014213338494300842,\n",
       "   -0.02550443634390831,\n",
       "   0.0007880102493800223,\n",
       "   -0.010317017324268818,\n",
       "   0.021896224468946457,\n",
       "   -0.010090647265315056,\n",
       "   -0.007470234297215939,\n",
       "   0.0019275810336694121,\n",
       "   0.0191934946924448,\n",
       "   -0.011860454455018044,\n",
       "   0.028646187856793404,\n",
       "   0.003241217229515314,\n",
       "   -0.022746829316020012,\n",
       "   0.017245333641767502,\n",
       "   -0.0009063403704203665,\n",
       "   0.007772061973810196,\n",
       "   0.00012551141844596714,\n",
       "   -0.012539566494524479,\n",
       "   -0.000797871092800051,\n",
       "   -0.013842913322150707,\n",
       "   0.004668038804084063,\n",
       "   -0.004589152056723833,\n",
       "   -0.00013194240455050021,\n",
       "   -0.006479004863649607,\n",
       "   -0.02284286543726921,\n",
       "   0.015571562573313713,\n",
       "   -0.020291049033403397,\n",
       "   -0.01684747077524662,\n",
       "   0.1812063455581665,\n",
       "   -0.03149982914328575,\n",
       "   -0.005076192319393158,\n",
       "   0.0032549367751926184,\n",
       "   0.020085258409380913,\n",
       "   -0.015626439824700356,\n",
       "   0.032377876341342926,\n",
       "   0.019330687820911407,\n",
       "   -0.008931354619562626,\n",
       "   -0.003296094946563244,\n",
       "   0.026917537674307823,\n",
       "   -0.006108579691499472,\n",
       "   -0.05345093086361885,\n",
       "   -0.0059233675710856915,\n",
       "   0.02636875957250595,\n",
       "   -0.0007502817898057401,\n",
       "   -0.011675242334604263,\n",
       "   -0.030100448057055473,\n",
       "   -0.0032395024318248034,\n",
       "   0.014377971179783344,\n",
       "   0.04036258906126022,\n",
       "   -0.014028125442564487,\n",
       "   0.001106129726395011,\n",
       "   -0.009850556962192059,\n",
       "   0.015571562573313713,\n",
       "   -0.009192023426294327,\n",
       "   0.028508992865681648,\n",
       "   0.01965995505452156,\n",
       "   -0.010008330456912518,\n",
       "   0.012786516919732094,\n",
       "   -0.0246126726269722,\n",
       "   -0.005364300217479467,\n",
       "   0.006996913347393274,\n",
       "   -0.008807879872620106,\n",
       "   -0.022829145193099976,\n",
       "   -0.0002493079227861017,\n",
       "   0.03556078299880028,\n",
       "   -0.026533393189311028,\n",
       "   0.01583223231136799,\n",
       "   -0.005590671207755804,\n",
       "   0.031307756900787354,\n",
       "   -0.03144495189189911,\n",
       "   0.02099074050784111,\n",
       "   -0.030073009431362152,\n",
       "   0.008163065649569035,\n",
       "   0.02735655941069126,\n",
       "   ...],\n",
       "  [-0.023945104330778122,\n",
       "   -0.004191430751234293,\n",
       "   0.01982283964753151,\n",
       "   -0.025535911321640015,\n",
       "   -0.015935735777020454,\n",
       "   0.008590358309447765,\n",
       "   0.018328864127397537,\n",
       "   -0.004672131035476923,\n",
       "   -0.016862554475665092,\n",
       "   -0.046147238463163376,\n",
       "   0.015617575496435165,\n",
       "   0.018162867054343224,\n",
       "   -0.005221996922045946,\n",
       "   -0.005173581186681986,\n",
       "   -0.0064669763669371605,\n",
       "   0.013521859422326088,\n",
       "   0.01400601863861084,\n",
       "   0.02347477898001671,\n",
       "   0.006010483950376511,\n",
       "   -0.009856087155640125,\n",
       "   -0.02860685996711254,\n",
       "   0.030681826174259186,\n",
       "   0.015465411357581615,\n",
       "   -0.018826855346560478,\n",
       "   0.0059862760826945305,\n",
       "   0.004558008164167404,\n",
       "   0.010782904922962189,\n",
       "   -0.025107085704803467,\n",
       "   -0.008472776971757412,\n",
       "   0.00890160258859396,\n",
       "   0.020196333527565002,\n",
       "   -0.0011075129732489586,\n",
       "   -0.018481027334928513,\n",
       "   -0.012844037264585495,\n",
       "   -0.0032404048833996058,\n",
       "   -0.0125327929854393,\n",
       "   0.02679472416639328,\n",
       "   -0.007711955811828375,\n",
       "   0.027251215651631355,\n",
       "   0.009351179003715515,\n",
       "   0.04086299240589142,\n",
       "   0.010935069061815739,\n",
       "   -0.0021475895773619413,\n",
       "   -0.037349384278059006,\n",
       "   0.00046946099610067904,\n",
       "   0.00394589314237237,\n",
       "   0.010782904922962189,\n",
       "   -0.04479159414768219,\n",
       "   0.0119241364300251,\n",
       "   0.003932060208171606,\n",
       "   -0.002355086151510477,\n",
       "   0.035661742091178894,\n",
       "   -0.024332430213689804,\n",
       "   -0.008887769654393196,\n",
       "   -0.01521641481667757,\n",
       "   0.007628957275301218,\n",
       "   0.015742072835564613,\n",
       "   -0.014317262917757034,\n",
       "   0.02690538950264454,\n",
       "   -0.022783122956752777,\n",
       "   -4.397144039103296e-06,\n",
       "   -0.007871036417782307,\n",
       "   -0.008687189780175686,\n",
       "   -0.0009397865505889058,\n",
       "   0.00034842133754864335,\n",
       "   -0.0046064238995313644,\n",
       "   -0.007711955811828375,\n",
       "   0.006754013244062662,\n",
       "   -0.0011213460238650441,\n",
       "   0.0007556333439424634,\n",
       "   0.0164890605956316,\n",
       "   0.008832436986267567,\n",
       "   -0.01586657017469406,\n",
       "   -0.020694324746727943,\n",
       "   0.04006067290902138,\n",
       "   -0.01542391162365675,\n",
       "   -0.00700300931930542,\n",
       "   -0.015825072303414345,\n",
       "   -0.002064590808004141,\n",
       "   0.011322395876049995,\n",
       "   0.005761487875133753,\n",
       "   -0.01506425067782402,\n",
       "   -0.01354952622205019,\n",
       "   0.02762470953166485,\n",
       "   0.014497093856334686,\n",
       "   -0.021883971989154816,\n",
       "   0.0006713378825224936,\n",
       "   0.0017740956973284483,\n",
       "   -0.016807222738862038,\n",
       "   0.024539927020668983,\n",
       "   0.027541711926460266,\n",
       "   -0.0008075074874795973,\n",
       "   0.028164200484752655,\n",
       "   0.001753346063196659,\n",
       "   -0.0032179260160773993,\n",
       "   0.01950467750430107,\n",
       "   -0.02234046347439289,\n",
       "   0.029160184785723686,\n",
       "   -0.018951352685689926,\n",
       "   -0.014082100242376328,\n",
       "   -0.0037522297352552414,\n",
       "   0.007400711067020893,\n",
       "   -0.0186193585395813,\n",
       "   -0.0180107019841671,\n",
       "   -0.03032216615974903,\n",
       "   0.021468978375196457,\n",
       "   0.010948901996016502,\n",
       "   0.0155760757625103,\n",
       "   0.0256742425262928,\n",
       "   0.0029291599057614803,\n",
       "   -0.012463627383112907,\n",
       "   0.029353847727179527,\n",
       "   -0.00304501224309206,\n",
       "   -0.04086299240589142,\n",
       "   -0.007981701754033566,\n",
       "   0.010305662639439106,\n",
       "   0.013639440760016441,\n",
       "   -0.014538592658936977,\n",
       "   -0.014068267308175564,\n",
       "   -0.01900668628513813,\n",
       "   0.0378473736345768,\n",
       "   0.005464076530188322,\n",
       "   0.022686291486024857,\n",
       "   -0.002619644161313772,\n",
       "   0.005000667180866003,\n",
       "   -0.011038817465305328,\n",
       "   0.005419118795543909,\n",
       "   -0.015811238437891006,\n",
       "   -0.008687189780175686,\n",
       "   -0.03350377827882767,\n",
       "   -0.01042324397712946,\n",
       "   0.034140102565288544,\n",
       "   -0.006999550852924585,\n",
       "   0.019283348694443703,\n",
       "   -0.0060174004174768925,\n",
       "   0.027970537543296814,\n",
       "   0.0009596716263331473,\n",
       "   -0.007981701754033566,\n",
       "   -0.016115566715598106,\n",
       "   -0.03073715791106224,\n",
       "   0.017969202250242233,\n",
       "   0.002054216107353568,\n",
       "   -0.012387544848024845,\n",
       "   -0.004340136423707008,\n",
       "   -0.02448459528386593,\n",
       "   0.03729404881596565,\n",
       "   0.003610440297052264,\n",
       "   0.0503801666200161,\n",
       "   -0.006906177382916212,\n",
       "   0.0001939876819960773,\n",
       "   -0.015562242828309536,\n",
       "   0.008687189780175686,\n",
       "   -0.00014178932178765535,\n",
       "   0.0013884978834539652,\n",
       "   0.03162247687578201,\n",
       "   0.014192765578627586,\n",
       "   0.012726455926895142,\n",
       "   0.023931270465254784,\n",
       "   -0.012145466171205044,\n",
       "   0.0005027469014748931,\n",
       "   -0.019297180697321892,\n",
       "   -0.007926369085907936,\n",
       "   -0.033310115337371826,\n",
       "   -0.004893460776656866,\n",
       "   0.007615124341100454,\n",
       "   0.043629612773656845,\n",
       "   -0.005557450000196695,\n",
       "   0.0036450230982154608,\n",
       "   -0.008230697363615036,\n",
       "   -0.017512710765004158,\n",
       "   -3.7824895116500556e-05,\n",
       "   0.031207483261823654,\n",
       "   -0.02185630612075329,\n",
       "   0.02192547172307968,\n",
       "   0.0054606180638074875,\n",
       "   0.015548409894108772,\n",
       "   0.009925252757966518,\n",
       "   -0.009869920089840889,\n",
       "   -0.03530208393931389,\n",
       "   -0.004800087306648493,\n",
       "   0.027569377794861794,\n",
       "   0.017319047823548317,\n",
       "   0.010520076379179955,\n",
       "   0.022617125883698463,\n",
       "   -0.017263714224100113,\n",
       "   -0.020265499129891396,\n",
       "   0.021496644243597984,\n",
       "   0.011453811079263687,\n",
       "   -0.00844511017203331,\n",
       "   0.010706823319196701,\n",
       "   0.008251447230577469,\n",
       "   0.022520294412970543,\n",
       "   -0.017651041969656944,\n",
       "   0.014012934640049934,\n",
       "   -0.6121978759765625,\n",
       "   -0.011156398802995682,\n",
       "   0.012290713377296925,\n",
       "   -0.04470859467983246,\n",
       "   -0.01309303380548954,\n",
       "   -0.009129849262535572,\n",
       "   0.0050352499820292,\n",
       "   -0.004091140814125538,\n",
       "   -0.02254796028137207,\n",
       "   0.03842836618423462,\n",
       "   -0.008265280164778233,\n",
       "   -0.012276880443096161,\n",
       "   0.01502275187522173,\n",
       "   -0.006985717918723822,\n",
       "   -0.003596607130020857,\n",
       "   -0.018176699057221413,\n",
       "   0.011910303495824337,\n",
       "   -0.02732038125395775,\n",
       "   0.008659523911774158,\n",
       "   0.033227115869522095,\n",
       "   -3.344801370985806e-05,\n",
       "   0.026518061757087708,\n",
       "   -0.013418111950159073,\n",
       "   -0.00318507244810462,\n",
       "   0.011640558019280434,\n",
       "   0.018135199323296547,\n",
       "   0.023405613377690315,\n",
       "   -0.005958609748631716,\n",
       "   0.029575178399682045,\n",
       "   -0.008818604052066803,\n",
       "   -0.030183834955096245,\n",
       "   -0.008071616291999817,\n",
       "   0.022119134664535522,\n",
       "   -0.009192097932100296,\n",
       "   0.04955017939209938,\n",
       "   -0.006176481023430824,\n",
       "   -0.02264479175209999,\n",
       "   0.029879506677389145,\n",
       "   0.0010443994542583823,\n",
       "   0.038981687277555466,\n",
       "   -0.030073169618844986,\n",
       "   -0.03184380754828453,\n",
       "   0.019352514296770096,\n",
       "   -0.0069165523163974285,\n",
       "   -0.007490626070648432,\n",
       "   0.03034983202815056,\n",
       "   -0.0018121367320418358,\n",
       "   0.015645241364836693,\n",
       "   -0.0020248207729309797,\n",
       "   -0.013231365010142326,\n",
       "   -0.012117799371480942,\n",
       "   0.005543616600334644,\n",
       "   -0.004392010625451803,\n",
       "   -0.008991518057882786,\n",
       "   0.011716639623045921,\n",
       "   -0.005453701596707106,\n",
       "   0.025231583043932915,\n",
       "   -0.024595260620117188,\n",
       "   0.04440426453948021,\n",
       "   -0.019864337518811226,\n",
       "   0.005412202328443527,\n",
       "   -0.0032576960511505604,\n",
       "   -0.025812573730945587,\n",
       "   0.008555775508284569,\n",
       "   -0.038788024336099625,\n",
       "   -0.010049750097095966,\n",
       "   -0.01304461807012558,\n",
       "   0.013383529148995876,\n",
       "   -0.0021320271771401167,\n",
       "   -0.025120917707681656,\n",
       "   0.028164200484752655,\n",
       "   0.010443993844091892,\n",
       "   0.0013547796988859773,\n",
       "   -0.006933843716979027,\n",
       "   0.0004074281605426222,\n",
       "   0.020874155685305595,\n",
       "   0.023032119497656822,\n",
       "   -0.01536857895553112,\n",
       "   0.015493077225983143,\n",
       "   0.018024535849690437,\n",
       "   0.009904502891004086,\n",
       "   -0.006615682505071163,\n",
       "   -0.035440415143966675,\n",
       "   -0.0002714746806304902,\n",
       "   0.020279331132769585,\n",
       "   0.0008870478486642241,\n",
       "   0.0041222651489079,\n",
       "   0.03198213875293732,\n",
       "   0.002294566249474883,\n",
       "   0.009434177540242672,\n",
       "   0.012539708986878395,\n",
       "   0.032203469425439835,\n",
       "   -0.03773671016097069,\n",
       "   -0.04426593333482742,\n",
       "   0.010478576645255089,\n",
       "   0.013923020102083683,\n",
       "   -0.004672131035476923,\n",
       "   0.006169564556330442,\n",
       "   0.026739390566945076,\n",
       "   -0.022285131737589836,\n",
       "   -0.008666439913213253,\n",
       "   -0.023405613377690315,\n",
       "   0.036159735172986984,\n",
       "   0.0065741827711462975,\n",
       "   0.011391561478376389,\n",
       "   0.01920034922659397,\n",
       "   -0.013265947811305523,\n",
       "   0.010568492114543915,\n",
       "   0.0198366716504097,\n",
       "   -0.018882187083363533,\n",
       "   -0.021067818626761436,\n",
       "   -0.025314580649137497,\n",
       "   -0.004440426826477051,\n",
       "   0.01057540811598301,\n",
       "   0.00037522296770475805,\n",
       "   -0.02852386236190796,\n",
       "   0.018577858805656433,\n",
       "   0.00032507797004655004,\n",
       "   0.009994418360292912,\n",
       "   -0.03640872985124588,\n",
       "   0.013521859422326088,\n",
       "   -0.01065840758383274,\n",
       "   -0.0015372037887573242,\n",
       "   -0.013743189163506031,\n",
       "   0.0007694664527662098,\n",
       "   -0.002462292555719614,\n",
       "   -0.0036484813317656517,\n",
       "   -0.0060174004174768925,\n",
       "   -0.001750752329826355,\n",
       "   0.0238482728600502,\n",
       "   0.0012484376784414053,\n",
       "   0.004340136423707008,\n",
       "   0.004398927092552185,\n",
       "   -0.00040721200639382005,\n",
       "   0.014116683043539524,\n",
       "   -0.006881969515234232,\n",
       "   -0.0004802680923603475,\n",
       "   -0.010360995307564735,\n",
       "   -0.004793170839548111,\n",
       "   -5.184712426853366e-05,\n",
       "   -0.024028101935982704,\n",
       "   0.007974784821271896,\n",
       "   0.006985717918723822,\n",
       "   -0.008638774044811726,\n",
       "   -0.023281116038560867,\n",
       "   -0.0001490300928708166,\n",
       "   -0.02729271538555622,\n",
       "   0.008867019787430763,\n",
       "   0.011066483333706856,\n",
       "   0.007303879130631685,\n",
       "   0.02184247225522995,\n",
       "   0.012850954197347164,\n",
       "   -0.015935735777020454,\n",
       "   0.010741406120359898,\n",
       "   -0.005982817616313696,\n",
       "   -0.005868694745004177,\n",
       "   -0.013335112482309341,\n",
       "   -0.019283348694443703,\n",
       "   -0.0094618434086442,\n",
       "   -0.020376162603497505,\n",
       "   0.007020300719887018,\n",
       "   0.03914768621325493,\n",
       "   -0.015949569642543793,\n",
       "   0.010153498500585556,\n",
       "   -0.013127616606652737,\n",
       "   -0.012090133503079414,\n",
       "   0.0003726292634382844,\n",
       "   0.0131414495408535,\n",
       "   -0.032092802226543427,\n",
       "   -0.004720546770840883,\n",
       "   -0.019062018021941185,\n",
       "   -0.037570711225271225,\n",
       "   -0.004952251445502043,\n",
       "   -0.0017429712461307645,\n",
       "   0.007898703217506409,\n",
       "   0.02488575503230095,\n",
       "   -0.01567290723323822,\n",
       "   0.009323512203991413,\n",
       "   -0.01111490000039339,\n",
       "   -0.021095484495162964,\n",
       "   0.006428935565054417,\n",
       "   -0.011294730007648468,\n",
       "   -0.031124485656619072,\n",
       "   0.006439310032874346,\n",
       "   0.016433728858828545,\n",
       "   0.01760954223573208,\n",
       "   0.03485942259430885,\n",
       "   -0.0026179151609539986,\n",
       "   -0.0068854279816150665,\n",
       "   -0.004312470555305481,\n",
       "   0.017139216884970665,\n",
       "   0.023115117102861404,\n",
       "   -0.010478576645255089,\n",
       "   0.02113698422908783,\n",
       "   -0.001865740050561726,\n",
       "   0.005180497653782368,\n",
       "   0.02285228855907917,\n",
       "   -0.024553760886192322,\n",
       "   0.007137882057577372,\n",
       "   0.04628556966781616,\n",
       "   0.012422127649188042,\n",
       "   -0.00637014489620924,\n",
       "   0.01334202941507101,\n",
       "   -0.008486609905958176,\n",
       "   0.012643457390367985,\n",
       "   -0.02081882208585739,\n",
       "   0.001337488298304379,\n",
       "   -0.03278445824980736,\n",
       "   0.027832206338644028,\n",
       "   0.0013962789671495557,\n",
       "   0.01460775826126337,\n",
       "   -0.00834136176854372,\n",
       "   -0.010762155055999756,\n",
       "   -0.011709722690284252,\n",
       "   -0.0011109712067991495,\n",
       "   0.048996858298778534,\n",
       "   -0.009199014864861965,\n",
       "   0.03469342738389969,\n",
       "   -0.011668223887681961,\n",
       "   -0.002951638773083687,\n",
       "   0.011094150133430958,\n",
       "   -0.0021787139121443033,\n",
       "   0.03405710309743881,\n",
       "   0.032812125980854034,\n",
       "   0.004879627842456102,\n",
       "   0.02882819063961506,\n",
       "   0.013203698210418224,\n",
       "   0.011135648936033249,\n",
       "   0.0024277097545564175,\n",
       "   -0.04213563725352287,\n",
       "   0.020860321819782257,\n",
       "   0.018038367852568626,\n",
       "   0.020887987688183784,\n",
       "   0.01820436492562294,\n",
       "   0.0063597699627280235,\n",
       "   0.00707563292235136,\n",
       "   -0.0011420956579968333,\n",
       "   -0.002847890369594097,\n",
       "   0.04360194504261017,\n",
       "   0.012145466171205044,\n",
       "   -0.0016677536768838763,\n",
       "   0.018425695598125458,\n",
       "   5.82503380428534e-05,\n",
       "   -0.012899369932711124,\n",
       "   0.01228379737585783,\n",
       "   -0.005519408732652664,\n",
       "   0.01395760290324688,\n",
       "   0.00606581661850214,\n",
       "   -0.022603293880820274,\n",
       "   0.019380180165171623,\n",
       "   -0.0036519395653158426,\n",
       "   0.020251665264368057,\n",
       "   -0.019297180697321892,\n",
       "   -0.008375944569706917,\n",
       "   0.0039597260765731335,\n",
       "   -0.00043487822404131293,\n",
       "   0.006307895760983229,\n",
       "   0.010596157982945442,\n",
       "   0.032618459314107895,\n",
       "   0.0287175253033638,\n",
       "   0.002045570407062769,\n",
       "   0.0035793157294392586,\n",
       "   0.0024899588897824287,\n",
       "   -0.001309822080656886,\n",
       "   0.009517176076769829,\n",
       "   -0.009724672883749008,\n",
       "   -0.0054606180638074875,\n",
       "   -0.03126281499862671,\n",
       "   -0.022810788825154305,\n",
       "   0.00018134336278308183,\n",
       "   -0.015105750411748886,\n",
       "   -0.012678040191531181,\n",
       "   -0.014497093856334686,\n",
       "   -0.018840689212083817,\n",
       "   -0.002446730388328433,\n",
       "   0.00788486935198307,\n",
       "   -0.0029775758739560843,\n",
       "   0.02880052477121353,\n",
       "   0.01258812565356493,\n",
       "   0.02387593872845173,\n",
       "   -0.010485493578016758,\n",
       "   -0.029077185317873955,\n",
       "   0.029492178931832314,\n",
       "   0.006128065288066864,\n",
       "   0.004519966896623373,\n",
       "   -0.020666658878326416,\n",
       "   -0.03618740290403366,\n",
       "   -0.007822620682418346,\n",
       "   0.00818919762969017,\n",
       "   -0.004869252908974886,\n",
       "   -0.01511958334594965,\n",
       "   0.01274028979241848,\n",
       "   -0.0016910970443859696,\n",
       "   -0.0014689028030261397,\n",
       "   -0.017415879294276237,\n",
       "   0.011107983067631721,\n",
       "   0.015811238437891006,\n",
       "   0.008784021250903606,\n",
       "   -0.017623376101255417,\n",
       "   0.005695780739188194,\n",
       "   -0.004647923167794943,\n",
       "   0.013023868203163147,\n",
       "   -0.031788475811481476,\n",
       "   -0.004834670107811689,\n",
       "   0.04133331775665283,\n",
       "   0.006055441685020924,\n",
       "   -0.01552074309438467,\n",
       "   -0.013860770501196384,\n",
       "   -0.0183150302618742,\n",
       "   -0.016502894461154938,\n",
       "   0.009724672883749008,\n",
       "   0.004132640082389116,\n",
       "   -0.016198566183447838,\n",
       "   0.00737996120005846,\n",
       "   0.01103190053254366,\n",
       "   0.02052832767367363,\n",
       "   -0.014732256531715393,\n",
       "   0.005017958581447601,\n",
       "   0.009807671420276165,\n",
       "   0.026531895622611046,\n",
       "   0.002282462315633893,\n",
       "   -0.016613557934761047,\n",
       "   0.005553991533815861,\n",
       "   0.025411413982510567,\n",
       "   0.029962504282593727,\n",
       "   0.0489138588309288,\n",
       "   -0.035136085003614426,\n",
       "   0.012691873125731945,\n",
       "   -0.04196963831782341,\n",
       "   0.029049519449472427,\n",
       "   -0.026158401742577553,\n",
       "   -0.014649257995188236,\n",
       "   0.010817487724125385,\n",
       "   0.01445559412240982,\n",
       "   0.0005598084535449743,\n",
       "   -0.01921418309211731,\n",
       "   0.004430051892995834,\n",
       "   0.0029326181393116713,\n",
       "   0.01718071661889553,\n",
       "   -0.00289284810423851,\n",
       "   0.003219655016437173,\n",
       "   -0.005930943414568901,\n",
       "   0.012090133503079414,\n",
       "   -0.007110215723514557,\n",
       "   -0.0033839233219623566,\n",
       "   -0.030598826706409454,\n",
       "   -0.0028012036345899105,\n",
       "   0.014234264381229877,\n",
       "   -0.0006929520750418305,\n",
       "   -0.00637014489620924,\n",
       "   0.007082549389451742,\n",
       "   0.035523414611816406,\n",
       "   0.002668060129508376,\n",
       "   -0.03104148618876934,\n",
       "   -0.019241848960518837,\n",
       "   0.01639222912490368,\n",
       "   -0.003638106631115079,\n",
       "   0.0076704565435647964,\n",
       "   -0.009344262070953846,\n",
       "   0.04365727677941322,\n",
       "   0.035025421530008316,\n",
       "   0.008825520984828472,\n",
       "   -0.0082445302978158,\n",
       "   0.004416218493133783,\n",
       "   0.017125383019447327,\n",
       "   0.03961801156401634,\n",
       "   -0.010990401729941368,\n",
       "   0.005135539919137955,\n",
       "   -0.0004949657595716417,\n",
       "   -0.006857761647552252,\n",
       "   -0.002963742706924677,\n",
       "   0.014165098778903484,\n",
       "   -0.004264054354280233,\n",
       "   -0.005975901149213314,\n",
       "   -0.0026732473634183407,\n",
       "   0.003520525060594082,\n",
       "   -0.022312797605991364,\n",
       "   -0.013971435837447643,\n",
       "   0.001948738587088883,\n",
       "   -0.02145514450967312,\n",
       "   0.020666658878326416,\n",
       "   -0.009759255684912205,\n",
       "   -0.0029931380413472652,\n",
       "   -0.005367244593799114,\n",
       "   -0.0031954471487551928,\n",
       "   -0.02538374625146389,\n",
       "   0.0198366716504097,\n",
       "   -0.024512261152267456,\n",
       "   -0.008562691509723663,\n",
       "   -0.025812573730945587,\n",
       "   -0.015838904306292534,\n",
       "   -0.02296295389533043,\n",
       "   -0.01902051828801632,\n",
       "   0.0078018708154559135,\n",
       "   -0.016917886212468147,\n",
       "   -0.030073169618844986,\n",
       "   -0.04371261224150658,\n",
       "   -0.02496875450015068,\n",
       "   0.019366346299648285,\n",
       "   0.009288929402828217,\n",
       "   0.029768841341137886,\n",
       "   -0.009690090082585812,\n",
       "   -0.017332879826426506,\n",
       "   0.00379372900351882,\n",
       "   -0.0018899479182437062,\n",
       "   -0.009579424746334553,\n",
       "   -0.009641674347221851,\n",
       "   -0.013113783672451973,\n",
       "   -0.000932870025280863,\n",
       "   0.012857871130108833,\n",
       "   -0.010762155055999756,\n",
       "   -0.0026697891298681498,\n",
       "   -0.038760360330343246,\n",
       "   0.004547633230686188,\n",
       "   0.004111890215426683,\n",
       "   0.0016495977761223912,\n",
       "   -0.02618606761097908,\n",
       "   -0.007580541539937258,\n",
       "   -0.0011619807919487357,\n",
       "   0.002896306337788701,\n",
       "   0.01576973870396614,\n",
       "   0.0235439445823431,\n",
       "   0.0229352880269289,\n",
       "   -0.01980900578200817,\n",
       "   -0.0009510259260423481,\n",
       "   -0.01111490000039339,\n",
       "   -0.00783645361661911,\n",
       "   -0.024954920634627342,\n",
       "   0.0055367001332342625,\n",
       "   -0.004634090233594179,\n",
       "   0.0008126948960125446,\n",
       "   0.019283348694443703,\n",
       "   -0.009821504354476929,\n",
       "   -0.003428880823776126,\n",
       "   -0.0009717756183817983,\n",
       "   -0.008521192707121372,\n",
       "   -0.010015168227255344,\n",
       "   0.029796507209539413,\n",
       "   0.010582325048744678,\n",
       "   0.000647562206722796,\n",
       "   0.018162867054343224,\n",
       "   0.023377947509288788,\n",
       "   -0.01042324397712946,\n",
       "   -0.009856087155640125,\n",
       "   0.00025764157180674374,\n",
       "   -0.006117690354585648,\n",
       "   0.01921418309211731,\n",
       "   0.0012302817776799202,\n",
       "   -0.015908069908618927,\n",
       "   0.031788475811481476,\n",
       "   -0.009489510208368301,\n",
       "   -0.03472109138965607,\n",
       "   -0.03521908447146416,\n",
       "   0.01689022034406662,\n",
       "   0.005550533067435026,\n",
       "   0.025120917707681656,\n",
       "   -0.0024830421898514032,\n",
       "   0.0012864787131547928,\n",
       "   -0.018245864659547806,\n",
       "   0.005062916316092014,\n",
       "   -0.012041717767715454,\n",
       "   0.005498659331351519,\n",
       "   -0.00940651074051857,\n",
       "   -0.012595041655004025,\n",
       "   0.004692880902439356,\n",
       "   0.012117799371480942,\n",
       "   0.007124049123376608,\n",
       "   -0.016143232583999634,\n",
       "   0.008880853652954102,\n",
       "   -0.027846040204167366,\n",
       "   -0.02639356441795826,\n",
       "   0.025812573730945587,\n",
       "   -0.00864569004625082,\n",
       "   0.012117799371480942,\n",
       "   -0.0024674800224602222,\n",
       "   0.008120032027363777,\n",
       "   0.006276771426200867,\n",
       "   -0.03458276018500328,\n",
       "   -0.009323512203991413,\n",
       "   -0.02478892356157303,\n",
       "   -0.00039294661837629974,\n",
       "   0.007407627534121275,\n",
       "   0.0341954343020916,\n",
       "   0.025051752105355263,\n",
       "   0.05336811766028404,\n",
       "   -0.007393794599920511,\n",
       "   0.01911734975874424,\n",
       "   0.01456625945866108,\n",
       "   -0.010181165300309658,\n",
       "   0.0034824840258806944,\n",
       "   0.004568382631987333,\n",
       "   0.0060174004174768925,\n",
       "   -0.027140552178025246,\n",
       "   -0.020472994074225426,\n",
       "   -0.0031781557481735945,\n",
       "   0.009807671420276165,\n",
       "   0.0018916770350188017,\n",
       "   0.01339044515043497,\n",
       "   0.021704141050577164,\n",
       "   0.017568042501807213,\n",
       "   -0.011142565868794918,\n",
       "   -0.008126948960125446,\n",
       "   0.0025556660257279873,\n",
       "   -0.027140552178025246,\n",
       "   -0.018688524141907692,\n",
       "   0.04047566279768944,\n",
       "   -0.01233221311122179,\n",
       "   0.006792054511606693,\n",
       "   -0.028385531157255173,\n",
       "   -0.0021130067761987448,\n",
       "   0.013307446613907814,\n",
       "   0.014704590663313866,\n",
       "   0.024954920634627342,\n",
       "   0.0006198959890753031,\n",
       "   0.0315118134021759,\n",
       "   -0.01647522859275341,\n",
       "   0.0016271189088001847,\n",
       "   -0.009786921553313732,\n",
       "   0.007497542537748814,\n",
       "   -0.00803703349083662,\n",
       "   -0.019255680963397026,\n",
       "   -0.03062649443745613,\n",
       "   0.017471211031079292,\n",
       "   -0.0008628399227745831,\n",
       "   0.013667107559740543,\n",
       "   0.016350729390978813,\n",
       "   0.0003665772674139589,\n",
       "   0.0017637208802625537,\n",
       "   0.01410976704210043,\n",
       "   -0.008991518057882786,\n",
       "   -0.013722440227866173,\n",
       "   -0.011218647472560406,\n",
       "   -0.00401851674541831,\n",
       "   -0.04783487692475319,\n",
       "   -0.0045614661648869514,\n",
       "   -0.042218636721372604,\n",
       "   -0.012463627383112907,\n",
       "   -0.004392010625451803,\n",
       "   0.006352853495627642,\n",
       "   0.011052650399506092,\n",
       "   -0.013867687433958054,\n",
       "   0.012193881906569004,\n",
       "   -0.032203469425439835,\n",
       "   -0.029768841341137886,\n",
       "   0.005678489338606596,\n",
       "   0.007691206410527229,\n",
       "   0.059814345091581345,\n",
       "   0.0061073158867657185,\n",
       "   0.028468528762459755,\n",
       "   0.004634090233594179,\n",
       "   0.011751222424209118,\n",
       "   -6.447118266805774e-06,\n",
       "   0.01345269475132227,\n",
       "   -0.0057891542091965675,\n",
       "   0.004623715300112963,\n",
       "   0.01415126584470272,\n",
       "   0.01578357256948948,\n",
       "   -0.015188748948276043,\n",
       "   -0.02568807452917099,\n",
       "   0.011080317199230194,\n",
       "   -0.011647474020719528,\n",
       "   -0.009952918626368046,\n",
       "   -0.025203917175531387,\n",
       "   0.02426326647400856,\n",
       "   -0.010347162373363972,\n",
       "   -0.006128065288066864,\n",
       "   -0.009531009010970592,\n",
       "   0.006027775350958109,\n",
       "   -0.03671305999159813,\n",
       "   -0.010001334361732006,\n",
       "   -0.011993302032351494,\n",
       "   -0.004509591963142157,\n",
       "   -0.013777771964669228,\n",
       "   -0.032369464635849,\n",
       "   -0.04421060159802437,\n",
       "   0.03427843376994133,\n",
       "   0.016212398186326027,\n",
       "   0.02599240280687809,\n",
       "   -0.007079091388732195,\n",
       "   -0.00753212533891201,\n",
       "   0.007386877667158842,\n",
       "   -0.013238281011581421,\n",
       "   -0.028468528762459755,\n",
       "   0.02860685996711254,\n",
       "   0.016240064054727554,\n",
       "   0.04623023420572281,\n",
       "   -0.013376612216234207,\n",
       "   0.029326181858778,\n",
       "   0.02346094511449337,\n",
       "   -0.018979020416736603,\n",
       "   0.005979359615594149,\n",
       "   0.00036700954660773277,\n",
       "   -0.016157066449522972,\n",
       "   0.0271958839148283,\n",
       "   -0.01921418309211731,\n",
       "   -0.001684180460870266,\n",
       "   -0.0038317700382322073,\n",
       "   -0.0025522077921777964,\n",
       "   0.00859727431088686,\n",
       "   -0.0039009354077279568,\n",
       "   -0.0128371212631464,\n",
       "   -0.02155197784304619,\n",
       "   -0.000813127204310149,\n",
       "   -0.007158631458878517,\n",
       "   0.03870502859354019,\n",
       "   -0.011149482801556587,\n",
       "   -0.004571841098368168,\n",
       "   0.011052650399506092,\n",
       "   -0.0020317372400313616,\n",
       "   -0.00440238555893302,\n",
       "   -0.015244081616401672,\n",
       "   -0.00966242328286171,\n",
       "   0.01738821342587471,\n",
       "   -0.0507398284971714,\n",
       "   -0.00394589314237237,\n",
       "   -0.021413646638393402,\n",
       "   0.00024121475871652365,\n",
       "   0.019573843106627464,\n",
       "   -0.014234264381229877,\n",
       "   -0.018647024407982826,\n",
       "   0.02336411364376545,\n",
       "   0.018564026802778244,\n",
       "   -0.02578490786254406,\n",
       "   -0.0029464513063430786,\n",
       "   0.019573843106627464,\n",
       "   0.0007171600009314716,\n",
       "   -0.01536857895553112,\n",
       "   -0.0003287523868493736,\n",
       "   0.0032628835178911686,\n",
       "   -0.014082100242376328,\n",
       "   0.02974117547273636,\n",
       "   -0.023516278713941574,\n",
       "   -0.020777322351932526,\n",
       "   -0.014497093856334686,\n",
       "   -0.01298928540199995,\n",
       "   0.015838904306292534,\n",
       "   -0.00024748287978582084,\n",
       "   -0.007815703749656677,\n",
       "   -0.002382752252742648,\n",
       "   -0.006812803912907839,\n",
       "   0.006833553779870272,\n",
       "   0.0008619753061793745,\n",
       "   -0.01961534284055233,\n",
       "   -0.0005312776775099337,\n",
       "   0.018052201718091965,\n",
       "   -0.004917668644338846,\n",
       "   0.011350062675774097,\n",
       "   -0.01980900578200817,\n",
       "   0.0033216741867363453,\n",
       "   -0.005190872587263584,\n",
       "   0.005540158599615097,\n",
       "   -0.0158804040402174,\n",
       "   -0.042605962604284286,\n",
       "   -0.0029533677734434605,\n",
       "   0.011246314272284508,\n",
       "   0.01606023497879505,\n",
       "   -0.02102631889283657,\n",
       "   0.021468978375196457,\n",
       "   -0.00834136176854372,\n",
       "   0.0032507795840501785,\n",
       "   -0.012422127649188042,\n",
       "   0.016046401113271713,\n",
       "   0.018149033188819885,\n",
       "   -0.010354079306125641,\n",
       "   0.011882636696100235,\n",
       "   0.0036000655964016914,\n",
       "   0.006684847641736269,\n",
       "   0.017263714224100113,\n",
       "   -0.024512261152267456,\n",
       "   -0.027043718844652176,\n",
       "   0.006567266304045916,\n",
       "   -0.005885986145585775,\n",
       "   -0.018439527601003647,\n",
       "   -0.01859169267117977,\n",
       "   -0.014068267308175564,\n",
       "   0.04196963831782341,\n",
       "   0.0008183146128430963,\n",
       "   -8.888850425137207e-05,\n",
       "   -0.014995085075497627,\n",
       "   -0.006397810764610767,\n",
       "   -0.029962504282593727,\n",
       "   -0.010063583962619305,\n",
       "   -0.023211950436234474,\n",
       "   0.012014050967991352,\n",
       "   0.0315118134021759,\n",
       "   -0.00338565232232213,\n",
       "   -0.012207714840769768,\n",
       "   0.04199730604887009,\n",
       "   -0.01451092679053545,\n",
       "   0.016710391268134117,\n",
       "   -0.016738057136535645,\n",
       "   0.03245246410369873,\n",
       "   -0.012263047508895397,\n",
       "   -0.01798303611576557,\n",
       "   -0.023128950968384743,\n",
       "   -0.00920593086630106,\n",
       "   -0.0268915556371212,\n",
       "   0.004924585111439228,\n",
       "   0.015728240832686424,\n",
       "   -0.025134751573204994,\n",
       "   0.0015493077225983143,\n",
       "   0.033725108951330185,\n",
       "   -0.017720207571983337,\n",
       "   -0.012809454463422298,\n",
       "   -0.01536857895553112,\n",
       "   0.01111490000039339,\n",
       "   0.028883522376418114,\n",
       "   0.010935069061815739,\n",
       "   0.010831320658326149,\n",
       "   -0.04349128156900406,\n",
       "   -0.004551091231405735,\n",
       "   -0.00047853897558525205,\n",
       "   0.004973001312464476,\n",
       "   -0.016004901379346848,\n",
       "   -0.017734039574861526,\n",
       "   0.009351179003715515,\n",
       "   0.004845045041292906,\n",
       "   0.0284131970256567,\n",
       "   -0.014483260922133923,\n",
       "   0.009752338752150536,\n",
       "   0.0029429930727928877,\n",
       "   -0.014344929717481136,\n",
       "   0.04047566279768944,\n",
       "   0.01982283964753151,\n",
       "   -0.0005126894684508443,\n",
       "   0.011820388026535511,\n",
       "   0.027458712458610535,\n",
       "   0.017457379028201103,\n",
       "   -0.0335867777466774,\n",
       "   -0.0032594252843409777,\n",
       "   -0.03328245133161545,\n",
       "   -0.013743189163506031,\n",
       "   0.02326728217303753,\n",
       "   -0.020265499129891396,\n",
       "   0.0004024568770546466,\n",
       "   -0.01982283964753151,\n",
       "   0.014469427056610584,\n",
       "   -0.0024899588897824287,\n",
       "   -0.016931720077991486,\n",
       "   0.005858319811522961,\n",
       "   0.021579643711447716,\n",
       "   -0.015340913087129593,\n",
       "   0.014856754802167416,\n",
       "   -0.01960150897502899,\n",
       "   -0.05096115916967392,\n",
       "   0.0037107302341610193,\n",
       "   0.002835786435753107,\n",
       "   0.013715523295104504,\n",
       "   -0.0003345882287248969,\n",
       "   -0.00890160258859396,\n",
       "   -0.005183956120163202,\n",
       "   0.01103190053254366,\n",
       "   -0.005847944878041744,\n",
       "   -0.01506425067782402,\n",
       "   0.002078423975035548,\n",
       "   -0.01103190053254366,\n",
       "   -0.005159748252481222,\n",
       "   0.004101515747606754,\n",
       "   -0.011654390953481197,\n",
       "   -0.014925920404493809,\n",
       "   0.18281830847263336,\n",
       "   -0.0333377830684185,\n",
       "   -0.00021549382654484361,\n",
       "   0.0008040491957217455,\n",
       "   -0.002042112173512578,\n",
       "   -0.018730023875832558,\n",
       "   0.026352064684033394,\n",
       "   -0.0010443994542583823,\n",
       "   -0.018370363861322403,\n",
       "   -0.007248546928167343,\n",
       "   0.005702697206288576,\n",
       "   0.003963184542953968,\n",
       "   -0.03203747048974037,\n",
       "   -0.0046513816341757774,\n",
       "   0.02256179414689541,\n",
       "   -0.016143232583999634,\n",
       "   -0.024111101403832436,\n",
       "   -0.04202497377991676,\n",
       "   -0.00850044284015894,\n",
       "   -0.0010124103864654899,\n",
       "   0.0022081092465668917,\n",
       "   -0.013425027951598167,\n",
       "   -0.00041628998587839305,\n",
       "   -0.029298515990376472,\n",
       "   0.011018067598342896,\n",
       "   0.0015916716074571013,\n",
       "   -0.0006207605474628508,\n",
       "   0.01608790084719658,\n",
       "   4.657190402213018e-06,\n",
       "   0.018176699057221413,\n",
       "   -0.033642109483480453,\n",
       "   0.008514275774359703,\n",
       "   0.022976787760853767,\n",
       "   -0.009911419823765755,\n",
       "   -0.020099500194191933,\n",
       "   -0.0048588779754936695,\n",
       "   0.022409629076719284,\n",
       "   -0.019269514828920364,\n",
       "   0.023391779512166977,\n",
       "   0.01451092679053545,\n",
       "   0.04517892003059387,\n",
       "   -0.014289597049355507,\n",
       "   -0.002991408808156848,\n",
       "   0.011861886829137802,\n",
       "   0.004748213104903698,\n",
       "   0.022575626149773598,\n",
       "   ...],\n",
       "  [-0.024334490299224854,\n",
       "   -0.009831630624830723,\n",
       "   0.017527448013424873,\n",
       "   -0.0370391346514225,\n",
       "   -0.012890665791928768,\n",
       "   0.016700681298971176,\n",
       "   0.01292511448264122,\n",
       "   0.0017827149713411927,\n",
       "   -0.013186924159526825,\n",
       "   -0.053877610713243484,\n",
       "   0.02463763765990734,\n",
       "   0.029047058895230293,\n",
       "   -0.010720403864979744,\n",
       "   0.00046936215949244797,\n",
       "   -0.016700681298971176,\n",
       "   0.006390215363353491,\n",
       "   0.03505489602684975,\n",
       "   0.01249106228351593,\n",
       "   -0.013765660114586353,\n",
       "   -0.034669071435928345,\n",
       "   -0.007020624820142984,\n",
       "   0.02098608762025833,\n",
       "   -0.006221416871994734,\n",
       "   -0.028330529108643532,\n",
       "   0.01058261003345251,\n",
       "   -0.0008521722047589719,\n",
       "   0.01624595932662487,\n",
       "   -0.03092106245458126,\n",
       "   -0.008274553343653679,\n",
       "   0.007847391068935394,\n",
       "   0.009852299466729164,\n",
       "   0.0018137188162654638,\n",
       "   -0.020490026101469994,\n",
       "   -0.015501869842410088,\n",
       "   -0.007475346326828003,\n",
       "   -0.031858064234256744,\n",
       "   0.015832576900720596,\n",
       "   -0.011498942971229553,\n",
       "   -0.002612926298752427,\n",
       "   -0.009287342429161072,\n",
       "   0.011064889840781689,\n",
       "   -0.012945783324539661,\n",
       "   -0.002573310397565365,\n",
       "   -0.019318774342536926,\n",
       "   -0.0006157687166705728,\n",
       "   0.008908407762646675,\n",
       "   0.004399086348712444,\n",
       "   -0.005177624523639679,\n",
       "   -0.011292250826954842,\n",
       "   0.0023183906450867653,\n",
       "   0.021234117448329926,\n",
       "   0.014688882976770401,\n",
       "   -0.012628857046365738,\n",
       "   0.005618566647171974,\n",
       "   0.01058261003345251,\n",
       "   -0.0019153421744704247,\n",
       "   -0.00724798534065485,\n",
       "   0.004040820524096489,\n",
       "   0.022226236760616302,\n",
       "   -0.012945783324539661,\n",
       "   -0.006786373909562826,\n",
       "   0.0021702616941183805,\n",
       "   -0.007606250699609518,\n",
       "   0.024830549955368042,\n",
       "   -0.008660377934575081,\n",
       "   -0.006248976103961468,\n",
       "   -0.028743911534547806,\n",
       "   -0.00830900203436613,\n",
       "   -0.004299185238778591,\n",
       "   0.010603278875350952,\n",
       "   0.028330529108643532,\n",
       "   0.010431036353111267,\n",
       "   -0.029267530888319016,\n",
       "   0.00031757302349433303,\n",
       "   0.03789345920085907,\n",
       "   -0.028936823830008507,\n",
       "   -0.00899797398597002,\n",
       "   -0.0024665198288857937,\n",
       "   -0.005363646894693375,\n",
       "   0.005125951487571001,\n",
       "   0.018381772562861443,\n",
       "   -0.024375829845666885,\n",
       "   -0.00041833517025224864,\n",
       "   0.04020840674638748,\n",
       "   -0.008336560800671577,\n",
       "   -0.0007923177909106016,\n",
       "   0.011278471909463406,\n",
       "   0.00013800970918964595,\n",
       "   -0.004347413312643766,\n",
       "   0.0012651247670874,\n",
       "   0.009246003814041615,\n",
       "   -0.00347069650888443,\n",
       "   0.03872022405266762,\n",
       "   0.021027425304055214,\n",
       "   -0.006459112279117107,\n",
       "   0.007716486230492592,\n",
       "   -0.01450975053012371,\n",
       "   0.029377765953540802,\n",
       "   -0.009121988900005817,\n",
       "   -0.03127932921051979,\n",
       "   -0.023328591138124466,\n",
       "   0.014909354038536549,\n",
       "   -0.018257757648825645,\n",
       "   -0.025547081604599953,\n",
       "   -0.04362570494413376,\n",
       "   0.008632819168269634,\n",
       "   0.023480165749788284,\n",
       "   -0.0021272010635584593,\n",
       "   -0.001709511736407876,\n",
       "   -0.013138695620000362,\n",
       "   -0.026980143040418625,\n",
       "   0.030673032626509666,\n",
       "   -0.02009042352437973,\n",
       "   -0.032712388783693314,\n",
       "   0.007296213414520025,\n",
       "   0.013255821540951729,\n",
       "   0.006080177612602711,\n",
       "   -0.006011280696839094,\n",
       "   -0.004182059783488512,\n",
       "   -0.03092106245458126,\n",
       "   0.005108727142214775,\n",
       "   -0.009053091518580914,\n",
       "   0.008067862130701542,\n",
       "   0.008529473096132278,\n",
       "   -0.007030959241092205,\n",
       "   -0.0025233598425984383,\n",
       "   -0.030121855437755585,\n",
       "   -0.028578558936715126,\n",
       "   -0.006038839463144541,\n",
       "   -0.004788355436176062,\n",
       "   0.005277525633573532,\n",
       "   0.015432972460985184,\n",
       "   -0.000658398843370378,\n",
       "   -0.012677084654569626,\n",
       "   -0.029928943142294884,\n",
       "   0.033456478267908096,\n",
       "   -0.012794209644198418,\n",
       "   -0.0006919862353242934,\n",
       "   -0.0075442432425916195,\n",
       "   -0.029460443183779716,\n",
       "   0.0033449591137468815,\n",
       "   0.011009772308170795,\n",
       "   -0.019525466486811638,\n",
       "   0.0010670453775674105,\n",
       "   -0.02462385967373848,\n",
       "   0.03254703804850578,\n",
       "   0.007971405982971191,\n",
       "   0.03877534344792366,\n",
       "   -6.249191210372373e-05,\n",
       "   0.005291305016726255,\n",
       "   0.012580628506839275,\n",
       "   -0.0052224076353013515,\n",
       "   -0.0026146487798541784,\n",
       "   0.0010894369333982468,\n",
       "   0.020145541056990623,\n",
       "   0.021743956953287125,\n",
       "   0.024706535041332245,\n",
       "   0.02725573256611824,\n",
       "   -0.040235962718725204,\n",
       "   -0.002194375731050968,\n",
       "   0.007365110795944929,\n",
       "   -0.0112715819850564,\n",
       "   -0.0026645991019904613,\n",
       "   0.00032467805431224406,\n",
       "   0.00659001711755991,\n",
       "   0.033952537924051285,\n",
       "   0.03916116803884506,\n",
       "   0.008791282773017883,\n",
       "   0.02200576476752758,\n",
       "   -0.02185419201850891,\n",
       "   -0.0008861902169883251,\n",
       "   0.03668086975812912,\n",
       "   -0.009769623167812824,\n",
       "   0.02374197542667389,\n",
       "   0.008495024405419827,\n",
       "   0.010761742480099201,\n",
       "   0.020490026101469994,\n",
       "   0.007496015168726444,\n",
       "   -0.007523573935031891,\n",
       "   -0.021564822643995285,\n",
       "   0.007241095416247845,\n",
       "   0.02076561562716961,\n",
       "   -0.0020927523728460073,\n",
       "   -0.010238124057650566,\n",
       "   -0.002185763558372855,\n",
       "   0.005783919710665941,\n",
       "   0.02477543242275715,\n",
       "   -0.014812897890806198,\n",
       "   0.008281443268060684,\n",
       "   0.007075742352753878,\n",
       "   -0.009831630624830723,\n",
       "   0.03003917820751667,\n",
       "   -0.014275499619543552,\n",
       "   0.02028333581984043,\n",
       "   -0.6129094958305359,\n",
       "   -0.009500923566520214,\n",
       "   0.00803341343998909,\n",
       "   -0.04224776104092598,\n",
       "   -0.010927096009254456,\n",
       "   -0.0014795673778280616,\n",
       "   -0.0013056019088253379,\n",
       "   -0.0016096108593046665,\n",
       "   -0.019070744514465332,\n",
       "   0.051645342260599136,\n",
       "   -0.008543252944946289,\n",
       "   -0.01711406372487545,\n",
       "   0.000851741642691195,\n",
       "   -0.007103301119059324,\n",
       "   -0.018767597153782845,\n",
       "   -0.014440853148698807,\n",
       "   0.0009447528282180429,\n",
       "   -0.010506822727620602,\n",
       "   0.003508589928969741,\n",
       "   0.030176972970366478,\n",
       "   0.0058907107450068,\n",
       "   0.006186968646943569,\n",
       "   0.0026198159903287888,\n",
       "   -0.002125478582456708,\n",
       "   -0.00025104416999965906,\n",
       "   0.01125780213624239,\n",
       "   -0.0010653228964656591,\n",
       "   -0.014785339124500751,\n",
       "   0.01975971646606922,\n",
       "   0.018299095332622528,\n",
       "   -0.044976092875003815,\n",
       "   0.014964471571147442,\n",
       "   0.01345562282949686,\n",
       "   -0.00155966030433774,\n",
       "   0.040759582072496414,\n",
       "   -0.013545189052820206,\n",
       "   -0.016659343615174294,\n",
       "   0.022915208712220192,\n",
       "   0.005136286374181509,\n",
       "   0.03681866452097893,\n",
       "   -0.02990138530731201,\n",
       "   -0.01831287518143654,\n",
       "   0.01726563833653927,\n",
       "   0.0159014742821455,\n",
       "   -0.013152475468814373,\n",
       "   0.019346334040164948,\n",
       "   -0.002983248792588711,\n",
       "   0.008984195068478584,\n",
       "   -0.010424146428704262,\n",
       "   -0.0016897037858143449,\n",
       "   -5.557527856581146e-06,\n",
       "   0.008763724006712437,\n",
       "   0.0054566580802202225,\n",
       "   0.006438443437218666,\n",
       "   0.02637384831905365,\n",
       "   0.003269172040745616,\n",
       "   0.01727941818535328,\n",
       "   -0.011230243369936943,\n",
       "   0.003854798385873437,\n",
       "   -0.014068808406591415,\n",
       "   0.01083752978593111,\n",
       "   0.0007354776025749743,\n",
       "   0.005997501313686371,\n",
       "   0.01796839013695717,\n",
       "   -0.048972129821777344,\n",
       "   0.027972262352705002,\n",
       "   -0.007840501144528389,\n",
       "   0.01674201898276806,\n",
       "   0.01748610846698284,\n",
       "   -0.04015328735113144,\n",
       "   0.0021754291374236345,\n",
       "   0.03516513109207153,\n",
       "   0.0022615506313741207,\n",
       "   -0.0036928898189216852,\n",
       "   -0.002382120583206415,\n",
       "   0.030810827389359474,\n",
       "   0.02288764901459217,\n",
       "   0.005088058300316334,\n",
       "   0.021881749853491783,\n",
       "   0.018023507669568062,\n",
       "   -3.1703475542599335e-05,\n",
       "   -0.005735691636800766,\n",
       "   0.007716486230492592,\n",
       "   -0.010782411321997643,\n",
       "   0.0426611453294754,\n",
       "   -0.026676995679736137,\n",
       "   0.002761055249720812,\n",
       "   -0.00553933484479785,\n",
       "   -0.01186409778892994,\n",
       "   -0.00864659808576107,\n",
       "   -0.015749899670481682,\n",
       "   0.027738012373447418,\n",
       "   -0.04434223845601082,\n",
       "   -0.04938551411032677,\n",
       "   0.009301122277975082,\n",
       "   0.034117892384529114,\n",
       "   -0.013462512753903866,\n",
       "   0.007082632277160883,\n",
       "   0.007558022625744343,\n",
       "   -0.021799074485898018,\n",
       "   -0.02077939547598362,\n",
       "   -0.009452695958316326,\n",
       "   0.01231881882995367,\n",
       "   0.020875850692391396,\n",
       "   0.014206602238118649,\n",
       "   0.012119017541408539,\n",
       "   -0.014647544361650944,\n",
       "   0.0032502254471182823,\n",
       "   0.04646427184343338,\n",
       "   -0.0073099927976727486,\n",
       "   0.009790292009711266,\n",
       "   -0.0017689355881884694,\n",
       "   -0.006731256376951933,\n",
       "   0.002246048767119646,\n",
       "   -0.014261719770729542,\n",
       "   -0.021702617406845093,\n",
       "   0.006397104822099209,\n",
       "   0.018753817304968834,\n",
       "   0.0042957402765750885,\n",
       "   -0.022832531481981277,\n",
       "   0.019869951531291008,\n",
       "   -0.017182961106300354,\n",
       "   -5.6786364439176396e-05,\n",
       "   -0.0056151216849684715,\n",
       "   0.012987121939659119,\n",
       "   -0.011843428947031498,\n",
       "   0.003231278620660305,\n",
       "   0.004219953436404467,\n",
       "   -0.011774531565606594,\n",
       "   0.012863107025623322,\n",
       "   0.004075269214808941,\n",
       "   -0.005384316202253103,\n",
       "   0.018988067284226418,\n",
       "   -0.01080997008830309,\n",
       "   0.002330447779968381,\n",
       "   0.014247940853238106,\n",
       "   0.007785383611917496,\n",
       "   -0.012511731125414371,\n",
       "   0.007020624820142984,\n",
       "   -0.03971234709024429,\n",
       "   -0.02531283162534237,\n",
       "   0.010265682823956013,\n",
       "   0.01100288238376379,\n",
       "   -0.012814879417419434,\n",
       "   -0.03177538886666298,\n",
       "   0.008687936700880527,\n",
       "   -0.0112715819850564,\n",
       "   -0.010775522328913212,\n",
       "   0.00838478934019804,\n",
       "   -0.018423110246658325,\n",
       "   0.022047104313969612,\n",
       "   -0.005570338573306799,\n",
       "   -0.007840501144528389,\n",
       "   0.013834557496011257,\n",
       "   -0.0008362397784367204,\n",
       "   -0.0011652238899841905,\n",
       "   -0.010548161342740059,\n",
       "   -0.042688705027103424,\n",
       "   -0.01834043487906456,\n",
       "   -0.027806909754872322,\n",
       "   -0.009618048556149006,\n",
       "   0.026304950937628746,\n",
       "   -0.015515649691224098,\n",
       "   0.008970415219664574,\n",
       "   -0.010100329294800758,\n",
       "   -0.01818886026740074,\n",
       "   0.003350126324221492,\n",
       "   0.013138695620000362,\n",
       "   -0.03172026947140694,\n",
       "   -0.016149504110217094,\n",
       "   -0.01729319617152214,\n",
       "   -0.037976134568452835,\n",
       "   0.00013402658805716783,\n",
       "   0.007030959241092205,\n",
       "   -0.005322308745235205,\n",
       "   0.02532660961151123,\n",
       "   -0.008687936700880527,\n",
       "   0.012663304805755615,\n",
       "   -0.0016655897488817573,\n",
       "   -0.022584501653909683,\n",
       "   0.006310983560979366,\n",
       "   -0.024210475385189056,\n",
       "   -0.020669160410761833,\n",
       "   -0.0030538684222847223,\n",
       "   0.03428324684500694,\n",
       "   0.014468411915004253,\n",
       "   0.04296429455280304,\n",
       "   0.013676093891263008,\n",
       "   0.000288722338154912,\n",
       "   0.01712784357368946,\n",
       "   0.008226325735449791,\n",
       "   0.03778322413563728,\n",
       "   0.009087540209293365,\n",
       "   -0.005704687908291817,\n",
       "   0.0054738824255764484,\n",
       "   -0.015818797051906586,\n",
       "   0.013324717991054058,\n",
       "   0.0011772809084504843,\n",
       "   -0.019497906789183617,\n",
       "   0.04335011914372444,\n",
       "   0.004516211338341236,\n",
       "   -0.004281960893422365,\n",
       "   0.002463074866682291,\n",
       "   -0.005828702822327614,\n",
       "   0.030011620372533798,\n",
       "   -0.00978340208530426,\n",
       "   0.002321835607290268,\n",
       "   -0.030948622152209282,\n",
       "   0.00466778501868248,\n",
       "   0.006528009660542011,\n",
       "   0.013462512753903866,\n",
       "   -0.01729319617152214,\n",
       "   0.005136286374181509,\n",
       "   -0.016177061945199966,\n",
       "   0.009714504703879356,\n",
       "   0.03475174680352211,\n",
       "   0.0016879814211279154,\n",
       "   0.025822671130299568,\n",
       "   -0.01974593661725521,\n",
       "   -0.0032812291756272316,\n",
       "   0.009108209982514381,\n",
       "   0.006662358995527029,\n",
       "   0.03053523786365986,\n",
       "   0.005494551733136177,\n",
       "   -0.01605304703116417,\n",
       "   0.03229900822043419,\n",
       "   0.011616067960858345,\n",
       "   0.012911335565149784,\n",
       "   -0.0046815648674964905,\n",
       "   -0.035358041524887085,\n",
       "   0.007241095416247845,\n",
       "   0.021743956953287125,\n",
       "   0.0024475730024278164,\n",
       "   0.019966408610343933,\n",
       "   0.000655384617857635,\n",
       "   -0.003589543979614973,\n",
       "   0.010672176256775856,\n",
       "   -0.016686901450157166,\n",
       "   0.03924384340643883,\n",
       "   -0.003474141238257289,\n",
       "   0.011526501737535,\n",
       "   0.005225852597504854,\n",
       "   0.007578691933304071,\n",
       "   -0.026236053556203842,\n",
       "   0.001758601050823927,\n",
       "   -0.00397881306707859,\n",
       "   0.015060927718877792,\n",
       "   0.0019997411873191595,\n",
       "   -0.023301033303141594,\n",
       "   0.016369974240660667,\n",
       "   -0.004939929116517305,\n",
       "   0.00136330327950418,\n",
       "   -0.012821768410503864,\n",
       "   -0.0050639440305531025,\n",
       "   0.008122979663312435,\n",
       "   -0.016383754089474678,\n",
       "   0.02167505957186222,\n",
       "   0.009025532752275467,\n",
       "   0.024499844759702682,\n",
       "   0.042275320738554,\n",
       "   -0.00864659808576107,\n",
       "   -0.00242518144659698,\n",
       "   0.010692845098674297,\n",
       "   -0.019690819084644318,\n",
       "   0.015749899670481682,\n",
       "   -0.017789257690310478,\n",
       "   0.01817508041858673,\n",
       "   -0.025119919329881668,\n",
       "   -0.025064801797270775,\n",
       "   0.013035350479185581,\n",
       "   -0.006479781586676836,\n",
       "   -0.012470393441617489,\n",
       "   -0.0026353178545832634,\n",
       "   -0.021895529702305794,\n",
       "   -0.007709596771746874,\n",
       "   -0.011120008304715157,\n",
       "   0.015515649691224098,\n",
       "   6.755154754500836e-05,\n",
       "   0.009025532752275467,\n",
       "   0.01726563833653927,\n",
       "   -0.018574684858322144,\n",
       "   -0.034999776631593704,\n",
       "   0.006851826328784227,\n",
       "   0.018119962885975838,\n",
       "   0.02166127972304821,\n",
       "   -0.027007702738046646,\n",
       "   -0.025822671130299568,\n",
       "   -0.008970415219664574,\n",
       "   -0.0032829514238983393,\n",
       "   -0.005566893611103296,\n",
       "   -0.008398568257689476,\n",
       "   -0.0027248843107372522,\n",
       "   -0.019773496314883232,\n",
       "   -0.03147223964333534,\n",
       "   -0.0370391346514225,\n",
       "   0.011106228455901146,\n",
       "   0.03827928379178047,\n",
       "   -0.018216419965028763,\n",
       "   -0.02268095873296261,\n",
       "   0.011443824507296085,\n",
       "   -0.0007165308925323188,\n",
       "   0.016507769003510475,\n",
       "   -0.028854146599769592,\n",
       "   -0.007427118252962828,\n",
       "   0.01694871112704277,\n",
       "   0.01204323023557663,\n",
       "   -0.014812897890806198,\n",
       "   -0.01659044623374939,\n",
       "   -0.011264692060649395,\n",
       "   -0.012429054826498032,\n",
       "   0.027476202696561813,\n",
       "   -0.0016897037858143449,\n",
       "   0.0068966094404459,\n",
       "   -0.01309735793620348,\n",
       "   -0.0025543635711073875,\n",
       "   0.009852299466729164,\n",
       "   -0.00728932349011302,\n",
       "   0.019346334040164948,\n",
       "   0.04084226116538048,\n",
       "   0.0038479084614664316,\n",
       "   0.006211082451045513,\n",
       "   -0.02689746581017971,\n",
       "   -0.0007341857999563217,\n",
       "   0.01817508041858673,\n",
       "   0.06366100907325745,\n",
       "   0.032905302941799164,\n",
       "   -0.027986042201519012,\n",
       "   0.003985702991485596,\n",
       "   -0.002194375731050968,\n",
       "   0.004957153461873531,\n",
       "   -0.0004508460406213999,\n",
       "   -0.012201693840324879,\n",
       "   0.023865990340709686,\n",
       "   0.005601342301815748,\n",
       "   -0.009342459961771965,\n",
       "   -0.008867069147527218,\n",
       "   0.007620030082762241,\n",
       "   -0.0027421084232628345,\n",
       "   0.0009636995964683592,\n",
       "   0.008557031862437725,\n",
       "   -0.0021013645455241203,\n",
       "   -0.01319381408393383,\n",
       "   0.007695816922932863,\n",
       "   -0.01249106228351593,\n",
       "   0.0021375357173383236,\n",
       "   -0.020186878740787506,\n",
       "   0.01431683823466301,\n",
       "   0.017169181257486343,\n",
       "   0.004967487882822752,\n",
       "   0.00030099463765509427,\n",
       "   0.02269473671913147,\n",
       "   0.03425568714737892,\n",
       "   -0.010451705195009708,\n",
       "   -0.03127932921051979,\n",
       "   -0.018574684858322144,\n",
       "   0.007695816922932863,\n",
       "   0.00968694593757391,\n",
       "   -0.007785383611917496,\n",
       "   -0.008763724006712437,\n",
       "   0.03268483281135559,\n",
       "   0.024885667487978935,\n",
       "   -0.023369930684566498,\n",
       "   0.002370063681155443,\n",
       "   -0.009370018728077412,\n",
       "   0.010555051267147064,\n",
       "   0.01623217947781086,\n",
       "   0.009618048556149006,\n",
       "   -0.020848292857408524,\n",
       "   -0.004316409584134817,\n",
       "   -0.015322737395763397,\n",
       "   0.017251858487725258,\n",
       "   0.05079101398587227,\n",
       "   -0.01660422421991825,\n",
       "   -0.0062248618341982365,\n",
       "   0.004392196424305439,\n",
       "   -0.0015941089950501919,\n",
       "   -0.031554918736219406,\n",
       "   -0.006803598254919052,\n",
       "   0.007158419117331505,\n",
       "   -0.008963525295257568,\n",
       "   0.01850578747689724,\n",
       "   -0.017568785697221756,\n",
       "   0.01457864698022604,\n",
       "   -0.0036205477081239223,\n",
       "   -0.013565858826041222,\n",
       "   -0.02131679281592369,\n",
       "   0.004006372299045324,\n",
       "   -0.024568742141127586,\n",
       "   -0.015088486485183239,\n",
       "   -0.012270591221749783,\n",
       "   -0.008777502924203873,\n",
       "   -0.014895574189722538,\n",
       "   -0.033787187188863754,\n",
       "   0.007282434031367302,\n",
       "   -0.015171162784099579,\n",
       "   -0.03254703804850578,\n",
       "   -0.05305084213614464,\n",
       "   -0.03232656419277191,\n",
       "   0.018767597153782845,\n",
       "   0.010982213541865349,\n",
       "   0.01074107363820076,\n",
       "   -0.022804973646998405,\n",
       "   0.007998964749276638,\n",
       "   -0.003892691805958748,\n",
       "   0.0004775437118951231,\n",
       "   -0.015129825100302696,\n",
       "   -0.012773540802299976,\n",
       "   -0.04511388763785362,\n",
       "   0.011126897297799587,\n",
       "   -0.017568785697221756,\n",
       "   -0.014757780358195305,\n",
       "   -0.01851956732571125,\n",
       "   -0.01710028573870659,\n",
       "   0.0069861761294305325,\n",
       "   0.013731211423873901,\n",
       "   0.009872968308627605,\n",
       "   -0.014537309296429157,\n",
       "   0.010224344208836555,\n",
       "   -0.01573611982166767,\n",
       "   -0.001319381408393383,\n",
       "   0.014275499619543552,\n",
       "   0.005484216846525669,\n",
       "   0.01641131192445755,\n",
       "   -0.03312577307224274,\n",
       "   0.010072770528495312,\n",
       "   -0.010789301246404648,\n",
       "   -0.025367949157953262,\n",
       "   -0.015791237354278564,\n",
       "   -0.012077678926289082,\n",
       "   0.003453472163528204,\n",
       "   -0.004461093805730343,\n",
       "   0.0016991771990433335,\n",
       "   -0.0065762377344071865,\n",
       "   -0.015253840014338493,\n",
       "   0.04224776104092598,\n",
       "   -0.014812897890806198,\n",
       "   0.005136286374181509,\n",
       "   0.024003785103559494,\n",
       "   0.02287387102842331,\n",
       "   0.012932004407048225,\n",
       "   0.022667178884148598,\n",
       "   0.01657666638493538,\n",
       "   -0.001197950099594891,\n",
       "   -0.013049129396677017,\n",
       "   0.008233215659856796,\n",
       "   -0.023314811289310455,\n",
       "   0.04246823489665985,\n",
       "   0.0015889416681602597,\n",
       "   -0.013820778578519821,\n",
       "   0.041310761123895645,\n",
       "   0.008612150326371193,\n",
       "   -0.00475390674546361,\n",
       "   -0.027986042201519012,\n",
       "   -0.0108237499371171,\n",
       "   -0.021178999915719032,\n",
       "   0.020352233201265335,\n",
       "   -0.0005950995837338269,\n",
       "   -0.0227360762655735,\n",
       "   -0.006569347810000181,\n",
       "   0.009480254724621773,\n",
       "   -0.0004004649817943573,\n",
       "   -0.010499932803213596,\n",
       "   -0.023259693756699562,\n",
       "   -0.015997929498553276,\n",
       "   0.01784437522292137,\n",
       "   -0.014247940853238106,\n",
       "   0.02115144021809101,\n",
       "   -0.0078542809933424,\n",
       "   0.02046246826648712,\n",
       "   -0.03455883637070656,\n",
       "   -0.012608187273144722,\n",
       "   0.01730697602033615,\n",
       "   0.004188949707895517,\n",
       "   0.013662314973771572,\n",
       "   -0.009066871367394924,\n",
       "   0.004640226252377033,\n",
       "   -0.007971405982971191,\n",
       "   -0.015818797051906586,\n",
       "   -0.014020579867064953,\n",
       "   -0.017086505889892578,\n",
       "   0.013510740362107754,\n",
       "   0.013607196509838104,\n",
       "   0.018051065504550934,\n",
       "   0.0058493721298873425,\n",
       "   0.04191705584526062,\n",
       "   -0.005546224303543568,\n",
       "   0.018726259469985962,\n",
       "   0.019497906789183617,\n",
       "   -0.009218445047736168,\n",
       "   -0.008102310821413994,\n",
       "   -0.009225334972143173,\n",
       "   0.007661368697881699,\n",
       "   -0.030617915093898773,\n",
       "   -0.026966363191604614,\n",
       "   -0.0025336944963783026,\n",
       "   0.008550142869353294,\n",
       "   0.0007557161734439433,\n",
       "   -0.004936484154313803,\n",
       "   0.04169658571481705,\n",
       "   0.02444472536444664,\n",
       "   0.006231751758605242,\n",
       "   -0.01657666638493538,\n",
       "   -0.03389742225408554,\n",
       "   -0.03827928379178047,\n",
       "   -0.02565731666982174,\n",
       "   0.04354302957653999,\n",
       "   0.008501914329826832,\n",
       "   0.004995047114789486,\n",
       "   -0.02042113058269024,\n",
       "   0.0012401496060192585,\n",
       "   0.02550574392080307,\n",
       "   0.0022615506313741207,\n",
       "   0.02061404287815094,\n",
       "   -0.004936484154313803,\n",
       "   0.04613356292247772,\n",
       "   -0.008081641048192978,\n",
       "   0.024224255234003067,\n",
       "   -0.015322737395763397,\n",
       "   0.040594231337308884,\n",
       "   -0.021578602492809296,\n",
       "   0.0037238935474306345,\n",
       "   -0.03444860130548477,\n",
       "   -0.002258105669170618,\n",
       "   -0.006803598254919052,\n",
       "   -0.0067415907979011536,\n",
       "   0.012084568850696087,\n",
       "   0.00020733750716317445,\n",
       "   0.009652497246861458,\n",
       "   0.008970415219664574,\n",
       "   -0.01953924633562565,\n",
       "   -0.012849327176809311,\n",
       "   -0.018423110246658325,\n",
       "   0.015805017203092575,\n",
       "   -0.0384446382522583,\n",
       "   -0.0011574729578569531,\n",
       "   -0.016011709347367287,\n",
       "   -0.002633595373481512,\n",
       "   0.0017439603107050061,\n",
       "   -0.006693362724035978,\n",
       "   -0.0019032851560041308,\n",
       "   -0.016369974240660667,\n",
       "   0.021978206932544708,\n",
       "   -0.03618481010198593,\n",
       "   -0.024334490299224854,\n",
       "   0.027545100077986717,\n",
       "   0.018781377002596855,\n",
       "   0.037976134568452835,\n",
       "   -0.0011901991674676538,\n",
       "   0.03161003440618515,\n",
       "   0.010927096009254456,\n",
       "   0.01223614253103733,\n",
       "   -0.003641217015683651,\n",
       "   0.021427029743790627,\n",
       "   0.0068966094404459,\n",
       "   -0.0068104881793260574,\n",
       "   0.01678335852921009,\n",
       "   0.0024958010762929916,\n",
       "   -0.026828568428754807,\n",
       "   -0.023301033303141594,\n",
       "   0.002078972989693284,\n",
       "   0.008357230573892593,\n",
       "   -0.016549106687307358,\n",
       "   -0.034310806542634964,\n",
       "   0.005504886154085398,\n",
       "   -0.012125907465815544,\n",
       "   0.0003505145141389221,\n",
       "   -0.023673078045248985,\n",
       "   -0.011299140751361847,\n",
       "   -0.04315720498561859,\n",
       "   0.01319381408393383,\n",
       "   -0.006338542327284813,\n",
       "   0.01660422421991825,\n",
       "   -0.002087585162371397,\n",
       "   -0.006107736844569445,\n",
       "   -0.028220292180776596,\n",
       "   0.05382249131798744,\n",
       "   0.009370018728077412,\n",
       "   0.015832576900720596,\n",
       "   -0.018285317346453667,\n",
       "   -0.0018585019279271364,\n",
       "   0.02042113058269024,\n",
       "   0.0035447608679533005,\n",
       "   -0.0003285535203758627,\n",
       "   0.023259693756699562,\n",
       "   -0.0021530375815927982,\n",
       "   0.036074575036764145,\n",
       "   -0.012911335565149784,\n",
       "   0.017927050590515137,\n",
       "   0.001380527624860406,\n",
       "   -0.009928086772561073,\n",
       "   0.00619730306789279,\n",
       "   -0.008439906872808933,\n",
       "   -0.006586572155356407,\n",
       "   0.028854146599769592,\n",
       "   -0.018423110246658325,\n",
       "   -0.015253840014338493,\n",
       "   -0.01766524277627468,\n",
       "   -0.0045058769173920155,\n",
       "   0.006014725659042597,\n",
       "   -0.024031342938542366,\n",
       "   -0.008529473096132278,\n",
       "   -0.04596821218729019,\n",
       "   0.004926149733364582,\n",
       "   -0.000716100272256881,\n",
       "   0.021344352513551712,\n",
       "   0.012332598678767681,\n",
       "   -7.040432683425024e-05,\n",
       "   0.0010868533281609416,\n",
       "   0.004884811583906412,\n",
       "   -0.001253067865036428,\n",
       "   0.006345432251691818,\n",
       "   -0.001464065513573587,\n",
       "   0.00881884153932333,\n",
       "   -0.03183050453662872,\n",
       "   -0.007241095416247845,\n",
       "   0.001033458043821156,\n",
       "   0.013593417592346668,\n",
       "   0.014564868062734604,\n",
       "   -0.007633809465914965,\n",
       "   -0.026759672909975052,\n",
       "   0.004960598424077034,\n",
       "   0.027159275487065315,\n",
       "   0.0006528009544126689,\n",
       "   0.013510740362107754,\n",
       "   0.01215346623212099,\n",
       "   -0.025188816711306572,\n",
       "   -0.0073788901790976524,\n",
       "   -0.005491106770932674,\n",
       "   -0.007626920007169247,\n",
       "   -0.005446323659271002,\n",
       "   0.03723204508423805,\n",
       "   -0.030673032626509666,\n",
       "   -0.01798216812312603,\n",
       "   -0.011409375816583633,\n",
       "   -0.01267019473016262,\n",
       "   0.026787230744957924,\n",
       "   -0.012112127617001534,\n",
       "   0.004182059783488512,\n",
       "   -0.00510528264567256,\n",
       "   0.007165308576077223,\n",
       "   0.02270851656794548,\n",
       "   -0.005122506525367498,\n",
       "   -0.017086505889892578,\n",
       "   0.019966408610343933,\n",
       "   0.002752443077042699,\n",
       "   -0.008687936700880527,\n",
       "   0.011051110923290253,\n",
       "   -0.030838387086987495,\n",
       "   0.0043301889672875404,\n",
       "   0.0003591266577132046,\n",
       "   0.012194803915917873,\n",
       "   -0.013462512753903866,\n",
       "   -0.035716306418180466,\n",
       "   0.0073513309471309185,\n",
       "   -0.020696718245744705,\n",
       "   0.009390688501298428,\n",
       "   -0.023976225405931473,\n",
       "   0.013207593001425266,\n",
       "   -0.008853290230035782,\n",
       "   -0.028358086943626404,\n",
       "   -0.0007905953680165112,\n",
       "   0.014702661894261837,\n",
       "   0.002197820693254471,\n",
       "   -0.014137704856693745,\n",
       "   0.0036136580165475607,\n",
       "   0.020641600713133812,\n",
       "   0.0235628429800272,\n",
       "   0.03353915736079216,\n",
       "   -0.011299140751361847,\n",
       "   -0.021427029743790627,\n",
       "   -0.022653399035334587,\n",
       "   -0.011588509194552898,\n",
       "   -0.0471256859600544,\n",
       "   -0.010637727566063404,\n",
       "   -0.026318730786442757,\n",
       "   0.034310806542634964,\n",
       "   0.007406448945403099,\n",
       "   -0.008729275315999985,\n",
       "   -0.01796839013695717,\n",
       "   0.0034586393740028143,\n",
       "   -0.03714936971664429,\n",
       "   0.0036102132871747017,\n",
       "   -0.002564698224887252,\n",
       "   0.01714162342250347,\n",
       "   -0.003401799127459526,\n",
       "   -0.0026559869293123484,\n",
       "   -0.03425568714737892,\n",
       "   0.02270851656794548,\n",
       "   -0.011919215321540833,\n",
       "   0.01678335852921009,\n",
       "   -0.0050604990683496,\n",
       "   0.008853290230035782,\n",
       "   0.0021650944836437702,\n",
       "   0.0016673122299835086,\n",
       "   -0.0003696765343192965,\n",
       "   -0.008660377934575081,\n",
       "   -0.008081641048192978,\n",
       "   0.0018757262732833624,\n",
       "   0.02990138530731201,\n",
       "   -0.005132841411978006,\n",
       "   -0.019001847133040428,\n",
       "   0.01624595932662487,\n",
       "   -0.001154889352619648,\n",
       "   0.01439951453357935,\n",
       "   -0.011753861792385578,\n",
       "   0.007881839759647846,\n",
       "   0.019456569105386734,\n",
       "   0.010431036353111267,\n",
       "   0.03863754868507385,\n",
       "   -0.03880290314555168,\n",
       "   0.014068808406591415,\n",
       "   0.006534899119287729,\n",
       "   -0.00431985454633832,\n",
       "   -0.03750763460993767,\n",
       "   0.015515649691224098,\n",
       "   0.005053609609603882,\n",
       "   0.015570767223834991,\n",
       "   0.03262971341609955,\n",
       "   -0.008991084061563015,\n",
       "   0.01318003423511982,\n",
       "   0.014881795272231102,\n",
       "   0.00026353177963756025,\n",
       "   0.05101148784160614,\n",
       "   0.009514703415334225,\n",
       "   0.01817508041858673,\n",
       "   0.01537785492837429,\n",
       "   0.020379791036248207,\n",
       "   0.0006484948680736125,\n",
       "   -0.019249876961112022,\n",
       "   -0.0011187182972207665,\n",
       "   -0.019167201593518257,\n",
       "   0.026332508772611618,\n",
       "   0.01816130243241787,\n",
       "   -0.01835421472787857,\n",
       "   0.0033656281884759665,\n",
       "   -0.015240060165524483,\n",
       "   0.014371955767273903,\n",
       "   -0.021344352513551712,\n",
       "   -0.005504886154085398,\n",
       "   0.002395900199189782,\n",
       "   0.01204323023557663,\n",
       "   -0.02776557207107544,\n",
       "   0.025932906195521355,\n",
       "   0.006824267562478781,\n",
       "   -0.01586013473570347,\n",
       "   -0.0022391590755432844,\n",
       "   0.008942856453359127,\n",
       "   0.004970932845026255,\n",
       "   -0.00632476294413209,\n",
       "   0.003954699262976646,\n",
       "   -0.004791800398379564,\n",
       "   -0.025271492078900337,\n",
       "   0.0009533650008961558,\n",
       "   -0.004419755190610886,\n",
       "   -0.010196785442531109,\n",
       "   -0.00035438998020254076,\n",
       "   -0.029460443183779716,\n",
       "   0.009928086772561073,\n",
       "   -0.013999911025166512,\n",
       "   -0.010086550377309322,\n",
       "   0.17902247607707977,\n",
       "   -0.03822416439652443,\n",
       "   -0.003958144225180149,\n",
       "   0.001937733730301261,\n",
       "   0.011188904754817486,\n",
       "   -0.010327690280973911,\n",
       "   0.01937389187514782,\n",
       "   0.011684965342283249,\n",
       "   -0.0204349085688591,\n",
       "   -0.0019515131134539843,\n",
       "   0.030976179987192154,\n",
       "   -0.012339488603174686,\n",
       "   -0.0291848536580801,\n",
       "   -0.0003412564401514828,\n",
       "   0.0204349085688591,\n",
       "   0.0022822197061032057,\n",
       "   -0.0060422844253480434,\n",
       "   -0.04318476468324661,\n",
       "   -0.002407957101240754,\n",
       "   0.01135425828397274,\n",
       "   0.034669071435928345,\n",
       "   -0.01196055393666029,\n",
       "   0.011333589442074299,\n",
       "   -0.015405413694679737,\n",
       "   0.03808637335896492,\n",
       "   -0.02619471587240696,\n",
       "   0.006428108550608158,\n",
       "   0.0103139104321599,\n",
       "   -0.006924168672412634,\n",
       "   0.01955302432179451,\n",
       "   -0.010975323617458344,\n",
       "   -0.0007527018897235394,\n",
       "   0.01319381408393383,\n",
       "   -0.0013977519702166319,\n",
       "   -0.029239971190690994,\n",
       "   -0.0029608572367578745,\n",
       "   0.03006673790514469,\n",
       "   -0.028068719431757927,\n",
       "   0.02219867706298828,\n",
       "   -0.014812897890806198,\n",
       "   0.017045166343450546,\n",
       "   -0.014661324210464954,\n",
       "   0.010355249047279358,\n",
       "   -0.013310939073562622,\n",
       "   0.0069758412428200245,\n",
       "   0.009459585882723331,\n",
       "   ...],\n",
       "  [-0.0081649674102664,\n",
       "   -0.0046259090304374695,\n",
       "   0.03896360099315643,\n",
       "   -0.020622994750738144,\n",
       "   -0.007655505556613207,\n",
       "   -0.005743327550590038,\n",
       "   0.019142160192131996,\n",
       "   0.00876952800899744,\n",
       "   -0.007913632318377495,\n",
       "   -0.05189712718129158,\n",
       "   0.019114989787340164,\n",
       "   0.022212514653801918,\n",
       "   -0.006887917406857014,\n",
       "   0.014142646454274654,\n",
       "   -0.012070837430655956,\n",
       "   0.005556524731218815,\n",
       "   0.043039292097091675,\n",
       "   0.01483551412820816,\n",
       "   0.0009178795153275132,\n",
       "   -0.012247450649738312,\n",
       "   -0.020595824345946312,\n",
       "   0.012369721196591854,\n",
       "   -0.005838426761329174,\n",
       "   -0.023652590811252594,\n",
       "   0.0014078115345910192,\n",
       "   0.015093641355633736,\n",
       "   0.01603104919195175,\n",
       "   -0.03282289579510689,\n",
       "   -0.007146044168621302,\n",
       "   0.007383793126791716,\n",
       "   0.003831149311736226,\n",
       "   0.0027392038609832525,\n",
       "   -0.013925276696681976,\n",
       "   -0.008260066621005535,\n",
       "   -0.002056525554507971,\n",
       "   -0.03230664134025574,\n",
       "   0.019807856529951096,\n",
       "   -0.021343033760786057,\n",
       "   0.008694807067513466,\n",
       "   0.0005047912709414959,\n",
       "   0.026899557560682297,\n",
       "   0.010664723813533783,\n",
       "   0.0021991748362779617,\n",
       "   -0.03238815441727638,\n",
       "   0.0027748660650104284,\n",
       "   0.015283839777112007,\n",
       "   0.009245024994015694,\n",
       "   -0.012933525256812572,\n",
       "   -0.005892769433557987,\n",
       "   0.009666179306805134,\n",
       "   0.015283839777112007,\n",
       "   0.023842791095376015,\n",
       "   -0.031138276681303978,\n",
       "   0.0020887914579361677,\n",
       "   -0.0152566684409976,\n",
       "   -0.008368751965463161,\n",
       "   -0.020514309406280518,\n",
       "   0.010800580494105816,\n",
       "   0.034480344504117966,\n",
       "   -0.011928187683224678,\n",
       "   -0.01634351909160614,\n",
       "   0.01247840654104948,\n",
       "   0.0022450261749327183,\n",
       "   0.006962638348340988,\n",
       "   -0.001350072561763227,\n",
       "   0.0013364868937060237,\n",
       "   -0.03825714811682701,\n",
       "   -0.009516737423837185,\n",
       "   -0.008049488998949528,\n",
       "   -0.021112076938152313,\n",
       "   0.027565253898501396,\n",
       "   0.021641917526721954,\n",
       "   -0.012661811895668507,\n",
       "   -0.014889856800436974,\n",
       "   0.02713051438331604,\n",
       "   -0.016098977997899055,\n",
       "   0.006911692209541798,\n",
       "   -0.011017950251698494,\n",
       "   -0.020650165155529976,\n",
       "   0.010630759410560131,\n",
       "   -0.0018170787952840328,\n",
       "   -0.02698107250034809,\n",
       "   -0.00868801400065422,\n",
       "   0.036001935601234436,\n",
       "   0.0029327990487217903,\n",
       "   -0.004856864921748638,\n",
       "   -0.0019495387095957994,\n",
       "   -0.009550701826810837,\n",
       "   -0.011833088472485542,\n",
       "   0.013463364914059639,\n",
       "   0.010155262425541878,\n",
       "   -0.011853466741740704,\n",
       "   0.024685099720954895,\n",
       "   0.021003393456339836,\n",
       "   -0.0041368259117007256,\n",
       "   0.004856864921748638,\n",
       "   -0.029535172507166862,\n",
       "   0.00783211924135685,\n",
       "   -0.011418726295232773,\n",
       "   -0.027252785861492157,\n",
       "   -0.0205414816737175,\n",
       "   0.020949050784111023,\n",
       "   -0.01304900273680687,\n",
       "   -0.027850553393363953,\n",
       "   -0.01054245326668024,\n",
       "   0.004833089653402567,\n",
       "   0.020663751289248466,\n",
       "   0.006001454312354326,\n",
       "   0.0037054819986224174,\n",
       "   0.003970401827245951,\n",
       "   -0.010053370147943497,\n",
       "   0.03673555701971054,\n",
       "   -0.009509945288300514,\n",
       "   -0.029453657567501068,\n",
       "   0.00307375006377697,\n",
       "   0.001842551864683628,\n",
       "   0.022008730098605156,\n",
       "   -0.008552157320082188,\n",
       "   0.00910916831344366,\n",
       "   -0.026383304968476295,\n",
       "   0.02429111674427986,\n",
       "   -0.021451719105243683,\n",
       "   0.01753905601799488,\n",
       "   -0.011785538867115974,\n",
       "   -0.006921881344169378,\n",
       "   -0.008776320144534111,\n",
       "   -0.012417270801961422,\n",
       "   -0.009455602616071701,\n",
       "   -0.011839881539344788,\n",
       "   -0.019155746325850487,\n",
       "   -0.007146044168621302,\n",
       "   0.010032991878688335,\n",
       "   0.0067316824570298195,\n",
       "   0.0029005331452935934,\n",
       "   -0.02847549133002758,\n",
       "   0.02761959657073021,\n",
       "   0.0025490049738436937,\n",
       "   -0.017308099195361137,\n",
       "   -0.0118942242115736,\n",
       "   -0.03355652093887329,\n",
       "   0.006429402157664299,\n",
       "   0.017090730369091034,\n",
       "   -0.025513824075460434,\n",
       "   -0.007621541619300842,\n",
       "   -0.021669087931513786,\n",
       "   0.037496354430913925,\n",
       "   0.003831149311736226,\n",
       "   0.048935458064079285,\n",
       "   0.010257154703140259,\n",
       "   -0.0016642403788864613,\n",
       "   0.01338185090571642,\n",
       "   -0.0038515275809913874,\n",
       "   -0.015759337693452835,\n",
       "   -0.007770983502268791,\n",
       "   0.0081649674102664,\n",
       "   0.012838426046073437,\n",
       "   0.024032989516854286,\n",
       "   0.030948078259825706,\n",
       "   -0.0081649674102664,\n",
       "   0.006894710008054972,\n",
       "   -0.006694321986287832,\n",
       "   -0.00919068232178688,\n",
       "   -0.010841337032616138,\n",
       "   0.0028937403112649918,\n",
       "   -0.009496359154582024,\n",
       "   0.028421150520443916,\n",
       "   0.030241625383496284,\n",
       "   0.010983986780047417,\n",
       "   0.005699174012988806,\n",
       "   -0.019508972764015198,\n",
       "   -0.014006790705025196,\n",
       "   0.03858320415019989,\n",
       "   -0.0033047059550881386,\n",
       "   0.03366520628333092,\n",
       "   0.013721492141485214,\n",
       "   0.008558950386941433,\n",
       "   0.024508487433195114,\n",
       "   -0.007424549665302038,\n",
       "   -0.022144585847854614,\n",
       "   -0.011119842529296875,\n",
       "   0.006324113346636295,\n",
       "   0.005498785991221666,\n",
       "   -0.006110139656811953,\n",
       "   0.0018748176516965032,\n",
       "   -0.010223190300166607,\n",
       "   0.0021363412961363792,\n",
       "   0.0339912585914135,\n",
       "   -0.007383793126791716,\n",
       "   -0.00026109893224202096,\n",
       "   0.00741775706410408,\n",
       "   -0.0044051422737538815,\n",
       "   0.016397861763834953,\n",
       "   -0.01119456347078085,\n",
       "   0.02157398872077465,\n",
       "   -0.626895546913147,\n",
       "   -0.012444442138075829,\n",
       "   0.01734885573387146,\n",
       "   -0.022198928520083427,\n",
       "   -0.013966033235192299,\n",
       "   0.006626393646001816,\n",
       "   -0.0032707417849451303,\n",
       "   -0.015732165426015854,\n",
       "   -0.016221249476075172,\n",
       "   0.05377194657921791,\n",
       "   -0.005026685073971748,\n",
       "   -0.019508972764015198,\n",
       "   0.006633186712861061,\n",
       "   0.0018306643469259143,\n",
       "   -0.015596309676766396,\n",
       "   -0.01869383454322815,\n",
       "   0.009802035987377167,\n",
       "   -0.010725859552621841,\n",
       "   0.020446380600333214,\n",
       "   0.02255215495824814,\n",
       "   0.002158417832106352,\n",
       "   0.017471127212047577,\n",
       "   0.006500726565718651,\n",
       "   0.002963366685435176,\n",
       "   -0.002903929678723216,\n",
       "   0.013096552342176437,\n",
       "   -0.000906841189134866,\n",
       "   -0.025989320129156113,\n",
       "   0.021696260198950768,\n",
       "   0.004557980690151453,\n",
       "   -0.04298495128750801,\n",
       "   0.011486655101180077,\n",
       "   0.019658414646983147,\n",
       "   -0.0023333330173045397,\n",
       "   0.05094613507390022,\n",
       "   -0.015813680365681648,\n",
       "   -0.018299851566553116,\n",
       "   0.02929062955081463,\n",
       "   0.007492478005588055,\n",
       "   0.03619213402271271,\n",
       "   -0.023802032694220543,\n",
       "   -0.018490049988031387,\n",
       "   0.013830176554620266,\n",
       "   0.014346431009471416,\n",
       "   -0.02154681831598282,\n",
       "   0.02340804971754551,\n",
       "   0.007023773621767759,\n",
       "   0.007573992013931274,\n",
       "   -0.011439105495810509,\n",
       "   -0.005318776238709688,\n",
       "   0.004547791555523872,\n",
       "   0.005756913218647242,\n",
       "   -0.004276078660041094,\n",
       "   -0.0005519164260476828,\n",
       "   0.026505574584007263,\n",
       "   -0.006622997112572193,\n",
       "   0.016941286623477936,\n",
       "   -0.011670061387121677,\n",
       "   0.026736531406641006,\n",
       "   -0.025323623791337013,\n",
       "   0.005193109158426523,\n",
       "   -0.004364385269582272,\n",
       "   -0.01753905601799488,\n",
       "   0.005216883961111307,\n",
       "   -0.05475011095404625,\n",
       "   0.023951474577188492,\n",
       "   -0.016302762553095818,\n",
       "   0.0038515275809913874,\n",
       "   0.0035356616135686636,\n",
       "   -0.0376865528523922,\n",
       "   0.009951477870345116,\n",
       "   0.012872389517724514,\n",
       "   0.01154778990894556,\n",
       "   -0.010786994360387325,\n",
       "   -0.0024182430934160948,\n",
       "   0.03638233244419098,\n",
       "   0.009238231927156448,\n",
       "   -7.583544356748462e-05,\n",
       "   0.003729257034137845,\n",
       "   0.006164482329040766,\n",
       "   -0.006028625648468733,\n",
       "   -0.0052134874276816845,\n",
       "   -0.008524985983967781,\n",
       "   -0.0013611108297482133,\n",
       "   0.026899557560682297,\n",
       "   -0.01937311701476574,\n",
       "   0.012362928129732609,\n",
       "   0.01548762433230877,\n",
       "   -0.014088303782045841,\n",
       "   0.0059471121057868,\n",
       "   -0.020595824345946312,\n",
       "   0.018911205232143402,\n",
       "   -0.03689858689904213,\n",
       "   -0.03537699580192566,\n",
       "   0.0014154533855617046,\n",
       "   0.03956137225031853,\n",
       "   -0.021139249205589294,\n",
       "   0.011235320940613747,\n",
       "   0.01129645574837923,\n",
       "   -0.016737503930926323,\n",
       "   0.0003152292047161609,\n",
       "   -0.011588547378778458,\n",
       "   0.016927702352404594,\n",
       "   0.005250848364084959,\n",
       "   0.01637069135904312,\n",
       "   0.008097038604319096,\n",
       "   -0.005040270742028952,\n",
       "   0.012784083373844624,\n",
       "   0.033583689481019974,\n",
       "   -0.02529645338654518,\n",
       "   -0.004680251237004995,\n",
       "   0.009319745935499668,\n",
       "   -0.00433381786569953,\n",
       "   -0.008531779050827026,\n",
       "   -0.015555553138256073,\n",
       "   -0.020949050784111023,\n",
       "   0.003248665016144514,\n",
       "   -0.005417271982878447,\n",
       "   0.014998541213572025,\n",
       "   -0.0272663701325655,\n",
       "   0.014006790705025196,\n",
       "   -0.0028071319684386253,\n",
       "   -0.0033981071319431067,\n",
       "   -0.024535657837986946,\n",
       "   0.008986897766590118,\n",
       "   -0.00547161465510726,\n",
       "   -0.002374089788645506,\n",
       "   -0.005824841093271971,\n",
       "   -0.00029930853634141386,\n",
       "   -0.002747694728896022,\n",
       "   0.00459534116089344,\n",
       "   -0.007295486517250538,\n",
       "   0.014794757589697838,\n",
       "   -0.011690439656376839,\n",
       "   0.0016582966782152653,\n",
       "   0.008280444890260696,\n",
       "   0.0031043177004903555,\n",
       "   -0.005064045544713736,\n",
       "   -0.012525956146419048,\n",
       "   -0.03624647483229637,\n",
       "   -0.013490536250174046,\n",
       "   0.004873846657574177,\n",
       "   0.02306840941309929,\n",
       "   -0.006144103594124317,\n",
       "   -0.024983983486890793,\n",
       "   0.009448809549212456,\n",
       "   -0.030703537166118622,\n",
       "   -0.007757397834211588,\n",
       "   0.004459484945982695,\n",
       "   -0.001626879908144474,\n",
       "   0.018816106021404266,\n",
       "   0.004534205887466669,\n",
       "   -0.02357107773423195,\n",
       "   0.014604558236896992,\n",
       "   -0.005970886908471584,\n",
       "   0.0020225613843649626,\n",
       "   -0.012512370012700558,\n",
       "   -0.04518582299351692,\n",
       "   -0.015365353785455227,\n",
       "   -0.04265889525413513,\n",
       "   -0.016751088201999664,\n",
       "   0.031246962025761604,\n",
       "   -0.016207663342356682,\n",
       "   0.021845702081918716,\n",
       "   -0.009163510985672474,\n",
       "   -0.012424063868820667,\n",
       "   -0.003569625783711672,\n",
       "   0.02357107773423195,\n",
       "   -0.04081124812364578,\n",
       "   -0.014142646454274654,\n",
       "   -0.012023287825286388,\n",
       "   -0.038311492651700974,\n",
       "   0.004693836905062199,\n",
       "   0.011479862034320831,\n",
       "   0.006412419956177473,\n",
       "   0.019848613068461418,\n",
       "   -0.009122754447162151,\n",
       "   0.007560406345874071,\n",
       "   -0.014346431009471416,\n",
       "   -0.011901016347110271,\n",
       "   0.0021431338973343372,\n",
       "   -0.024685099720954895,\n",
       "   -0.013415815308690071,\n",
       "   0.0024505089968442917,\n",
       "   0.024494901299476624,\n",
       "   0.0073362430557608604,\n",
       "   0.04934302717447281,\n",
       "   0.010488110594451427,\n",
       "   -0.00825327355414629,\n",
       "   0.005658417008817196,\n",
       "   0.004653080366551876,\n",
       "   0.026885973289608955,\n",
       "   -0.019508972764015198,\n",
       "   0.0099378926679492,\n",
       "   -0.005600678268820047,\n",
       "   0.00028444925555959344,\n",
       "   0.03298592194914818,\n",
       "   -0.0015266857808455825,\n",
       "   -0.0026746720541268587,\n",
       "   0.04469674080610275,\n",
       "   0.00953711662441492,\n",
       "   -0.010997571982443333,\n",
       "   -0.0069422596134245396,\n",
       "   0.007961182855069637,\n",
       "   0.013191652484238148,\n",
       "   -0.01498495601117611,\n",
       "   0.006884520873427391,\n",
       "   -0.02020183950662613,\n",
       "   0.010766616091132164,\n",
       "   -0.010447354055941105,\n",
       "   0.00649053743109107,\n",
       "   -0.016615232452750206,\n",
       "   0.008796699345111847,\n",
       "   -0.03035031072795391,\n",
       "   -0.003094128565862775,\n",
       "   0.051842786371707916,\n",
       "   -0.0038922845851629972,\n",
       "   0.021519646048545837,\n",
       "   -0.022035900503396988,\n",
       "   0.01938670128583908,\n",
       "   0.007139251567423344,\n",
       "   0.01179912406951189,\n",
       "   0.029426487162709236,\n",
       "   0.018625905737280846,\n",
       "   -0.016588060185313225,\n",
       "   0.02560892328619957,\n",
       "   0.00809024553745985,\n",
       "   0.024657929316163063,\n",
       "   0.006755457259714603,\n",
       "   -0.04529450833797455,\n",
       "   0.0014103588182479143,\n",
       "   0.025364382192492485,\n",
       "   -0.0008015524945221841,\n",
       "   0.03067636489868164,\n",
       "   0.00311960163526237,\n",
       "   0.01870742067694664,\n",
       "   0.011106257326900959,\n",
       "   0.005053856410086155,\n",
       "   0.03727898374199867,\n",
       "   -0.00935371033847332,\n",
       "   0.005607470870018005,\n",
       "   0.012512370012700558,\n",
       "   0.005152352154254913,\n",
       "   -0.013803006149828434,\n",
       "   -0.0012762006372213364,\n",
       "   0.0014536629896610975,\n",
       "   0.007798154838383198,\n",
       "   -0.010284326039254665,\n",
       "   -0.03170887380838394,\n",
       "   0.020758850499987602,\n",
       "   -0.003525472478941083,\n",
       "   -0.002995632588863373,\n",
       "   -0.01805531047284603,\n",
       "   0.004391556605696678,\n",
       "   -0.0006792817730456591,\n",
       "   -0.023312950506806374,\n",
       "   0.007370207458734512,\n",
       "   0.0038820954505354166,\n",
       "   0.02918194606900215,\n",
       "   0.019821442663669586,\n",
       "   -0.006456573493778706,\n",
       "   -0.016723917797207832,\n",
       "   0.014183403924107552,\n",
       "   -0.010053370147943497,\n",
       "   0.028747204691171646,\n",
       "   -0.019658414646983147,\n",
       "   0.021709846332669258,\n",
       "   -0.021832115948200226,\n",
       "   -0.021832115948200226,\n",
       "   0.021628331393003464,\n",
       "   -0.0030924302991479635,\n",
       "   -0.029589515179395676,\n",
       "   0.004934981931000948,\n",
       "   -0.02460358664393425,\n",
       "   0.00476855831220746,\n",
       "   -0.004106258507817984,\n",
       "   0.017960209399461746,\n",
       "   0.0025354193057864904,\n",
       "   0.010800580494105816,\n",
       "   0.019970884546637535,\n",
       "   -0.03287723660469055,\n",
       "   -0.043039292097091675,\n",
       "   0.013463364914059639,\n",
       "   0.018449293449521065,\n",
       "   0.009890343062579632,\n",
       "   -0.0259349774569273,\n",
       "   -0.0353226512670517,\n",
       "   0.009048033505678177,\n",
       "   0.004367781803011894,\n",
       "   -0.009659387171268463,\n",
       "   -0.016723917797207832,\n",
       "   0.0003243570390623063,\n",
       "   -0.016751088201999664,\n",
       "   -0.038664717227220535,\n",
       "   -0.027769038453698158,\n",
       "   0.01490344200283289,\n",
       "   0.03592041879892349,\n",
       "   0.0016752786468714476,\n",
       "   -0.016438618302345276,\n",
       "   -0.0025676852092146873,\n",
       "   0.006279959809035063,\n",
       "   0.008552157320082188,\n",
       "   -0.041843757033348083,\n",
       "   -0.027021829038858414,\n",
       "   0.01971275731921196,\n",
       "   0.013266373425722122,\n",
       "   -0.02089470811188221,\n",
       "   0.0007569746230728924,\n",
       "   -0.016547303646802902,\n",
       "   -0.017987381666898727,\n",
       "   0.016452204436063766,\n",
       "   0.006310527678579092,\n",
       "   0.015025712549686432,\n",
       "   -0.008898591622710228,\n",
       "   0.0031246962025761604,\n",
       "   0.028937403112649918,\n",
       "   -0.002178796334192157,\n",
       "   0.017756424844264984,\n",
       "   0.03469771146774292,\n",
       "   0.012655019760131836,\n",
       "   -0.0063614738173782825,\n",
       "   -0.03719747066497803,\n",
       "   -0.00024708875571377575,\n",
       "   0.011683646589517593,\n",
       "   0.05447839945554733,\n",
       "   0.04026782512664795,\n",
       "   -0.02983405627310276,\n",
       "   0.012301793321967125,\n",
       "   -0.022008730098605156,\n",
       "   -0.00278165889903903,\n",
       "   -0.01885686255991459,\n",
       "   -0.021016977727413177,\n",
       "   0.021804945543408394,\n",
       "   0.004870450124144554,\n",
       "   -0.01669674552977085,\n",
       "   0.004102861974388361,\n",
       "   0.012804461643099785,\n",
       "   -0.006711304187774658,\n",
       "   0.014101889915764332,\n",
       "   0.012430856935679913,\n",
       "   0.013252787292003632,\n",
       "   -0.006860746070742607,\n",
       "   -0.010399804450571537,\n",
       "   0.002475982066243887,\n",
       "   -0.0034915083087980747,\n",
       "   -0.025350796058773994,\n",
       "   0.012573505751788616,\n",
       "   0.022633668035268784,\n",
       "   0.00792042538523674,\n",
       "   -0.006521105300635099,\n",
       "   0.02579912170767784,\n",
       "   0.026002906262874603,\n",
       "   -0.006001454312354326,\n",
       "   -0.027673939242959023,\n",
       "   -0.02714410051703453,\n",
       "   0.014550215564668179,\n",
       "   0.009883549995720387,\n",
       "   0.008280444890260696,\n",
       "   -0.022688010707497597,\n",
       "   0.03988742455840111,\n",
       "   0.014074718579649925,\n",
       "   -0.00044025949318893254,\n",
       "   -0.0026865594554692507,\n",
       "   -0.004721008241176605,\n",
       "   0.013544878922402859,\n",
       "   0.0350237675011158,\n",
       "   -0.005593885667622089,\n",
       "   -0.01935953088104725,\n",
       "   0.0018000967102125287,\n",
       "   -0.019984470680356026,\n",
       "   0.014427945017814636,\n",
       "   0.02918194606900215,\n",
       "   -0.015270254574716091,\n",
       "   -0.027171270921826363,\n",
       "   -0.0002247998199891299,\n",
       "   0.010236776433885098,\n",
       "   -0.02732071280479431,\n",
       "   -0.0018289661966264248,\n",
       "   0.009808829054236412,\n",
       "   -0.017810767516493797,\n",
       "   0.007770983502268791,\n",
       "   -0.012838426046073437,\n",
       "   0.013585635460913181,\n",
       "   0.0006614506128244102,\n",
       "   -0.02176418900489807,\n",
       "   -0.017593398690223694,\n",
       "   0.003280930919572711,\n",
       "   -0.02325860783457756,\n",
       "   -0.011262492276728153,\n",
       "   -0.027877723798155785,\n",
       "   -0.002508247969672084,\n",
       "   -0.015052883885800838,\n",
       "   -0.02915477380156517,\n",
       "   0.008885005488991737,\n",
       "   -0.01737602800130844,\n",
       "   -0.04369140416383743,\n",
       "   -0.048935458064079285,\n",
       "   -0.020758850499987602,\n",
       "   0.01616690680384636,\n",
       "   0.013463364914059639,\n",
       "   0.026328962296247482,\n",
       "   -0.019237259402871132,\n",
       "   0.011140220798552036,\n",
       "   0.004884035792201757,\n",
       "   0.0011887431610375643,\n",
       "   -0.01540611032396555,\n",
       "   -0.013021831400692463,\n",
       "   -0.01918291673064232,\n",
       "   0.0003353953652549535,\n",
       "   -0.005077631212770939,\n",
       "   -0.004214943386614323,\n",
       "   -0.02158757485449314,\n",
       "   -0.008307616226375103,\n",
       "   0.0013305431930348277,\n",
       "   0.025228524580597878,\n",
       "   0.007064530625939369,\n",
       "   -0.005322172772139311,\n",
       "   0.016588060185313225,\n",
       "   0.0074381353333592415,\n",
       "   0.008022317662835121,\n",
       "   0.02339446358382702,\n",
       "   0.013938861899077892,\n",
       "   0.021451719105243683,\n",
       "   -0.03909945860505104,\n",
       "   0.006103346589952707,\n",
       "   -0.01649296097457409,\n",
       "   -0.015324597246944904,\n",
       "   -0.034099943935871124,\n",
       "   -0.010447354055941105,\n",
       "   0.0014562102733179927,\n",
       "   -0.0020633183885365725,\n",
       "   -0.0071868011727929115,\n",
       "   -0.007424549665302038,\n",
       "   -0.010719066485762596,\n",
       "   0.040512364357709885,\n",
       "   -0.022946137934923172,\n",
       "   0.010352253913879395,\n",
       "   0.024725856259465218,\n",
       "   0.009516737423837185,\n",
       "   0.0069422596134245396,\n",
       "   0.027660353109240532,\n",
       "   0.009971856139600277,\n",
       "   0.007621541619300842,\n",
       "   -0.019821442663669586,\n",
       "   0.005359533242881298,\n",
       "   -0.015963122248649597,\n",
       "   0.041191648691892624,\n",
       "   -0.0007599464734084904,\n",
       "   -0.019468216225504875,\n",
       "   0.02915477380156517,\n",
       "   0.009611837565898895,\n",
       "   -0.012267828918993473,\n",
       "   -0.027578840032219887,\n",
       "   -0.0028309067711234093,\n",
       "   -0.0005880033131688833,\n",
       "   0.01940028741955757,\n",
       "   0.0010486412793397903,\n",
       "   -0.015704995021224022,\n",
       "   -0.00044068405986763537,\n",
       "   0.012661811895668507,\n",
       "   -0.00263051874935627,\n",
       "   0.002049732720479369,\n",
       "   -0.021682674065232277,\n",
       "   -0.008389130234718323,\n",
       "   0.014047547243535519,\n",
       "   0.0009144830983132124,\n",
       "   0.012091215699911118,\n",
       "   -0.01001940667629242,\n",
       "   0.0227559395134449,\n",
       "   -0.037659380584955215,\n",
       "   -0.02662784606218338,\n",
       "   -0.00032966394792310894,\n",
       "   0.012709362432360649,\n",
       "   0.017226586118340492,\n",
       "   0.008620086126029491,\n",
       "   -0.0024420178961008787,\n",
       "   -0.013463364914059639,\n",
       "   -0.019318774342536926,\n",
       "   -0.020446380600333214,\n",
       "   -0.011778745800256729,\n",
       "   0.008144588209688663,\n",
       "   0.007526441942900419,\n",
       "   0.0367899015545845,\n",
       "   0.010719066485762596,\n",
       "   0.0495060570538044,\n",
       "   -0.007234350778162479,\n",
       "   0.01179912406951189,\n",
       "   0.011656475253403187,\n",
       "   -0.011941773816943169,\n",
       "   -0.0030584661290049553,\n",
       "   -0.006470159161835909,\n",
       "   0.0011310041882097721,\n",
       "   -0.02695390023291111,\n",
       "   -0.02441338635981083,\n",
       "   0.0057806880213320255,\n",
       "   0.0008643860346637666,\n",
       "   0.010868508368730545,\n",
       "   -0.003793788840994239,\n",
       "   0.0478757806122303,\n",
       "   0.023679763078689575,\n",
       "   -0.011839881539344788,\n",
       "   -0.024902470409870148,\n",
       "   -0.032279469072818756,\n",
       "   -0.0515710711479187,\n",
       "   -0.02037845365703106,\n",
       "   0.04279475286602974,\n",
       "   -0.006473555229604244,\n",
       "   0.007397378794848919,\n",
       "   -0.025867050513625145,\n",
       "   0.006432798225432634,\n",
       "   0.0068335747346282005,\n",
       "   0.005067442078143358,\n",
       "   0.014224160462617874,\n",
       "   -0.0029582721181213856,\n",
       "   0.03858320415019989,\n",
       "   -0.003831149311736226,\n",
       "   0.014713243581354618,\n",
       "   0.0038039779756218195,\n",
       "   0.021275104954838753,\n",
       "   -0.023367293179035187,\n",
       "   -0.004316835664212704,\n",
       "   -0.04143618792295456,\n",
       "   0.017049971967935562,\n",
       "   -0.005970886908471584,\n",
       "   -0.0075400276109576225,\n",
       "   0.011344005353748798,\n",
       "   -0.0012464820174500346,\n",
       "   0.010590002872049809,\n",
       "   0.00454439502209425,\n",
       "   -0.006653564982116222,\n",
       "   0.004520620219409466,\n",
       "   -0.012233864516019821,\n",
       "   0.007981561124324799,\n",
       "   -0.034099943935871124,\n",
       "   -0.0054342541843652725,\n",
       "   -0.02090829238295555,\n",
       "   -0.00035301424213685095,\n",
       "   0.006422609090805054,\n",
       "   -0.00010369660594733432,\n",
       "   -0.004428917076438665,\n",
       "   -0.024467729032039642,\n",
       "   0.018435707315802574,\n",
       "   -0.03521396592259407,\n",
       "   -0.035811737179756165,\n",
       "   0.016968458890914917,\n",
       "   0.013884519226849079,\n",
       "   0.04059387743473053,\n",
       "   -0.005835030693560839,\n",
       "   0.03198058530688286,\n",
       "   0.005390101112425327,\n",
       "   0.0198757853358984,\n",
       "   0.004829693585634232,\n",
       "   0.012593884021043777,\n",
       "   0.012512370012700558,\n",
       "   -0.000860140542499721,\n",
       "   0.023883547633886337,\n",
       "   0.0013746964978054166,\n",
       "   -0.017905868589878082,\n",
       "   -0.026559917256236076,\n",
       "   0.002971857786178589,\n",
       "   -0.00233842758461833,\n",
       "   -0.006572050973773003,\n",
       "   -0.03893643245100975,\n",
       "   0.017226586118340492,\n",
       "   -0.008355165831744671,\n",
       "   -0.009414845146238804,\n",
       "   -0.005366325844079256,\n",
       "   -0.01805531047284603,\n",
       "   -0.01703638769686222,\n",
       "   0.002385977189987898,\n",
       "   -0.010080541484057903,\n",
       "   0.01869383454322815,\n",
       "   -0.002036147052422166,\n",
       "   -0.013042210601270199,\n",
       "   -0.02577195130288601,\n",
       "   0.04431634396314621,\n",
       "   0.021519646048545837,\n",
       "   0.03173604607582092,\n",
       "   -0.01363318506628275,\n",
       "   0.0035764186177402735,\n",
       "   0.01922367513179779,\n",
       "   0.0053255693055689335,\n",
       "   -0.016588060185313225,\n",
       "   0.012682191096246243,\n",
       "   -0.006853953003883362,\n",
       "   0.05472293868660927,\n",
       "   -0.016900530084967613,\n",
       "   0.009978649206459522,\n",
       "   0.008830662816762924,\n",
       "   -0.018571563065052032,\n",
       "   0.019495386630296707,\n",
       "   -0.008511400781571865,\n",
       "   -0.008484229445457458,\n",
       "   0.027076171711087227,\n",
       "   -0.010229983367025852,\n",
       "   -0.01533818244934082,\n",
       "   -0.003151867538690567,\n",
       "   0.010494903661310673,\n",
       "   0.00409267283976078,\n",
       "   -0.02308199554681778,\n",
       "   -0.006096553988754749,\n",
       "   -0.02863851934671402,\n",
       "   -0.017090730369091034,\n",
       "   -0.00925861019641161,\n",
       "   0.02289179526269436,\n",
       "   -0.0012303490657359362,\n",
       "   -0.006758853793144226,\n",
       "   0.005050459876656532,\n",
       "   0.008783113211393356,\n",
       "   -0.012335756793618202,\n",
       "   -0.0161804910749197,\n",
       "   -0.0010724160820245743,\n",
       "   0.0064803482964634895,\n",
       "   -0.03146433085203171,\n",
       "   -0.00643959129229188,\n",
       "   -0.02022901177406311,\n",
       "   0.0013492234284058213,\n",
       "   0.011384762823581696,\n",
       "   -0.01851722039282322,\n",
       "   -0.014998541213572025,\n",
       "   0.003753031836822629,\n",
       "   0.008307616226375103,\n",
       "   -0.014794757589697838,\n",
       "   0.012695776298642159,\n",
       "   -0.008260066621005535,\n",
       "   -0.0073294504545629025,\n",
       "   -0.017606982961297035,\n",
       "   -0.009129547514021397,\n",
       "   -0.0030024254228919744,\n",
       "   -0.0017967002931982279,\n",
       "   0.04597378894686699,\n",
       "   -0.03255118429660797,\n",
       "   -0.023679763078689575,\n",
       "   -0.02240271307528019,\n",
       "   -0.012098008766770363,\n",
       "   0.02325860783457756,\n",
       "   -0.014930613338947296,\n",
       "   0.005026685073971748,\n",
       "   -0.004547791555523872,\n",
       "   -0.0036443467251956463,\n",
       "   0.006555069237947464,\n",
       "   0.002504851669073105,\n",
       "   -0.024399802088737488,\n",
       "   0.006116932258009911,\n",
       "   0.0061407070606946945,\n",
       "   -0.01238330639898777,\n",
       "   0.008728770539164543,\n",
       "   -0.022959724068641663,\n",
       "   0.022647254168987274,\n",
       "   -0.003671518061310053,\n",
       "   0.021533232182264328,\n",
       "   -0.010263947769999504,\n",
       "   -0.034453172236680984,\n",
       "   0.005807859357446432,\n",
       "   0.005407082848250866,\n",
       "   0.0038243564777076244,\n",
       "   -0.025867050513625145,\n",
       "   0.0012558221351355314,\n",
       "   -0.006558465771377087,\n",
       "   -0.019631244242191315,\n",
       "   -0.017063558101654053,\n",
       "   0.012580298818647861,\n",
       "   -0.0002776564215309918,\n",
       "   -0.014726828783750534,\n",
       "   0.0024267341941595078,\n",
       "   -0.004707422573119402,\n",
       "   0.008973312564194202,\n",
       "   0.008640464395284653,\n",
       "   -0.004809314850717783,\n",
       "   -0.018408536911010742,\n",
       "   -0.005128577351570129,\n",
       "   -0.01634351909160614,\n",
       "   -0.034806396812200546,\n",
       "   -0.004897621460258961,\n",
       "   -0.019278015941381454,\n",
       "   0.041001446545124054,\n",
       "   0.01338185090571642,\n",
       "   -0.015283839777112007,\n",
       "   -0.0038956808857619762,\n",
       "   -0.0043032499961555,\n",
       "   -0.037143126130104065,\n",
       "   -0.001344128861092031,\n",
       "   0.007044151891022921,\n",
       "   0.005597281735390425,\n",
       "   0.009632215835154057,\n",
       "   0.005624453071504831,\n",
       "   -0.02628820389509201,\n",
       "   0.03388257324695587,\n",
       "   -0.022823868319392204,\n",
       "   0.01918291673064232,\n",
       "   -0.005892769433557987,\n",
       "   0.03219795599579811,\n",
       "   -0.009265403263270855,\n",
       "   -0.008110624738037586,\n",
       "   -0.006527897901833057,\n",
       "   -0.010549246333539486,\n",
       "   -0.02154681831598282,\n",
       "   0.010630759410560131,\n",
       "   0.02210382930934429,\n",
       "   -0.005998058244585991,\n",
       "   -0.012695776298642159,\n",
       "   0.01885686255991459,\n",
       "   -0.011948566883802414,\n",
       "   5.1529892516555265e-05,\n",
       "   -0.01717224344611168,\n",
       "   0.006320716813206673,\n",
       "   0.031926244497299194,\n",
       "   0.013273165561258793,\n",
       "   0.02698107250034809,\n",
       "   -0.029209116473793983,\n",
       "   0.008620086126029491,\n",
       "   0.0005765404202975333,\n",
       "   0.00014721309707965702,\n",
       "   -0.026356132701039314,\n",
       "   0.011384762823581696,\n",
       "   0.020609408617019653,\n",
       "   0.013442986644804478,\n",
       "   0.03298592194914818,\n",
       "   -0.024060159921646118,\n",
       "   -0.004656476434320211,\n",
       "   0.009115961380302906,\n",
       "   -0.01069189514964819,\n",
       "   0.043066464364528656,\n",
       "   0.003797185141593218,\n",
       "   0.009000483900308609,\n",
       "   0.018761763349175453,\n",
       "   0.0313284769654274,\n",
       "   0.011710817925632,\n",
       "   -0.02124793455004692,\n",
       "   -0.01112663559615612,\n",
       "   -0.016954872757196426,\n",
       "   0.0028818529099226,\n",
       "   0.013375057838857174,\n",
       "   -0.025242110714316368,\n",
       "   -0.0084230937063694,\n",
       "   -0.028883060440421104,\n",
       "   0.01753905601799488,\n",
       "   -0.028067922219634056,\n",
       "   -0.012030079960823059,\n",
       "   -0.00850460771471262,\n",
       "   0.012437649071216583,\n",
       "   -0.020636580884456635,\n",
       "   0.028665691614151,\n",
       "   -0.00010486412793397903,\n",
       "   -0.018449293449521065,\n",
       "   0.016900530084967613,\n",
       "   0.0017016008496284485,\n",
       "   0.006249392405152321,\n",
       "   -0.009489566087722778,\n",
       "   -0.021682674065232277,\n",
       "   -0.00433381786569953,\n",
       "   -0.014699657447636127,\n",
       "   0.001367903663776815,\n",
       "   -0.0029667632188647985,\n",
       "   -0.0038651132490485907,\n",
       "   -0.01164289005100727,\n",
       "   -0.029698198661208153,\n",
       "   0.006758853793144226,\n",
       "   -0.01448228769004345,\n",
       "   -0.023217851296067238,\n",
       "   0.1725919097661972,\n",
       "   -0.03861037641763687,\n",
       "   0.00011876817006850615,\n",
       "   -0.017484713345766068,\n",
       "   0.01448228769004345,\n",
       "   -0.011466276831924915,\n",
       "   0.035811737179756165,\n",
       "   0.00036001933040097356,\n",
       "   -0.018313435837626457,\n",
       "   -0.005852012429386377,\n",
       "   0.024032989516854286,\n",
       "   -0.008144588209688663,\n",
       "   -0.03907228633761406,\n",
       "   -0.008015524595975876,\n",
       "   0.020106740295886993,\n",
       "   -0.004574962891638279,\n",
       "   -0.0323609821498394,\n",
       "   -0.041870929300785065,\n",
       "   0.010841337032616138,\n",
       "   0.00883745588362217,\n",
       "   0.025391552597284317,\n",
       "   0.005692381411790848,\n",
       "   -0.0009025956387631595,\n",
       "   -0.007662298623472452,\n",
       "   0.031437162309885025,\n",
       "   -0.00876952800899744,\n",
       "   0.01087530143558979,\n",
       "   0.013110138475894928,\n",
       "   0.0008847645367495716,\n",
       "   0.014264917001128197,\n",
       "   -0.021859288215637207,\n",
       "   0.003688500029966235,\n",
       "   0.009095583111047745,\n",
       "   0.001434982754290104,\n",
       "   -0.03214361518621445,\n",
       "   0.015012127347290516,\n",
       "   0.02929062955081463,\n",
       "   -0.00766909122467041,\n",
       "   0.010162055492401123,\n",
       "   -0.004863657522946596,\n",
       "   0.037170298397541046,\n",
       "   -0.012532749213278294,\n",
       "   0.010990778915584087,\n",
       "   0.0017967002931982279,\n",
       "   0.009822414256632328,\n",
       "   0.014631729573011398,\n",
       "   ...],\n",
       "  [-0.003869858104735613,\n",
       "   0.0011020530946552753,\n",
       "   0.025785069912672043,\n",
       "   -0.04516582190990448,\n",
       "   0.0037999418564140797,\n",
       "   0.003530764952301979,\n",
       "   0.010997808538377285,\n",
       "   -0.001035632798448205,\n",
       "   -0.02166001871228218,\n",
       "   -0.0546744167804718,\n",
       "   0.004887138027697802,\n",
       "   0.019436685368418694,\n",
       "   -0.001670121680945158,\n",
       "   -0.009256896562874317,\n",
       "   -0.011983625590801239,\n",
       "   0.003887337166815996,\n",
       "   0.02108670584857464,\n",
       "   0.0226668119430542,\n",
       "   0.00895625725388527,\n",
       "   0.0013563730753958225,\n",
       "   -0.01661207340657711,\n",
       "   0.018429894000291824,\n",
       "   -0.006732923910021782,\n",
       "   -0.037922512739896774,\n",
       "   -0.010326613672077656,\n",
       "   0.01835997775197029,\n",
       "   0.017926497384905815,\n",
       "   -0.024107083678245544,\n",
       "   -0.0020240722224116325,\n",
       "   0.0016814831178635359,\n",
       "   0.024890143424272537,\n",
       "   0.002067769644781947,\n",
       "   -0.026204567402601242,\n",
       "   -0.010564328171312809,\n",
       "   -0.011766885407269001,\n",
       "   -0.019520584493875504,\n",
       "   0.034874167293310165,\n",
       "   0.002592140808701515,\n",
       "   0.02491811104118824,\n",
       "   -0.008236120454967022,\n",
       "   0.028162218630313873,\n",
       "   0.009844192303717136,\n",
       "   0.009438678622245789,\n",
       "   -0.004132043570280075,\n",
       "   -0.004359270911663771,\n",
       "   0.012724736705422401,\n",
       "   -0.010578311048448086,\n",
       "   -0.006624553818255663,\n",
       "   0.0019943576771765947,\n",
       "   0.007124454248696566,\n",
       "   0.00788654014468193,\n",
       "   0.0226668119430542,\n",
       "   -0.030874965712428093,\n",
       "   -0.0038943288382142782,\n",
       "   0.0009412459912709892,\n",
       "   0.011829810217022896,\n",
       "   -0.012242315337061882,\n",
       "   0.018024379387497902,\n",
       "   0.010067923925817013,\n",
       "   -0.007194370497018099,\n",
       "   0.002777418587356806,\n",
       "   -0.008229129016399384,\n",
       "   -0.017255302518606186,\n",
       "   0.0031322429422289133,\n",
       "   -0.003133990801870823,\n",
       "   -0.005491912364959717,\n",
       "   -0.023393938317894936,\n",
       "   -0.010417504236102104,\n",
       "   0.027980437502264977,\n",
       "   -0.0006222536321729422,\n",
       "   0.010816026479005814,\n",
       "   0.023282073438167572,\n",
       "   -0.014780270867049694,\n",
       "   -0.004743809811770916,\n",
       "   0.03616062551736832,\n",
       "   -0.02950460836291313,\n",
       "   0.0033647140953689814,\n",
       "   -0.005656215362250805,\n",
       "   0.00872553326189518,\n",
       "   0.00569816492497921,\n",
       "   0.003667101263999939,\n",
       "   -0.019101088866591454,\n",
       "   -0.012794652953743935,\n",
       "   0.022610878571867943,\n",
       "   0.0034363779705017805,\n",
       "   0.003520277328789234,\n",
       "   0.0028088807594031096,\n",
       "   0.019646434113383293,\n",
       "   -0.011382346972823143,\n",
       "   0.008327011950314045,\n",
       "   0.015842996537685394,\n",
       "   -0.007900523021817207,\n",
       "   0.0394047349691391,\n",
       "   0.013242116197943687,\n",
       "   0.008403919637203217,\n",
       "   -0.011242514476180077,\n",
       "   -0.031490229070186615,\n",
       "   0.020974840968847275,\n",
       "   -0.01615062728524208,\n",
       "   -0.02115662209689617,\n",
       "   -0.011438279412686825,\n",
       "   0.004327808972448111,\n",
       "   -0.031154630705714226,\n",
       "   -0.024400731548666954,\n",
       "   -0.02157611958682537,\n",
       "   -0.021548153832554817,\n",
       "   0.015269683673977852,\n",
       "   -0.0037405132316052914,\n",
       "   -0.00836896151304245,\n",
       "   -0.015829013660550117,\n",
       "   -0.005491912364959717,\n",
       "   0.03456653654575348,\n",
       "   -0.005663206800818443,\n",
       "   -0.019184987992048264,\n",
       "   -0.011675994843244553,\n",
       "   0.0011719692265614867,\n",
       "   0.022736728191375732,\n",
       "   0.00631342688575387,\n",
       "   3.8617741665802896e-05,\n",
       "   -0.03613265976309776,\n",
       "   0.02916901186108589,\n",
       "   -0.00828506238758564,\n",
       "   0.020107880234718323,\n",
       "   -0.005956854671239853,\n",
       "   0.007488017901778221,\n",
       "   -0.00011028177686966956,\n",
       "   0.02304435893893242,\n",
       "   -0.013822419568896294,\n",
       "   -0.008327011950314045,\n",
       "   -0.014892136678099632,\n",
       "   -0.0014455161290243268,\n",
       "   0.016961654648184776,\n",
       "   0.007236320059746504,\n",
       "   0.005879946984350681,\n",
       "   -0.024680396541953087,\n",
       "   0.03288855031132698,\n",
       "   -0.006970638874918222,\n",
       "   -0.002509989310055971,\n",
       "   -0.009242912754416466,\n",
       "   -0.004432682879269123,\n",
       "   0.0009709603618830442,\n",
       "   0.003298293799161911,\n",
       "   -0.010711152106523514,\n",
       "   0.00417399313300848,\n",
       "   -0.023897334933280945,\n",
       "   0.02621855027973652,\n",
       "   -0.005533861927688122,\n",
       "   0.03685978800058365,\n",
       "   0.008194170892238617,\n",
       "   -0.01341690681874752,\n",
       "   -0.006935680750757456,\n",
       "   0.0022233331110328436,\n",
       "   0.01329105719923973,\n",
       "   0.018471842631697655,\n",
       "   0.02049941010773182,\n",
       "   0.0039467657916247845,\n",
       "   0.020946873351931572,\n",
       "   0.01640232466161251,\n",
       "   -0.013437881134450436,\n",
       "   4.8367764975409955e-05,\n",
       "   0.003950261510908604,\n",
       "   0.004303338006138802,\n",
       "   -0.02391131781041622,\n",
       "   0.005191273055970669,\n",
       "   0.0007935481262393296,\n",
       "   0.029644440859556198,\n",
       "   0.03968439996242523,\n",
       "   0.018206162378191948,\n",
       "   0.012591895647346973,\n",
       "   -0.021016789600253105,\n",
       "   -0.010669202543795109,\n",
       "   0.03135039657354355,\n",
       "   -0.005121356807649136,\n",
       "   0.03163006156682968,\n",
       "   0.026679998263716698,\n",
       "   -0.0035412523429840803,\n",
       "   0.015227734111249447,\n",
       "   -0.00361116835847497,\n",
       "   -0.011165606789290905,\n",
       "   -0.010767084546387196,\n",
       "   0.012899527326226234,\n",
       "   0.007502001244574785,\n",
       "   0.01644427515566349,\n",
       "   -0.006673495285212994,\n",
       "   -0.012556937523186207,\n",
       "   -0.005782064516097307,\n",
       "   0.02659609727561474,\n",
       "   -0.009914107620716095,\n",
       "   -0.004670397844165564,\n",
       "   -0.021212555468082428,\n",
       "   0.00542549230158329,\n",
       "   0.01970236748456955,\n",
       "   -0.006260990165174007,\n",
       "   0.008648625575006008,\n",
       "   -0.622421383857727,\n",
       "   -0.011431287974119186,\n",
       "   0.012948468327522278,\n",
       "   -0.021268488839268684,\n",
       "   -0.015717146918177605,\n",
       "   -0.003025620710104704,\n",
       "   0.001587970182299614,\n",
       "   -0.009319820441305637,\n",
       "   -0.0005903543788008392,\n",
       "   0.041390351951122284,\n",
       "   0.0010644731810316443,\n",
       "   0.002045047003775835,\n",
       "   0.011997608467936516,\n",
       "   -0.029812240973114967,\n",
       "   -0.006456755101680756,\n",
       "   -0.0351538322865963,\n",
       "   0.026973644271492958,\n",
       "   -0.02365962043404579,\n",
       "   0.0016989620635285974,\n",
       "   0.02428886480629444,\n",
       "   -0.016052745282649994,\n",
       "   0.003827908309176564,\n",
       "   0.00735517730936408,\n",
       "   -0.0013310284120962024,\n",
       "   -0.006449763663113117,\n",
       "   0.02788255549967289,\n",
       "   -0.0008363717934116721,\n",
       "   -0.019716350361704826,\n",
       "   0.022485028952360153,\n",
       "   -0.01644427515566349,\n",
       "   -0.03518179804086685,\n",
       "   0.0070685213431715965,\n",
       "   0.00949461106210947,\n",
       "   0.01736716739833355,\n",
       "   0.05212946981191635,\n",
       "   -0.031658027321100235,\n",
       "   -0.00834798626601696,\n",
       "   0.020932890474796295,\n",
       "   0.023435888811945915,\n",
       "   0.04905316233634949,\n",
       "   -0.0453336238861084,\n",
       "   -0.024736328050494194,\n",
       "   0.030567334964871407,\n",
       "   0.006697965785861015,\n",
       "   -0.011717944405972958,\n",
       "   0.030035972595214844,\n",
       "   0.007418102119117975,\n",
       "   0.01840192638337612,\n",
       "   -0.0004223371506668627,\n",
       "   -0.014123059809207916,\n",
       "   -0.01806632988154888,\n",
       "   -0.001500575104728341,\n",
       "   -0.015269683673977852,\n",
       "   -0.0023474341724067926,\n",
       "   0.03739114850759506,\n",
       "   0.0011815826874226332,\n",
       "   0.018919305875897408,\n",
       "   -0.016681989654898643,\n",
       "   0.01445865724235773,\n",
       "   -0.017772682011127472,\n",
       "   -0.002057282254099846,\n",
       "   -0.013060334138572216,\n",
       "   -0.015577315352857113,\n",
       "   -0.0006987243541516364,\n",
       "   -0.053947288542985916,\n",
       "   0.023058341816067696,\n",
       "   0.005743610672652721,\n",
       "   0.007166403811424971,\n",
       "   0.011109674349427223,\n",
       "   -0.03358771279454231,\n",
       "   0.008781466633081436,\n",
       "   0.028371967375278473,\n",
       "   0.013731529004871845,\n",
       "   -0.00872553326189518,\n",
       "   -0.003523773280903697,\n",
       "   0.02992410585284233,\n",
       "   0.02065322734415531,\n",
       "   -0.0014865918783470988,\n",
       "   -0.00692868884652853,\n",
       "   -0.003252848284319043,\n",
       "   -0.013829411938786507,\n",
       "   -0.009690375998616219,\n",
       "   -0.004132043570280075,\n",
       "   0.017045553773641586,\n",
       "   0.01808031275868416,\n",
       "   -0.008194170892238617,\n",
       "   0.00014573145017493516,\n",
       "   0.0014542556600645185,\n",
       "   -0.0121863828971982,\n",
       "   0.0021674002055078745,\n",
       "   -0.0011737170862033963,\n",
       "   0.029141046106815338,\n",
       "   -0.0347343347966671,\n",
       "   -0.038509808480739594,\n",
       "   0.0050654239021241665,\n",
       "   0.02066721022129059,\n",
       "   -0.008438877761363983,\n",
       "   0.012514987960457802,\n",
       "   0.02083500847220421,\n",
       "   -0.0237435195595026,\n",
       "   -0.015395533293485641,\n",
       "   -0.020009998232126236,\n",
       "   0.017129452899098396,\n",
       "   0.012263290584087372,\n",
       "   0.013535764068365097,\n",
       "   0.020639242604374886,\n",
       "   0.004579506814479828,\n",
       "   0.0016307939076796174,\n",
       "   0.026931695640087128,\n",
       "   -0.02124052122235298,\n",
       "   -0.007802640553563833,\n",
       "   0.004072614945471287,\n",
       "   -0.011543153785169125,\n",
       "   5.180894731893204e-05,\n",
       "   0.0002340005594305694,\n",
       "   -0.029952071607112885,\n",
       "   0.019800249487161636,\n",
       "   0.010032965801656246,\n",
       "   0.008711550384759903,\n",
       "   -0.023701569065451622,\n",
       "   0.02358970418572426,\n",
       "   -0.0008018506923690438,\n",
       "   0.016584107652306557,\n",
       "   -0.019143037497997284,\n",
       "   0.001688474672846496,\n",
       "   0.004649423062801361,\n",
       "   0.0034241427201777697,\n",
       "   0.02062525972723961,\n",
       "   0.0029557046946138144,\n",
       "   0.004943070933222771,\n",
       "   0.008012388832867146,\n",
       "   -0.008166204206645489,\n",
       "   0.01745106652379036,\n",
       "   -0.015563331544399261,\n",
       "   0.005974333733320236,\n",
       "   -0.009110072627663612,\n",
       "   0.009445670060813427,\n",
       "   -0.01799641363322735,\n",
       "   -0.0019209458259865642,\n",
       "   -0.03722335025668144,\n",
       "   -0.029644440859556198,\n",
       "   0.007208353374153376,\n",
       "   0.011766885407269001,\n",
       "   -0.040886957198381424,\n",
       "   -0.029448676854372025,\n",
       "   -0.010669202543795109,\n",
       "   -0.019129054620862007,\n",
       "   0.009284863248467445,\n",
       "   0.017590899020433426,\n",
       "   -0.002308980328962207,\n",
       "   0.02407911792397499,\n",
       "   0.0020695175044238567,\n",
       "   -0.00997703243046999,\n",
       "   0.016472240909934044,\n",
       "   -0.004796246998012066,\n",
       "   -0.0034363779705017805,\n",
       "   -0.01235418114811182,\n",
       "   -0.04180984944105148,\n",
       "   -0.015465449541807175,\n",
       "   -0.029532575979828835,\n",
       "   -0.00786556489765644,\n",
       "   0.03702758625149727,\n",
       "   -0.0034503613132983446,\n",
       "   -0.013137241825461388,\n",
       "   -0.005421996116638184,\n",
       "   -0.016640041023492813,\n",
       "   -0.005600282456725836,\n",
       "   0.02199561707675457,\n",
       "   -0.03669198974967003,\n",
       "   -0.009571518748998642,\n",
       "   -0.01389932818710804,\n",
       "   -0.04253697767853737,\n",
       "   0.004541052971035242,\n",
       "   0.006222535856068134,\n",
       "   0.006247006822377443,\n",
       "   0.027770688757300377,\n",
       "   -0.019632451236248016,\n",
       "   0.012934485450387001,\n",
       "   -0.0034888151567429304,\n",
       "   -0.032497018575668335,\n",
       "   0.012277273461222649,\n",
       "   -0.015269683673977852,\n",
       "   -0.02233121357858181,\n",
       "   -0.009312829002737999,\n",
       "   0.03490213304758072,\n",
       "   0.010655218735337257,\n",
       "   0.022806642577052116,\n",
       "   0.012214348651468754,\n",
       "   0.0006860520807094872,\n",
       "   0.006568620912730694,\n",
       "   -0.008914307691156864,\n",
       "   0.030539367347955704,\n",
       "   -0.017674800008535385,\n",
       "   0.004576011095196009,\n",
       "   0.003376949345692992,\n",
       "   -0.008047346957027912,\n",
       "   0.032245323061943054,\n",
       "   -0.005844988860189915,\n",
       "   -0.01740911789238453,\n",
       "   0.031042763963341713,\n",
       "   0.014822220429778099,\n",
       "   -0.003992211539298296,\n",
       "   0.00470535596832633,\n",
       "   -0.008459852077066898,\n",
       "   0.019548552110791206,\n",
       "   -0.005918400827795267,\n",
       "   0.017590899020433426,\n",
       "   -0.023268090561032295,\n",
       "   0.01833201013505459,\n",
       "   0.006138636730611324,\n",
       "   0.029280876740813255,\n",
       "   0.0016683738213032484,\n",
       "   -0.015968846157193184,\n",
       "   -0.012305240146815777,\n",
       "   -0.008375952951610088,\n",
       "   0.021184589713811874,\n",
       "   -0.004083102103322744,\n",
       "   0.035293664783239365,\n",
       "   -0.013228132389485836,\n",
       "   -0.0014577513793483377,\n",
       "   -0.001959399785846472,\n",
       "   0.00805433839559555,\n",
       "   0.028945280238986015,\n",
       "   0.021296454593539238,\n",
       "   -0.01827607862651348,\n",
       "   0.013486822135746479,\n",
       "   0.003929286729544401,\n",
       "   0.006135141011327505,\n",
       "   0.009263888001441956,\n",
       "   -0.03534960001707077,\n",
       "   -0.005942871328443289,\n",
       "   0.007250303402543068,\n",
       "   0.007676791865378618,\n",
       "   0.02312825806438923,\n",
       "   0.009543552063405514,\n",
       "   0.020597293972969055,\n",
       "   0.005764585454016924,\n",
       "   -0.0034800756257027388,\n",
       "   0.02695966139435768,\n",
       "   0.0014498858945444226,\n",
       "   0.012948468327522278,\n",
       "   0.002691771136596799,\n",
       "   0.004897625185549259,\n",
       "   -0.016388341784477234,\n",
       "   0.0010819522431120276,\n",
       "   -0.006474234163761139,\n",
       "   0.005701660644263029,\n",
       "   0.007809632457792759,\n",
       "   -0.025854986160993576,\n",
       "   0.014486622996628284,\n",
       "   0.0010863220086321235,\n",
       "   0.008942273445427418,\n",
       "   -0.0035010504070669413,\n",
       "   0.006428788881748915,\n",
       "   0.01577308028936386,\n",
       "   -0.02600880153477192,\n",
       "   0.007900523021817207,\n",
       "   0.012682787142693996,\n",
       "   0.029812240973114967,\n",
       "   0.02462446317076683,\n",
       "   0.017856581136584282,\n",
       "   -0.0035325128119438887,\n",
       "   0.017730731517076492,\n",
       "   -0.021687984466552734,\n",
       "   0.004324312787503004,\n",
       "   -0.0281762033700943,\n",
       "   -0.013493814505636692,\n",
       "   -0.011864768341183662,\n",
       "   -0.01682182215154171,\n",
       "   0.012368164025247097,\n",
       "   -0.01160607859492302,\n",
       "   -0.023701569065451622,\n",
       "   -0.0032248818315565586,\n",
       "   -0.014892136678099632,\n",
       "   -0.011235523037612438,\n",
       "   -0.006240014918148518,\n",
       "   0.012598888017237186,\n",
       "   0.004114564508199692,\n",
       "   0.010235722176730633,\n",
       "   0.030315635725855827,\n",
       "   -0.0359928272664547,\n",
       "   -0.03439873829483986,\n",
       "   0.014346791431307793,\n",
       "   0.0111236572265625,\n",
       "   0.020401528105139732,\n",
       "   -0.0241630170494318,\n",
       "   -0.0260507520288229,\n",
       "   0.0024750311858952045,\n",
       "   -0.016010794788599014,\n",
       "   -0.0011422549141570926,\n",
       "   -0.009774276055395603,\n",
       "   -0.00650220038369298,\n",
       "   -0.0025956365279853344,\n",
       "   -0.022149432450532913,\n",
       "   -0.020471444353461266,\n",
       "   0.025519389659166336,\n",
       "   0.038845404982566833,\n",
       "   -0.007229328621178865,\n",
       "   -0.010109873488545418,\n",
       "   0.001114288461394608,\n",
       "   0.0068517811596393585,\n",
       "   0.015451465733349323,\n",
       "   -0.0296724084764719,\n",
       "   -0.0021149630192667246,\n",
       "   0.04312427341938019,\n",
       "   -0.012501005083322525,\n",
       "   -0.034790270030498505,\n",
       "   -0.00809628888964653,\n",
       "   -0.01615062728524208,\n",
       "   -0.0004098833305761218,\n",
       "   0.025309640914201736,\n",
       "   0.007704758085310459,\n",
       "   -0.009536560624837875,\n",
       "   0.01770276576280594,\n",
       "   0.009403720498085022,\n",
       "   0.010997808538377285,\n",
       "   0.0008748256368562579,\n",
       "   -0.0038348999805748463,\n",
       "   0.03442670404911041,\n",
       "   -0.0005999678396619856,\n",
       "   0.020960858091711998,\n",
       "   -0.02888934686779976,\n",
       "   0.011822818778455257,\n",
       "   0.011836801655590534,\n",
       "   0.02838595025241375,\n",
       "   0.04217341169714928,\n",
       "   -0.024386748671531677,\n",
       "   0.01936676912009716,\n",
       "   -0.016626056283712387,\n",
       "   0.003133990801870823,\n",
       "   -0.006061729043722153,\n",
       "   -0.007079008501023054,\n",
       "   0.015563331544399261,\n",
       "   0.0064008221961557865,\n",
       "   -0.016472240909934044,\n",
       "   0.006299444008618593,\n",
       "   0.02942070923745632,\n",
       "   -0.008459852077066898,\n",
       "   0.012130449526011944,\n",
       "   0.00722233671694994,\n",
       "   0.014934086240828037,\n",
       "   -0.021855784580111504,\n",
       "   0.008124254643917084,\n",
       "   0.00847383588552475,\n",
       "   -0.014668405055999756,\n",
       "   -0.013277074322104454,\n",
       "   0.010480429045855999,\n",
       "   0.007809632457792759,\n",
       "   0.01377347856760025,\n",
       "   -0.016178593039512634,\n",
       "   0.04642431437969208,\n",
       "   0.04404716566205025,\n",
       "   0.011011791415512562,\n",
       "   -0.028148235753178596,\n",
       "   -0.006180586293339729,\n",
       "   0.02525370754301548,\n",
       "   0.001411431934684515,\n",
       "   -0.007746707648038864,\n",
       "   -0.015157817862927914,\n",
       "   0.032329220324754715,\n",
       "   0.008082305081188679,\n",
       "   -0.011592095717787743,\n",
       "   0.011235523037612438,\n",
       "   0.004324312787503004,\n",
       "   0.011116665787994862,\n",
       "   0.029364777728915215,\n",
       "   0.0011553640943020582,\n",
       "   -0.002780914306640625,\n",
       "   0.01719936914741993,\n",
       "   -0.010151823051273823,\n",
       "   0.008711550384759903,\n",
       "   0.02354775369167328,\n",
       "   -0.0026410820428282022,\n",
       "   -0.020848991349339485,\n",
       "   0.010158814489841461,\n",
       "   -0.011941676028072834,\n",
       "   -0.030399534851312637,\n",
       "   0.008683583699166775,\n",
       "   -0.007928489707410336,\n",
       "   0.005914905108511448,\n",
       "   0.009837199933826923,\n",
       "   -0.015759097412228584,\n",
       "   0.011717944405972958,\n",
       "   -0.025239724665880203,\n",
       "   -0.028148235753178596,\n",
       "   -0.011997608467936516,\n",
       "   0.008998206816613674,\n",
       "   -0.02079305797815323,\n",
       "   -0.02346385456621647,\n",
       "   -0.01368957944214344,\n",
       "   -0.005460449960082769,\n",
       "   -0.006914705969393253,\n",
       "   -0.03084699809551239,\n",
       "   0.008320020511746407,\n",
       "   -0.0053241136483848095,\n",
       "   -0.02666601352393627,\n",
       "   -0.049780286848545074,\n",
       "   -0.022694777697324753,\n",
       "   -0.003992211539298296,\n",
       "   0.02888934686779976,\n",
       "   0.012200365774333477,\n",
       "   -0.003536008531227708,\n",
       "   0.010802042670547962,\n",
       "   -0.00131704518571496,\n",
       "   0.00022285767772700638,\n",
       "   -0.002918998710811138,\n",
       "   -0.017171403393149376,\n",
       "   -0.0203316118568182,\n",
       "   -0.01158510334789753,\n",
       "   0.00010951707372441888,\n",
       "   -0.006278468761593103,\n",
       "   -0.01766081526875496,\n",
       "   -0.018961256369948387,\n",
       "   -0.027700772508978844,\n",
       "   0.00219187093898654,\n",
       "   0.004733322188258171,\n",
       "   -0.0038453873712569475,\n",
       "   -0.010627252981066704,\n",
       "   -0.000423211109591648,\n",
       "   0.00999800767749548,\n",
       "   0.014850187115371227,\n",
       "   0.016514191403985023,\n",
       "   0.01427687518298626,\n",
       "   -0.03341991454362869,\n",
       "   -0.00029124438879080117,\n",
       "   -0.0010566075798124075,\n",
       "   -0.022862575948238373,\n",
       "   -0.01778666488826275,\n",
       "   -0.004513086751103401,\n",
       "   0.014990019612014294,\n",
       "   0.012619862332940102,\n",
       "   -0.008110271766781807,\n",
       "   0.007963447831571102,\n",
       "   -0.01974431611597538,\n",
       "   0.022862575948238373,\n",
       "   -0.01848582550883293,\n",
       "   0.003873353824019432,\n",
       "   0.025785069912672043,\n",
       "   0.008844391442835331,\n",
       "   0.012242315337061882,\n",
       "   0.015185784548521042,\n",
       "   0.005656215362250805,\n",
       "   0.01835997775197029,\n",
       "   -0.008431886322796345,\n",
       "   -0.014486622996628284,\n",
       "   -0.02174391783773899,\n",
       "   0.0385657399892807,\n",
       "   -0.0010268932674080133,\n",
       "   -0.023016391322016716,\n",
       "   0.038593705743551254,\n",
       "   -0.012417105957865715,\n",
       "   -0.000459698581835255,\n",
       "   -0.027225343510508537,\n",
       "   -0.003873353824019432,\n",
       "   0.0012724737171083689,\n",
       "   0.027770688757300377,\n",
       "   0.017520982772111893,\n",
       "   -0.02340792305767536,\n",
       "   -0.0015643734950572252,\n",
       "   -0.009970040991902351,\n",
       "   -0.01329105719923973,\n",
       "   -0.02058331109583378,\n",
       "   -0.0279664546251297,\n",
       "   -0.005446467082947493,\n",
       "   -0.004967541433870792,\n",
       "   -0.00957851018756628,\n",
       "   0.004394229035824537,\n",
       "   -0.017842598259449005,\n",
       "   0.023365972563624382,\n",
       "   -0.030231736600399017,\n",
       "   -0.023016391322016716,\n",
       "   0.02691771276295185,\n",
       "   -0.003569218795746565,\n",
       "   0.0075719174928963184,\n",
       "   0.0009884394239634275,\n",
       "   0.0006410435889847577,\n",
       "   -0.03965643048286438,\n",
       "   -0.034706369042396545,\n",
       "   -0.011550145223736763,\n",
       "   -0.027574924752116203,\n",
       "   0.02279265969991684,\n",
       "   0.00894926581531763,\n",
       "   0.012004600837826729,\n",
       "   0.003549991874024272,\n",
       "   0.05800242722034454,\n",
       "   -0.007802640553563833,\n",
       "   0.007215345278382301,\n",
       "   0.014137042686343193,\n",
       "   -0.0030168811790645123,\n",
       "   -0.0005234970594756305,\n",
       "   0.019856182858347893,\n",
       "   0.008676592260599136,\n",
       "   -0.01903117261826992,\n",
       "   -0.033783476799726486,\n",
       "   -0.006344889290630817,\n",
       "   -0.002856074133887887,\n",
       "   0.004953558091074228,\n",
       "   0.01890532299876213,\n",
       "   0.005138835869729519,\n",
       "   0.03213345631957054,\n",
       "   -0.019436685368418694,\n",
       "   -0.004733322188258171,\n",
       "   -0.036216557025909424,\n",
       "   -0.03957253322005272,\n",
       "   -0.03129446133971214,\n",
       "   0.04256494343280792,\n",
       "   0.003604176687076688,\n",
       "   0.008396928198635578,\n",
       "   -0.02826010249555111,\n",
       "   0.011438279412686825,\n",
       "   0.03339194506406784,\n",
       "   0.004153018351644278,\n",
       "   0.02212146483361721,\n",
       "   -0.005005995277315378,\n",
       "   0.03518179804086685,\n",
       "   -0.00756492605432868,\n",
       "   0.007515984587371349,\n",
       "   0.0032004110980778933,\n",
       "   0.012703761458396912,\n",
       "   -0.014109076000750065,\n",
       "   0.0006497830618172884,\n",
       "   -0.025407522916793823,\n",
       "   0.008334003388881683,\n",
       "   0.0008228255319409072,\n",
       "   -0.0025064933579415083,\n",
       "   0.007935481145977974,\n",
       "   0.0058834427036345005,\n",
       "   0.021142639219760895,\n",
       "   0.008599684573709965,\n",
       "   -0.01919897086918354,\n",
       "   -0.01083000935614109,\n",
       "   -0.029728339985013008,\n",
       "   0.001062725204974413,\n",
       "   -0.020611276850104332,\n",
       "   -0.006526671350002289,\n",
       "   -0.04600481688976288,\n",
       "   -0.006995109375566244,\n",
       "   0.017087504267692566,\n",
       "   -0.011857776902616024,\n",
       "   0.003212646348401904,\n",
       "   -0.021562136709690094,\n",
       "   0.01789853163063526,\n",
       "   -0.03434280678629875,\n",
       "   -0.030455468222498894,\n",
       "   0.02633041702210903,\n",
       "   0.02145026996731758,\n",
       "   0.05772276222705841,\n",
       "   0.005170298274606466,\n",
       "   0.016681989654898643,\n",
       "   0.02487616054713726,\n",
       "   0.015968846157193184,\n",
       "   -0.005977829452604055,\n",
       "   0.006760890129953623,\n",
       "   0.010697168298065662,\n",
       "   -0.004908112809062004,\n",
       "   0.015143834985792637,\n",
       "   0.006732923910021782,\n",
       "   -0.025910919532179832,\n",
       "   -0.020960858091711998,\n",
       "   5.724383663618937e-05,\n",
       "   -0.010263688862323761,\n",
       "   -0.0030902931466698647,\n",
       "   -0.049192994832992554,\n",
       "   0.020597293972969055,\n",
       "   -0.016807839274406433,\n",
       "   0.001220036530867219,\n",
       "   -0.019296852871775627,\n",
       "   -0.001910458435304463,\n",
       "   -0.03392330929636955,\n",
       "   0.015171801671385765,\n",
       "   -0.003025620710104704,\n",
       "   0.003504546359181404,\n",
       "   -0.007928489707410336,\n",
       "   -0.012794652953743935,\n",
       "   -0.018345994874835014,\n",
       "   0.039796262979507446,\n",
       "   0.027197375893592834,\n",
       "   0.02195366658270359,\n",
       "   -0.027309242635965347,\n",
       "   -0.007774674333631992,\n",
       "   0.008529768325388432,\n",
       "   0.008599684573709965,\n",
       "   -0.017185386270284653,\n",
       "   0.022051548585295677,\n",
       "   -0.0010137839708477259,\n",
       "   0.04762687161564827,\n",
       "   -0.005226231180131435,\n",
       "   0.01787056401371956,\n",
       "   0.007522976025938988,\n",
       "   -0.01749301701784134,\n",
       "   0.0013074317248538136,\n",
       "   -0.007446068339049816,\n",
       "   0.0037055551074445248,\n",
       "   0.02950460836291313,\n",
       "   -0.017255302518606186,\n",
       "   -0.01273172814399004,\n",
       "   -0.01936676912009716,\n",
       "   0.00446764100342989,\n",
       "   0.002585149137303233,\n",
       "   -0.0111236572265625,\n",
       "   -0.0167379230260849,\n",
       "   -0.03680385276675224,\n",
       "   -0.021142639219760895,\n",
       "   0.01644427515566349,\n",
       "   0.03341991454362869,\n",
       "   -0.019422702491283417,\n",
       "   -0.008781466633081436,\n",
       "   -0.0029294861014932394,\n",
       "   0.0033454871736466885,\n",
       "   -0.0006497830618172884,\n",
       "   -7.668926264159381e-05,\n",
       "   -0.0067014615051448345,\n",
       "   0.014053143560886383,\n",
       "   -0.05061928182840347,\n",
       "   0.017255302518606186,\n",
       "   0.008389935828745365,\n",
       "   0.009536560624837875,\n",
       "   0.02700161188840866,\n",
       "   0.0015905920881778002,\n",
       "   -0.014283866621553898,\n",
       "   0.0018772482872009277,\n",
       "   0.02788255549967289,\n",
       "   -0.029616475105285645,\n",
       "   0.01150120422244072,\n",
       "   0.00836896151304245,\n",
       "   -0.01615062728524208,\n",
       "   -0.010906917043030262,\n",
       "   -0.0163184255361557,\n",
       "   -0.029141046106815338,\n",
       "   -0.0012523727491497993,\n",
       "   0.04278867319226265,\n",
       "   -0.024400731548666954,\n",
       "   -0.02462446317076683,\n",
       "   -0.01594087854027748,\n",
       "   -0.013997210189700127,\n",
       "   0.011298447847366333,\n",
       "   -0.006005796138197184,\n",
       "   0.007620858959853649,\n",
       "   -0.013780470006167889,\n",
       "   -0.0009595989831723273,\n",
       "   0.00373002584092319,\n",
       "   0.008173196576535702,\n",
       "   0.0019454164430499077,\n",
       "   0.0201218631118536,\n",
       "   0.001955903833732009,\n",
       "   -0.004306833725422621,\n",
       "   0.013228132389485836,\n",
       "   -0.020681193098425865,\n",
       "   0.019562534987926483,\n",
       "   -0.015073918737471104,\n",
       "   0.00941071193665266,\n",
       "   -0.01281562726944685,\n",
       "   -0.05548544600605965,\n",
       "   0.0037544965744018555,\n",
       "   0.0015468945493921638,\n",
       "   0.021967649459838867,\n",
       "   -0.04880146309733391,\n",
       "   0.0025694179348647594,\n",
       "   0.012668803334236145,\n",
       "   -0.002558930544182658,\n",
       "   -0.02028966322541237,\n",
       "   0.03146225959062576,\n",
       "   -0.002870057476684451,\n",
       "   -0.026400333270430565,\n",
       "   -0.001411431934684515,\n",
       "   0.0036845803260803223,\n",
       "   0.009739317931234837,\n",
       "   0.024274881929159164,\n",
       "   -0.01436077430844307,\n",
       "   -0.022596895694732666,\n",
       "   0.0005868585431016982,\n",
       "   0.0018964752089232206,\n",
       "   -0.036468256264925,\n",
       "   -0.011473237536847591,\n",
       "   -0.012319223023951054,\n",
       "   0.040467459708452225,\n",
       "   0.007844590581953526,\n",
       "   -0.02424691617488861,\n",
       "   -0.008005397394299507,\n",
       "   -0.007802640553563833,\n",
       "   -0.022512996569275856,\n",
       "   0.002142929472029209,\n",
       "   -0.010389537550508976,\n",
       "   0.006166602950543165,\n",
       "   -0.005516383331269026,\n",
       "   -0.014416706748306751,\n",
       "   -0.018290061503648758,\n",
       "   0.03367161005735397,\n",
       "   -0.018052347004413605,\n",
       "   0.010732126422226429,\n",
       "   -0.0029557046946138144,\n",
       "   0.01861167512834072,\n",
       "   -0.010109873488545418,\n",
       "   -0.0033454871736466885,\n",
       "   -0.00189123151358217,\n",
       "   -0.007900523021817207,\n",
       "   -0.019212953746318817,\n",
       "   -0.0015617517055943608,\n",
       "   0.027197375893592834,\n",
       "   0.005533861927688122,\n",
       "   0.005764585454016924,\n",
       "   0.03230125457048416,\n",
       "   0.009585502557456493,\n",
       "   0.01160607859492302,\n",
       "   -0.02633041702210903,\n",
       "   0.007313227746635675,\n",
       "   0.029560541734099388,\n",
       "   0.007767682895064354,\n",
       "   0.010116864927113056,\n",
       "   -0.04566922038793564,\n",
       "   0.010179789736866951,\n",
       "   0.002672544214874506,\n",
       "   0.0004302027227822691,\n",
       "   -0.021436287090182304,\n",
       "   0.013319023884832859,\n",
       "   -0.008445869199931622,\n",
       "   0.011717944405972958,\n",
       "   0.0102706803008914,\n",
       "   -0.02115662209689617,\n",
       "   -0.008375952951610088,\n",
       "   0.000326420966302976,\n",
       "   -0.024862177670001984,\n",
       "   0.04815823584794998,\n",
       "   0.0031304950825870037,\n",
       "   0.007502001244574785,\n",
       "   0.013787462376058102,\n",
       "   0.017437083646655083,\n",
       "   0.004586498718708754,\n",
       "   -0.016598090529441833,\n",
       "   0.0070685213431715965,\n",
       "   -0.013801445253193378,\n",
       "   0.001861517084762454,\n",
       "   0.015115868300199509,\n",
       "   -0.02342190593481064,\n",
       "   -0.009655417874455452,\n",
       "   -0.02609270252287388,\n",
       "   0.013829411938786507,\n",
       "   -0.0169476717710495,\n",
       "   0.004104077350348234,\n",
       "   0.010144831612706184,\n",
       "   -0.014430690556764603,\n",
       "   -0.017045553773641586,\n",
       "   0.012640837579965591,\n",
       "   0.007578908931463957,\n",
       "   -0.03109869733452797,\n",
       "   0.003981723915785551,\n",
       "   -0.009942074306309223,\n",
       "   0.008438877761363983,\n",
       "   -0.011452263221144676,\n",
       "   0.0041390350088477135,\n",
       "   -0.0020485427230596542,\n",
       "   -0.01799641363322735,\n",
       "   -0.003681084606796503,\n",
       "   -0.010319621302187443,\n",
       "   0.004376749973744154,\n",
       "   0.0028211160097271204,\n",
       "   -0.024610480293631554,\n",
       "   0.016220543533563614,\n",
       "   -0.01782861538231373,\n",
       "   -0.0067434110678732395,\n",
       "   0.18502606451511383,\n",
       "   -0.02634439989924431,\n",
       "   0.004146026913076639,\n",
       "   0.008299045264720917,\n",
       "   -0.003995707258582115,\n",
       "   -0.020177796483039856,\n",
       "   0.015563331544399261,\n",
       "   -0.005526870489120483,\n",
       "   -0.018639640882611275,\n",
       "   0.003967740572988987,\n",
       "   0.021226538345217705,\n",
       "   0.015353583730757236,\n",
       "   -0.036580123007297516,\n",
       "   -0.0113334059715271,\n",
       "   0.006697965785861015,\n",
       "   0.003529016859829426,\n",
       "   -0.028525782749056816,\n",
       "   -0.04130645468831062,\n",
       "   -0.004006194416433573,\n",
       "   0.030819032341241837,\n",
       "   0.02028966322541237,\n",
       "   0.0027232335414737463,\n",
       "   -0.0035936892963945866,\n",
       "   -0.013263090513646603,\n",
       "   0.027994420379400253,\n",
       "   0.0004420010664034635,\n",
       "   -0.01114463247358799,\n",
       "   9.012626833282411e-05,\n",
       "   0.011095690540969372,\n",
       "   0.016751905903220177,\n",
       "   -0.014416706748306751,\n",
       "   -0.010585302487015724,\n",
       "   0.020191779360175133,\n",
       "   -0.006718940567225218,\n",
       "   -0.014206958934664726,\n",
       "   0.004324312787503004,\n",
       "   -0.00460397731512785,\n",
       "   -0.011165606789290905,\n",
       "   0.011983625590801239,\n",
       "   0.0051737939938902855,\n",
       "   0.022596895694732666,\n",
       "   0.0006764386198483407,\n",
       "   -0.0011938180541619658,\n",
       "   -0.008327011950314045,\n",
       "   0.013249107636511326,\n",
       "   0.017884546890854836,\n",
       "   ...],\n",
       "  [-0.03543269634246826,\n",
       "   0.007381811272352934,\n",
       "   0.015142871998250484,\n",
       "   -0.03862922266125679,\n",
       "   -0.008587281219661236,\n",
       "   -0.003524984233081341,\n",
       "   0.00480494974181056,\n",
       "   -0.005025049671530724,\n",
       "   -0.005133406259119511,\n",
       "   -0.03310302272439003,\n",
       "   0.004053223878145218,\n",
       "   0.01567111164331436,\n",
       "   -0.008485697209835052,\n",
       "   0.00038411663263104856,\n",
       "   -0.014438551850616932,\n",
       "   0.004601780790835619,\n",
       "   0.02792898193001747,\n",
       "   0.008079358376562595,\n",
       "   -0.009223878383636475,\n",
       "   0.007463078945875168,\n",
       "   -0.02217252366244793,\n",
       "   0.011783809401094913,\n",
       "   0.0018860866548493505,\n",
       "   -0.04415542259812355,\n",
       "   0.0031965274829417467,\n",
       "   0.011878620833158493,\n",
       "   0.032236166298389435,\n",
       "   -0.030719170346856117,\n",
       "   -0.005658260080963373,\n",
       "   -0.008932668715715408,\n",
       "   0.010754418559372425,\n",
       "   4.745903788716532e-05,\n",
       "   -0.03407823294401169,\n",
       "   -0.026601610705256462,\n",
       "   -0.0218609981238842,\n",
       "   -0.0047372267581522465,\n",
       "   0.017797615379095078,\n",
       "   -0.010734101757407188,\n",
       "   0.019802216440439224,\n",
       "   0.0027275453321635723,\n",
       "   0.028931282460689545,\n",
       "   0.014709444716572762,\n",
       "   0.0023245932534337044,\n",
       "   -0.02294456586241722,\n",
       "   -0.004341046791523695,\n",
       "   0.004740613047033548,\n",
       "   0.00864823255687952,\n",
       "   -0.00964376050978899,\n",
       "   -0.006630085874348879,\n",
       "   0.014790711924433708,\n",
       "   0.026276540011167526,\n",
       "   0.020357545465230942,\n",
       "   -0.03610992431640625,\n",
       "   -0.015427308157086372,\n",
       "   0.006900977808982134,\n",
       "   0.013896767981350422,\n",
       "   -0.012393316254019737,\n",
       "   0.02049299143254757,\n",
       "   0.01874573715031147,\n",
       "   -0.024136491119861603,\n",
       "   -0.003213458228856325,\n",
       "   -1.882224387372844e-05,\n",
       "   -0.007673020474612713,\n",
       "   0.019192710518836975,\n",
       "   -0.012901239097118378,\n",
       "   -0.005915607791393995,\n",
       "   -0.023445716127753258,\n",
       "   -0.0006488713552244008,\n",
       "   0.02661515399813652,\n",
       "   0.007334405090659857,\n",
       "   0.02500334568321705,\n",
       "   0.04177157208323479,\n",
       "   -0.0048963758163154125,\n",
       "   0.0026073369663208723,\n",
       "   0.039333540946245193,\n",
       "   -0.03394278883934021,\n",
       "   -0.0008863253169693053,\n",
       "   0.0023872372694313526,\n",
       "   -0.00617295503616333,\n",
       "   0.009880791418254375,\n",
       "   -0.012955417856574059,\n",
       "   -0.03117968700826168,\n",
       "   -0.0035486873239278793,\n",
       "   0.021129587665200233,\n",
       "   -0.004168353043496609,\n",
       "   -0.014750078320503235,\n",
       "   -0.00967084988951683,\n",
       "   0.015129326842725277,\n",
       "   -0.01370037067681551,\n",
       "   0.029635602608323097,\n",
       "   0.003660430433228612,\n",
       "   0.02061489410698414,\n",
       "   0.030475368723273277,\n",
       "   -0.009176472201943398,\n",
       "   -0.0009650533320382237,\n",
       "   0.017282919958233833,\n",
       "   -0.03142349049448967,\n",
       "   0.01855611242353916,\n",
       "   0.005532972514629364,\n",
       "   -0.02073679491877556,\n",
       "   -0.0037891040556132793,\n",
       "   0.012014067731797695,\n",
       "   -0.021522382274270058,\n",
       "   -0.019571958109736443,\n",
       "   -0.017770525068044662,\n",
       "   -0.016619233414530754,\n",
       "   0.010761191137135029,\n",
       "   0.0029374868609011173,\n",
       "   0.0028596054762601852,\n",
       "   -0.01701202802360058,\n",
       "   -0.01622644066810608,\n",
       "   0.017797615379095078,\n",
       "   -0.0075849806889891624,\n",
       "   -0.015644023194909096,\n",
       "   -0.006447233259677887,\n",
       "   0.014844890683889389,\n",
       "   0.01549503207206726,\n",
       "   -0.012887694872915745,\n",
       "   0.00366381648927927,\n",
       "   -0.01457399781793356,\n",
       "   0.02168491668999195,\n",
       "   0.013402389362454414,\n",
       "   0.012887694872915745,\n",
       "   0.0032320821192115545,\n",
       "   0.014519819989800453,\n",
       "   -0.02349989488720894,\n",
       "   0.020357545465230942,\n",
       "   -0.014953247271478176,\n",
       "   -0.018285220488905907,\n",
       "   -0.01759444549679756,\n",
       "   -0.013619103468954563,\n",
       "   0.020628437399864197,\n",
       "   0.004679662175476551,\n",
       "   0.010713784955441952,\n",
       "   -0.0052485354244709015,\n",
       "   0.030719170346856117,\n",
       "   -0.003941481001675129,\n",
       "   0.012020839378237724,\n",
       "   -0.019382333382964134,\n",
       "   -0.013063774444162846,\n",
       "   0.01099144946783781,\n",
       "   -0.0015821795677766204,\n",
       "   -0.03247997164726257,\n",
       "   -0.0007457999745383859,\n",
       "   -0.026601610705256462,\n",
       "   0.03684133291244507,\n",
       "   0.019016629084944725,\n",
       "   0.04938364028930664,\n",
       "   0.004442631267011166,\n",
       "   -0.0011402866803109646,\n",
       "   0.0059596276842057705,\n",
       "   0.003497895086184144,\n",
       "   0.013463340699672699,\n",
       "   0.02415003627538681,\n",
       "   0.018664469942450523,\n",
       "   0.007395355962216854,\n",
       "   -0.0014907533768564463,\n",
       "   0.029960673302412033,\n",
       "   -0.014045758172869682,\n",
       "   -0.003538528922945261,\n",
       "   0.0029476452618837357,\n",
       "   0.005146950948983431,\n",
       "   -0.018610291182994843,\n",
       "   -0.004994574002921581,\n",
       "   0.01324662659317255,\n",
       "   0.03299466520547867,\n",
       "   0.027482010424137115,\n",
       "   0.004964098799973726,\n",
       "   0.013652964495122433,\n",
       "   -0.01843421161174774,\n",
       "   0.001821749727241695,\n",
       "   0.022145433351397514,\n",
       "   -0.008194487541913986,\n",
       "   0.01445209700614214,\n",
       "   -0.01544085331261158,\n",
       "   -0.013923857361078262,\n",
       "   0.02247050404548645,\n",
       "   -0.00035046672564931214,\n",
       "   -0.017811158671975136,\n",
       "   -0.01797369495034218,\n",
       "   0.013287260197103024,\n",
       "   0.022131890058517456,\n",
       "   0.006091687362641096,\n",
       "   0.01099144946783781,\n",
       "   0.0015356199583038688,\n",
       "   0.003734925761818886,\n",
       "   0.014194749295711517,\n",
       "   -0.02673705667257309,\n",
       "   0.00014994305092841387,\n",
       "   -0.018840549513697624,\n",
       "   -0.0029087045695632696,\n",
       "   0.009386413730680943,\n",
       "   -0.009027481079101562,\n",
       "   -0.0016363579779863358,\n",
       "   -0.6310703754425049,\n",
       "   -0.012366226874291897,\n",
       "   0.027062127366662025,\n",
       "   -0.04415542259812355,\n",
       "   -0.010490299202501774,\n",
       "   0.003975342493504286,\n",
       "   0.011221707798540592,\n",
       "   -0.008580509573221207,\n",
       "   -0.019964752718806267,\n",
       "   0.025558674708008766,\n",
       "   -0.018772827461361885,\n",
       "   -5.7511675549903885e-05,\n",
       "   -0.0008376493351534009,\n",
       "   -0.016023270785808563,\n",
       "   -0.001605882542207837,\n",
       "   -0.02979813702404499,\n",
       "   0.016808858141303062,\n",
       "   -0.028091516345739365,\n",
       "   0.00955572072416544,\n",
       "   0.030069028958678246,\n",
       "   -0.01223078090697527,\n",
       "   0.008715955540537834,\n",
       "   -0.009061343036592007,\n",
       "   -0.003613024251535535,\n",
       "   0.012752248905599117,\n",
       "   0.01605036109685898,\n",
       "   0.00773397134616971,\n",
       "   -0.00457807769998908,\n",
       "   0.025314873084425926,\n",
       "   0.006030736956745386,\n",
       "   -0.016808858141303062,\n",
       "   0.0019741265568882227,\n",
       "   0.016145173460245132,\n",
       "   0.003941481001675129,\n",
       "   0.03955025598406792,\n",
       "   3.4893237170763314e-05,\n",
       "   -0.017215196043252945,\n",
       "   0.021346302703022957,\n",
       "   0.02145465835928917,\n",
       "   0.04724359139800072,\n",
       "   -0.04158194735646248,\n",
       "   -0.00910197664052248,\n",
       "   0.029581423848867416,\n",
       "   0.01689012534916401,\n",
       "   0.0028663775883615017,\n",
       "   0.02817278541624546,\n",
       "   0.01741836592555046,\n",
       "   0.0012427178444340825,\n",
       "   -0.004903147928416729,\n",
       "   -0.008966530673205853,\n",
       "   -0.00292224925942719,\n",
       "   -0.005343347787857056,\n",
       "   -0.01445209700614214,\n",
       "   0.0014475799398496747,\n",
       "   0.020628437399864197,\n",
       "   -0.013801955617964268,\n",
       "   0.028524944558739662,\n",
       "   -0.021102499216794968,\n",
       "   0.014248928055167198,\n",
       "   0.000973518704995513,\n",
       "   0.011424876749515533,\n",
       "   -0.01874573715031147,\n",
       "   -0.01729646511375904,\n",
       "   -0.0004749078070744872,\n",
       "   -0.03383443132042885,\n",
       "   0.025314873084425926,\n",
       "   -0.0109304990619421,\n",
       "   0.02654743194580078,\n",
       "   -0.006982245482504368,\n",
       "   -0.027373652905225754,\n",
       "   0.0022517910692840815,\n",
       "   0.030800439417362213,\n",
       "   -0.008566964417696,\n",
       "   -0.007435990031808615,\n",
       "   -0.003738311817869544,\n",
       "   0.03123386576771736,\n",
       "   0.02600564807653427,\n",
       "   -0.016375431790947914,\n",
       "   -0.02012728713452816,\n",
       "   0.020777428522706032,\n",
       "   0.00805226992815733,\n",
       "   -0.00865500420331955,\n",
       "   0.0059393104165792465,\n",
       "   0.0019216412911191583,\n",
       "   0.008431518450379372,\n",
       "   0.0003618949849624187,\n",
       "   -0.007713654078543186,\n",
       "   0.008397657424211502,\n",
       "   -0.01651087775826454,\n",
       "   0.004632255993783474,\n",
       "   0.009860474616289139,\n",
       "   0.013592014089226723,\n",
       "   -0.0522821880877018,\n",
       "   -0.05970463156700134,\n",
       "   0.01621289551258087,\n",
       "   0.02187454141676426,\n",
       "   -0.003941481001675129,\n",
       "   0.015386674553155899,\n",
       "   0.018840549513697624,\n",
       "   -0.013680053874850273,\n",
       "   -0.03749147430062294,\n",
       "   -0.021942265331745148,\n",
       "   0.007077057845890522,\n",
       "   0.0031558936461806297,\n",
       "   -0.0003985077782999724,\n",
       "   0.029716869816184044,\n",
       "   0.007754288148134947,\n",
       "   0.005610853899270296,\n",
       "   0.03700387105345726,\n",
       "   -0.019436512142419815,\n",
       "   0.004012590274214745,\n",
       "   -0.018705103546380997,\n",
       "   -0.002807120094075799,\n",
       "   0.009345779195427895,\n",
       "   -0.005143565125763416,\n",
       "   -0.03413241356611252,\n",
       "   0.01015168335288763,\n",
       "   0.01321276556700468,\n",
       "   0.008194487541913986,\n",
       "   -0.014397918246686459,\n",
       "   0.012088562361896038,\n",
       "   -0.004645800683647394,\n",
       "   0.007618842180818319,\n",
       "   -0.0226465854793787,\n",
       "   0.0060408953577280045,\n",
       "   0.0023262863978743553,\n",
       "   -0.003254092065617442,\n",
       "   0.00825543887913227,\n",
       "   -0.011316520161926746,\n",
       "   0.01856965757906437,\n",
       "   0.01309763640165329,\n",
       "   0.013483657501637936,\n",
       "   0.02331027016043663,\n",
       "   -0.010178772732615471,\n",
       "   0.018827004358172417,\n",
       "   0.010077188722789288,\n",
       "   0.009068114683032036,\n",
       "   -0.02224024571478367,\n",
       "   0.010192317888140678,\n",
       "   -0.021319212391972542,\n",
       "   -0.010253268294036388,\n",
       "   0.00964376050978899,\n",
       "   0.016781769692897797,\n",
       "   -0.028687478974461555,\n",
       "   -0.027427831664681435,\n",
       "   -0.0071312361396849155,\n",
       "   -0.02836241014301777,\n",
       "   0.010497070848941803,\n",
       "   -0.0006213589222170413,\n",
       "   -0.010679922997951508,\n",
       "   0.005086000543087721,\n",
       "   0.019070807844400406,\n",
       "   -0.00047533109318464994,\n",
       "   -0.0068738884292542934,\n",
       "   -0.005234991200268269,\n",
       "   -0.0063117872923612595,\n",
       "   -0.023039378225803375,\n",
       "   -0.031694382429122925,\n",
       "   -0.01499388087540865,\n",
       "   -0.037220582365989685,\n",
       "   -0.012779337354004383,\n",
       "   0.01640252023935318,\n",
       "   -0.032344523817300797,\n",
       "   0.014966791495680809,\n",
       "   -0.00907488726079464,\n",
       "   -0.01976158283650875,\n",
       "   -0.012616802006959915,\n",
       "   0.01760799065232277,\n",
       "   -0.013720688410103321,\n",
       "   -0.019070807844400406,\n",
       "   0.001979205757379532,\n",
       "   -0.03719349205493927,\n",
       "   0.02433966100215912,\n",
       "   0.019124986603856087,\n",
       "   -0.004530671518296003,\n",
       "   0.02505752444267273,\n",
       "   -0.00011343609367031604,\n",
       "   0.008526330813765526,\n",
       "   -0.006850185338407755,\n",
       "   -0.03250705823302269,\n",
       "   -0.0019047105452045798,\n",
       "   -0.02211834490299225,\n",
       "   -0.0280644278973341,\n",
       "   -0.0055397446267306805,\n",
       "   0.018163319677114487,\n",
       "   0.014194749295711517,\n",
       "   0.024732453748583794,\n",
       "   0.0022230087779462337,\n",
       "   -0.0003540645120665431,\n",
       "   0.0004918385529890656,\n",
       "   0.007205731701105833,\n",
       "   0.030123207718133926,\n",
       "   -0.004950554110109806,\n",
       "   -0.002959496807307005,\n",
       "   -0.00975888967514038,\n",
       "   -0.005482180044054985,\n",
       "   0.03164020553231239,\n",
       "   0.0037146087270230055,\n",
       "   -0.0013612330658361316,\n",
       "   0.0375998318195343,\n",
       "   0.00915615539997816,\n",
       "   0.002370306523516774,\n",
       "   0.01138424314558506,\n",
       "   -0.01366650965064764,\n",
       "   0.022131890058517456,\n",
       "   -0.01465526595711708,\n",
       "   -0.002074018120765686,\n",
       "   -0.02271430753171444,\n",
       "   0.006247450597584248,\n",
       "   0.01133006438612938,\n",
       "   0.02390623278915882,\n",
       "   -0.017499633133411407,\n",
       "   -0.02498980239033699,\n",
       "   -0.014343739487230778,\n",
       "   -0.010537705384194851,\n",
       "   0.022876843810081482,\n",
       "   0.0003881376760546118,\n",
       "   0.03491799905896187,\n",
       "   0.0007356415153481066,\n",
       "   -0.0010217713424935937,\n",
       "   -0.012061473913490772,\n",
       "   0.002355068689212203,\n",
       "   0.03757274150848389,\n",
       "   0.009874018840491772,\n",
       "   -0.004164966754615307,\n",
       "   0.0226465854793787,\n",
       "   -0.003333666594699025,\n",
       "   -0.003653658088296652,\n",
       "   -0.00859405379742384,\n",
       "   -0.03432203829288483,\n",
       "   0.005387367680668831,\n",
       "   0.026642244309186935,\n",
       "   0.01951777935028076,\n",
       "   0.029364710673689842,\n",
       "   -0.0072937714867293835,\n",
       "   0.0005201976164244115,\n",
       "   0.008973303250968456,\n",
       "   0.008465380407869816,\n",
       "   0.04675598815083504,\n",
       "   0.00873627234250307,\n",
       "   0.015142871998250484,\n",
       "   0.012670980766415596,\n",
       "   -0.016239985823631287,\n",
       "   -0.008478924632072449,\n",
       "   0.004503582138568163,\n",
       "   0.006142479833215475,\n",
       "   0.024949168786406517,\n",
       "   0.0020774041768163443,\n",
       "   -0.011018538847565651,\n",
       "   0.005868201609700918,\n",
       "   -0.020276278257369995,\n",
       "   -0.004283482208848,\n",
       "   -0.01651087775826454,\n",
       "   0.018718648701906204,\n",
       "   0.01755381189286709,\n",
       "   -0.0017828090349212289,\n",
       "   -0.0028612983878701925,\n",
       "   0.0007525722612626851,\n",
       "   0.028335319831967354,\n",
       "   0.03676006570458412,\n",
       "   0.005387367680668831,\n",
       "   -0.006928067188709974,\n",
       "   0.005441546440124512,\n",
       "   -0.011682224459946156,\n",
       "   0.0038432825822383165,\n",
       "   -0.007828783243894577,\n",
       "   0.012596485204994678,\n",
       "   -0.021535927429795265,\n",
       "   -0.012122424319386482,\n",
       "   0.01327371597290039,\n",
       "   0.0007390276878140867,\n",
       "   0.002241632668301463,\n",
       "   0.007198959123343229,\n",
       "   -0.02860621176660061,\n",
       "   -0.013300805352628231,\n",
       "   0.002231474267318845,\n",
       "   0.0126777533441782,\n",
       "   0.011228480376303196,\n",
       "   0.0076323868706822395,\n",
       "   0.012278187088668346,\n",
       "   -0.03155893832445145,\n",
       "   -0.03380734100937843,\n",
       "   0.015156416222453117,\n",
       "   0.0013612330658361316,\n",
       "   -0.0009489691001363099,\n",
       "   -0.025572219863533974,\n",
       "   -0.02600564807653427,\n",
       "   -0.0037146087270230055,\n",
       "   -0.01689012534916401,\n",
       "   -0.0010615586070343852,\n",
       "   0.005688735283911228,\n",
       "   -0.0038297378923743963,\n",
       "   -0.0076933372765779495,\n",
       "   -0.008952985517680645,\n",
       "   -0.022267336025834084,\n",
       "   0.03708513826131821,\n",
       "   0.026533886790275574,\n",
       "   0.009257739409804344,\n",
       "   0.007273454684764147,\n",
       "   0.012698070146143436,\n",
       "   0.002195919631049037,\n",
       "   0.015765924006700516,\n",
       "   -0.02505752444267273,\n",
       "   0.0023381379432976246,\n",
       "   0.03280504047870636,\n",
       "   0.0018200566992163658,\n",
       "   -0.008038724772632122,\n",
       "   -0.0037823317106813192,\n",
       "   -0.019450057297945023,\n",
       "   -0.005367050878703594,\n",
       "   0.011790581047534943,\n",
       "   -0.002444801852107048,\n",
       "   0.0020062951371073723,\n",
       "   0.0038466686382889748,\n",
       "   0.027468465268611908,\n",
       "   0.011756720021367073,\n",
       "   -9.380699339089915e-05,\n",
       "   0.01687658205628395,\n",
       "   0.028741657733917236,\n",
       "   0.0004694053204730153,\n",
       "   0.01743191108107567,\n",
       "   -0.009657305665314198,\n",
       "   0.0023736925795674324,\n",
       "   0.00132144580129534,\n",
       "   0.04775828868150711,\n",
       "   0.03443039208650589,\n",
       "   -0.01855611242353916,\n",
       "   0.015820102766156197,\n",
       "   -0.023107102140784264,\n",
       "   -0.0003741697873920202,\n",
       "   -0.018068507313728333,\n",
       "   -0.007192187011241913,\n",
       "   0.014194749295711517,\n",
       "   0.01405930332839489,\n",
       "   -0.029473066329956055,\n",
       "   -0.011817670427262783,\n",
       "   0.030719170346856117,\n",
       "   -0.018596746027469635,\n",
       "   0.002520990092307329,\n",
       "   0.00859405379742384,\n",
       "   0.001059865579009056,\n",
       "   -0.007375039160251617,\n",
       "   0.0020367703400552273,\n",
       "   0.003971956204622984,\n",
       "   -0.007517257239669561,\n",
       "   -0.008018407970666885,\n",
       "   0.029635602608323097,\n",
       "   0.009332234971225262,\n",
       "   0.007618842180818319,\n",
       "   -0.01827167719602585,\n",
       "   0.02817278541624546,\n",
       "   0.04480556398630142,\n",
       "   -0.01318567618727684,\n",
       "   -0.03237161412835121,\n",
       "   -0.0008359563071280718,\n",
       "   0.022443415597081184,\n",
       "   0.005499111022800207,\n",
       "   0.004953940398991108,\n",
       "   -0.024556374177336693,\n",
       "   0.023405082523822784,\n",
       "   0.016267074272036552,\n",
       "   0.007916823029518127,\n",
       "   0.023215457797050476,\n",
       "   -0.013781638815999031,\n",
       "   0.021197311580181122,\n",
       "   0.031071331351995468,\n",
       "   0.004608552902936935,\n",
       "   0.0076323868706822395,\n",
       "   -0.008031952194869518,\n",
       "   0.00016084221715573221,\n",
       "   0.005333189386874437,\n",
       "   0.006416758056730032,\n",
       "   -0.014397918246686459,\n",
       "   -0.011986978352069855,\n",
       "   0.002851139986887574,\n",
       "   -0.016781769692897797,\n",
       "   -0.027590366080403328,\n",
       "   -0.008661776781082153,\n",
       "   0.007463078945875168,\n",
       "   -0.0057192109525203705,\n",
       "   -0.009386413730680943,\n",
       "   -0.0109304990619421,\n",
       "   -0.0018082051537930965,\n",
       "   -0.016497332602739334,\n",
       "   -0.021901631727814674,\n",
       "   -0.004253007005900145,\n",
       "   0.00916292704641819,\n",
       "   -0.020939964801073074,\n",
       "   -0.01369359903037548,\n",
       "   -0.010023009963333607,\n",
       "   0.005011504981666803,\n",
       "   -0.013036685064435005,\n",
       "   -0.023594707250595093,\n",
       "   -0.005133406259119511,\n",
       "   -0.023567618802189827,\n",
       "   -0.01729646511375904,\n",
       "   -0.05274270474910736,\n",
       "   -0.02847076579928398,\n",
       "   0.01183798722922802,\n",
       "   0.02205062098801136,\n",
       "   0.024055223912000656,\n",
       "   -0.0243938397616148,\n",
       "   0.0036333410535007715,\n",
       "   -0.006504797842353582,\n",
       "   -0.01120816357433796,\n",
       "   -0.027983160689473152,\n",
       "   -0.033915698528289795,\n",
       "   -0.024529285728931427,\n",
       "   0.0035588457249104977,\n",
       "   0.004253007005900145,\n",
       "   -0.014614632353186607,\n",
       "   -0.025748299434781075,\n",
       "   -0.02325609140098095,\n",
       "   -0.012183374725282192,\n",
       "   0.03015029802918434,\n",
       "   -0.004185284022241831,\n",
       "   0.0018928589997813106,\n",
       "   -0.01357846986502409,\n",
       "   0.004937009420245886,\n",
       "   0.01783824898302555,\n",
       "   -0.004733840469270945,\n",
       "   0.01457399781793356,\n",
       "   0.019192710518836975,\n",
       "   -0.03166729211807251,\n",
       "   0.0003559692413546145,\n",
       "   2.1599946194328368e-05,\n",
       "   0.0009828306501731277,\n",
       "   -0.03009611926972866,\n",
       "   0.008580509573221207,\n",
       "   0.01541376393288374,\n",
       "   0.008092903532087803,\n",
       "   0.01603681594133377,\n",
       "   -0.006975473370403051,\n",
       "   -0.020682616159319878,\n",
       "   0.01228495966643095,\n",
       "   -0.026831869035959244,\n",
       "   0.00545170484110713,\n",
       "   0.03069208189845085,\n",
       "   0.01180412620306015,\n",
       "   0.008939441293478012,\n",
       "   0.026317173615098,\n",
       "   -0.0026124161668121815,\n",
       "   -0.0019030174007639289,\n",
       "   -0.0334010049700737,\n",
       "   -0.0005858042859472334,\n",
       "   -0.006247450597584248,\n",
       "   0.01977512799203396,\n",
       "   0.007923595607280731,\n",
       "   -0.02198289893567562,\n",
       "   0.02420421503484249,\n",
       "   -0.0006662253872491419,\n",
       "   -0.01712038367986679,\n",
       "   -0.0368955135345459,\n",
       "   0.005367050878703594,\n",
       "   -0.014018669724464417,\n",
       "   0.03478255495429039,\n",
       "   0.0038466686382889748,\n",
       "   -0.00940673053264618,\n",
       "   -0.019206253811717033,\n",
       "   9.899203723762184e-05,\n",
       "   -0.011153984814882278,\n",
       "   -0.001791274407878518,\n",
       "   -0.014248928055167198,\n",
       "   -0.013354983180761337,\n",
       "   0.0066504026763141155,\n",
       "   0.011912482790648937,\n",
       "   0.007598524913191795,\n",
       "   -0.032696682959795,\n",
       "   0.021671373397111893,\n",
       "   -0.027265295386314392,\n",
       "   -0.03142349049448967,\n",
       "   0.045997489243745804,\n",
       "   -0.00994851440191269,\n",
       "   0.019192710518836975,\n",
       "   0.002070632064715028,\n",
       "   0.01591491512954235,\n",
       "   -0.02685895748436451,\n",
       "   -0.014763622544705868,\n",
       "   0.006023964378982782,\n",
       "   -0.039685700088739395,\n",
       "   0.00528578320518136,\n",
       "   0.01790597103536129,\n",
       "   0.007496940437704325,\n",
       "   0.024678274989128113,\n",
       "   0.050142139196395874,\n",
       "   -0.006982245482504368,\n",
       "   0.00802518054842949,\n",
       "   0.006582679692655802,\n",
       "   -0.006376124452799559,\n",
       "   0.018610291182994843,\n",
       "   0.014140570536255836,\n",
       "   0.016443153843283653,\n",
       "   -0.020289823412895203,\n",
       "   -0.008038724772632122,\n",
       "   0.0020875628106296062,\n",
       "   -0.015698200091719627,\n",
       "   0.01045643724501133,\n",
       "   0.017567357048392296,\n",
       "   0.026290085166692734,\n",
       "   0.026330718770623207,\n",
       "   -0.02516588196158409,\n",
       "   -0.005065683275461197,\n",
       "   -0.0082622105255723,\n",
       "   -0.028985461220145226,\n",
       "   -0.023472806438803673,\n",
       "   0.034159500151872635,\n",
       "   0.008370568044483662,\n",
       "   0.011377470567822456,\n",
       "   -0.02637135237455368,\n",
       "   0.004418928176164627,\n",
       "   0.006995790172368288,\n",
       "   0.024122947826981544,\n",
       "   0.013057001866400242,\n",
       "   -0.008384112268686295,\n",
       "   0.03329264745116234,\n",
       "   0.010530932806432247,\n",
       "   0.022565316408872604,\n",
       "   -0.004561146721243858,\n",
       "   0.01499388087540865,\n",
       "   -0.0257076658308506,\n",
       "   -0.011492599733173847,\n",
       "   -0.032290346920490265,\n",
       "   0.008702410385012627,\n",
       "   0.020560715347528458,\n",
       "   0.007056740578263998,\n",
       "   0.012623574584722519,\n",
       "   -0.0072125038132071495,\n",
       "   0.024894990026950836,\n",
       "   -0.0065691350027918816,\n",
       "   -0.007639158982783556,\n",
       "   -0.015278317965567112,\n",
       "   -0.0060984599404037,\n",
       "   0.006555590312927961,\n",
       "   -0.023161279037594795,\n",
       "   -0.006687650457024574,\n",
       "   -0.032777950167655945,\n",
       "   -0.02764454483985901,\n",
       "   0.007036423776298761,\n",
       "   -0.00264289160259068,\n",
       "   0.015481486916542053,\n",
       "   -0.0190572626888752,\n",
       "   0.011147212237119675,\n",
       "   -0.025937924161553383,\n",
       "   -0.026926681399345398,\n",
       "   0.024258393794298172,\n",
       "   0.005770002957433462,\n",
       "   0.028497856110334396,\n",
       "   0.009372868575155735,\n",
       "   0.004666117485612631,\n",
       "   0.007977774366736412,\n",
       "   0.01947714574635029,\n",
       "   -0.0007546886336058378,\n",
       "   -0.000741990574169904,\n",
       "   0.015088693238794804,\n",
       "   0.003458954393863678,\n",
       "   0.01195311639457941,\n",
       "   -0.010449664667248726,\n",
       "   -0.021942265331745148,\n",
       "   -0.022267336025834084,\n",
       "   0.012948645278811455,\n",
       "   -0.007510485127568245,\n",
       "   -0.010334535501897335,\n",
       "   -0.030664993450045586,\n",
       "   0.012772565707564354,\n",
       "   -0.010835686698555946,\n",
       "   0.0035893211606889963,\n",
       "   0.0006014652317389846,\n",
       "   0.024312570691108704,\n",
       "   -0.027007948607206345,\n",
       "   -0.0013036684831604362,\n",
       "   0.003694291925057769,\n",
       "   0.007415672764182091,\n",
       "   -0.012406861409544945,\n",
       "   -0.012488128617405891,\n",
       "   -0.022145433351397514,\n",
       "   0.03941480815410614,\n",
       "   0.009034253656864166,\n",
       "   0.042638424783945084,\n",
       "   -0.015847191214561462,\n",
       "   -0.014736533164978027,\n",
       "   0.017946606501936913,\n",
       "   -0.010984676890075207,\n",
       "   -0.01186507660895586,\n",
       "   0.012278187088668346,\n",
       "   0.0006662253872491419,\n",
       "   0.04588913172483444,\n",
       "   -0.006995790172368288,\n",
       "   0.010998222045600414,\n",
       "   0.028524944558739662,\n",
       "   0.00497087137773633,\n",
       "   0.0024143264163285494,\n",
       "   -0.0021688302513211966,\n",
       "   -0.0027952685486525297,\n",
       "   0.03510762378573418,\n",
       "   -0.024122947826981544,\n",
       "   -0.010260040871798992,\n",
       "   -0.002314434852451086,\n",
       "   -0.007077057845890522,\n",
       "   0.004280095919966698,\n",
       "   -0.014885524287819862,\n",
       "   -0.008390884846448898,\n",
       "   -0.029391799122095108,\n",
       "   -0.014790711924433708,\n",
       "   0.019422968849539757,\n",
       "   0.02211834490299225,\n",
       "   -0.0021231172140687704,\n",
       "   -0.01754026673734188,\n",
       "   0.002720773220062256,\n",
       "   -0.0015119168674573302,\n",
       "   0.0012664208188652992,\n",
       "   0.010070416145026684,\n",
       "   -0.004730454180389643,\n",
       "   0.00907488726079464,\n",
       "   -0.02871456928551197,\n",
       "   -0.010415803641080856,\n",
       "   0.007246365305036306,\n",
       "   0.00029120908584445715,\n",
       "   0.027251752093434334,\n",
       "   -0.015454397536814213,\n",
       "   -0.026709966361522675,\n",
       "   0.004225917626172304,\n",
       "   0.028931282460689545,\n",
       "   -0.010334535501897335,\n",
       "   0.015061603859066963,\n",
       "   0.007090602535754442,\n",
       "   0.006630085874348879,\n",
       "   -0.013233082368969917,\n",
       "   -0.01615871675312519,\n",
       "   -0.017161019146442413,\n",
       "   -0.012745476327836514,\n",
       "   0.014140570536255836,\n",
       "   -0.01928752101957798,\n",
       "   -0.010246495716273785,\n",
       "   -0.024800177663564682,\n",
       "   -0.010497070848941803,\n",
       "   0.01015168335288763,\n",
       "   0.010598655790090561,\n",
       "   0.0010734101524576545,\n",
       "   0.005553289316594601,\n",
       "   0.0031491213012486696,\n",
       "   0.0218609981238842,\n",
       "   -0.007686565164476633,\n",
       "   -0.028443677350878716,\n",
       "   0.018366487696766853,\n",
       "   0.012867378070950508,\n",
       "   -0.01580655761063099,\n",
       "   0.03749147430062294,\n",
       "   -0.015955548733472824,\n",
       "   -0.0024939009454101324,\n",
       "   -0.022849753499031067,\n",
       "   -0.002590406220406294,\n",
       "   -0.0007474930607713759,\n",
       "   -0.040715090930461884,\n",
       "   0.013463340699672699,\n",
       "   -0.013002824038267136,\n",
       "   -0.001828522072173655,\n",
       "   -0.023811420425772667,\n",
       "   -0.010381941683590412,\n",
       "   -9.645242244005203e-05,\n",
       "   -0.004967485088855028,\n",
       "   -0.004774474538862705,\n",
       "   0.027238206937909126,\n",
       "   -0.005319644697010517,\n",
       "   -0.0006641090731136501,\n",
       "   0.006528500933200121,\n",
       "   -0.0012274801265448332,\n",
       "   0.006735056173056364,\n",
       "   0.018948907032608986,\n",
       "   -0.017377732321619987,\n",
       "   -0.027373652905225754,\n",
       "   -0.004584849812090397,\n",
       "   -0.00811322033405304,\n",
       "   -0.03123386576771736,\n",
       "   -0.005109703168272972,\n",
       "   -0.02079097367823124,\n",
       "   0.03364480659365654,\n",
       "   0.009738572873175144,\n",
       "   -0.013991580344736576,\n",
       "   0.0030255268793553114,\n",
       "   0.003160972846671939,\n",
       "   -0.02895837277173996,\n",
       "   0.0033099635038524866,\n",
       "   -0.005499111022800207,\n",
       "   -0.0064912536181509495,\n",
       "   -0.00850601401180029,\n",
       "   0.005336575675755739,\n",
       "   -0.02018146589398384,\n",
       "   0.03310302272439003,\n",
       "   -0.0009921425953507423,\n",
       "   0.011262341402471066,\n",
       "   0.009298373013734818,\n",
       "   -0.0018336012726649642,\n",
       "   -0.021102499216794968,\n",
       "   -0.01093727070838213,\n",
       "   -0.0082622105255723,\n",
       "   -0.014560453593730927,\n",
       "   -0.014885524287819862,\n",
       "   -0.011174301616847515,\n",
       "   0.023703064769506454,\n",
       "   0.003575776470825076,\n",
       "   0.006816323846578598,\n",
       "   0.035053446888923645,\n",
       "   0.012474584393203259,\n",
       "   -0.0010463210055604577,\n",
       "   -0.013761322014033794,\n",
       "   0.023120645433664322,\n",
       "   0.029364710673689842,\n",
       "   -0.0068197101354599,\n",
       "   0.013747776858508587,\n",
       "   -0.028850015252828598,\n",
       "   0.0032998051028698683,\n",
       "   -0.0037958764005452394,\n",
       "   0.003494508797302842,\n",
       "   -0.01039548683911562,\n",
       "   -0.009298373013734818,\n",
       "   -0.004378294572234154,\n",
       "   0.005180812440812588,\n",
       "   0.015102238394320011,\n",
       "   -0.02836241014301777,\n",
       "   0.018041417002677917,\n",
       "   -0.003006902989000082,\n",
       "   -0.014831345528364182,\n",
       "   0.03792490437626839,\n",
       "   -0.0006082375766709447,\n",
       "   0.002717386931180954,\n",
       "   0.025084614753723145,\n",
       "   0.01831231079995632,\n",
       "   0.00982661359012127,\n",
       "   -0.03662462159991264,\n",
       "   -0.012332365848124027,\n",
       "   -0.008803995326161385,\n",
       "   -0.0016566748963668942,\n",
       "   0.03164020553231239,\n",
       "   -0.015291862189769745,\n",
       "   -0.003494508797302842,\n",
       "   -0.004987801890820265,\n",
       "   0.011729630641639233,\n",
       "   -0.02047944813966751,\n",
       "   -0.0028223576955497265,\n",
       "   0.009921425022184849,\n",
       "   0.021170223131775856,\n",
       "   -0.023526983335614204,\n",
       "   0.025761844590306282,\n",
       "   -0.007056740578263998,\n",
       "   -0.0334010049700737,\n",
       "   0.008038724772632122,\n",
       "   -0.0047643156722188,\n",
       "   0.007903278805315495,\n",
       "   -0.0003809421032201499,\n",
       "   0.00030411878833547235,\n",
       "   0.012698070146143436,\n",
       "   -0.007625614292919636,\n",
       "   0.009196789003908634,\n",
       "   -0.005383981857448816,\n",
       "   -0.0020096811931580305,\n",
       "   0.025274239480495453,\n",
       "   -0.025518041104078293,\n",
       "   0.029066728428006172,\n",
       "   -0.006498025730252266,\n",
       "   0.005190970841795206,\n",
       "   0.1956924945116043,\n",
       "   -0.032886307686567307,\n",
       "   0.006582679692655802,\n",
       "   0.02060134895145893,\n",
       "   -0.01087632030248642,\n",
       "   -0.004903147928416729,\n",
       "   0.004215759225189686,\n",
       "   -0.01561693288385868,\n",
       "   -0.02584311179816723,\n",
       "   0.0024583463091403246,\n",
       "   0.014181205071508884,\n",
       "   0.011167529039084911,\n",
       "   -0.03551396355032921,\n",
       "   0.0013129804283380508,\n",
       "   0.001339223119430244,\n",
       "   -0.0261410940438509,\n",
       "   -0.02211834490299225,\n",
       "   -0.02024918980896473,\n",
       "   -0.013422706164419651,\n",
       "   0.029662691056728363,\n",
       "   0.01855611242353916,\n",
       "   0.011871849186718464,\n",
       "   0.006464164238423109,\n",
       "   -0.025382595136761665,\n",
       "   0.027725812047719955,\n",
       "   -0.00019957133918069303,\n",
       "   -0.009366096928715706,\n",
       "   0.00027766445418819785,\n",
       "   -0.005373823456466198,\n",
       "   0.021373391151428223,\n",
       "   -0.016795314848423004,\n",
       "   -0.022849753499031067,\n",
       "   0.024610552936792374,\n",
       "   -0.011966661550104618,\n",
       "   -0.029364710673689842,\n",
       "   -0.004090471658855677,\n",
       "   0.0033522904850542545,\n",
       "   -0.003704450326040387,\n",
       "   0.009081659838557243,\n",
       "   0.019124986603856087,\n",
       "   0.029608512297272682,\n",
       "   -0.0032761020120233297,\n",
       "   0.0035859348718076944,\n",
       "   -0.0058004786260426044,\n",
       "   0.011702541261911392,\n",
       "   0.014668810181319714,\n",
       "   ...],\n",
       "  [-0.03221584111452103,\n",
       "   0.0018332282779738307,\n",
       "   0.024379555135965347,\n",
       "   -0.047289807349443436,\n",
       "   -0.0021189262624830008,\n",
       "   0.009448437951505184,\n",
       "   0.006404395215213299,\n",
       "   0.0022073565050959587,\n",
       "   -0.0052752080373466015,\n",
       "   -0.04228329285979271,\n",
       "   0.014679430983960629,\n",
       "   0.027998395264148712,\n",
       "   0.002950511174276471,\n",
       "   -0.00912872888147831,\n",
       "   -0.01463861670345068,\n",
       "   -0.0007835435681045055,\n",
       "   0.02231164649128914,\n",
       "   0.0033212383277714252,\n",
       "   -0.002357007935643196,\n",
       "   -0.02559037134051323,\n",
       "   -0.00889744982123375,\n",
       "   0.004996312316507101,\n",
       "   -0.009237566031515598,\n",
       "   -0.03210700303316116,\n",
       "   0.0015585841611027718,\n",
       "   0.017631642520427704,\n",
       "   0.03224305063486099,\n",
       "   -0.033413052558898926,\n",
       "   0.010774892754852772,\n",
       "   -0.005125556606799364,\n",
       "   0.020556645467877388,\n",
       "   0.0035678227432072163,\n",
       "   -0.011856463737785816,\n",
       "   -0.0342293307185173,\n",
       "   -0.007475762162357569,\n",
       "   -0.02088315784931183,\n",
       "   0.014543384313583374,\n",
       "   -0.015890246257185936,\n",
       "   0.013067279011011124,\n",
       "   -0.0009327696752734482,\n",
       "   0.006462214980274439,\n",
       "   0.015264431945979595,\n",
       "   -0.011768033728003502,\n",
       "   -0.01974036544561386,\n",
       "   0.005860208533704281,\n",
       "   0.010285125114023685,\n",
       "   0.008210414089262486,\n",
       "   -0.008128785528242588,\n",
       "   -0.0005752221331931651,\n",
       "   0.0010322538437321782,\n",
       "   0.02360408939421177,\n",
       "   0.023903392255306244,\n",
       "   -0.01900571398437023,\n",
       "   -0.01798536442220211,\n",
       "   0.00013115744513925165,\n",
       "   -0.0054758768528699875,\n",
       "   -0.019713155925273895,\n",
       "   0.020529435947537422,\n",
       "   0.018556760624051094,\n",
       "   -0.02126408740878105,\n",
       "   0.00841448362916708,\n",
       "   0.003409668570384383,\n",
       "   -0.022842228412628174,\n",
       "   0.01590385101735592,\n",
       "   -0.013264546170830727,\n",
       "   -0.0024131271056830883,\n",
       "   -0.014815477654337883,\n",
       "   0.006258144974708557,\n",
       "   0.017998969182372093,\n",
       "   0.017767690122127533,\n",
       "   0.03344026207923889,\n",
       "   0.014883501455187798,\n",
       "   -0.007795471698045731,\n",
       "   0.005319423042237759,\n",
       "   0.030719328671693802,\n",
       "   -0.031834911555051804,\n",
       "   0.014012802392244339,\n",
       "   0.000805651128757745,\n",
       "   -0.0050031146965920925,\n",
       "   0.004822853021323681,\n",
       "   -0.003413069760426879,\n",
       "   -0.029059559106826782,\n",
       "   -0.011006171815097332,\n",
       "   0.02900514006614685,\n",
       "   0.01136669609695673,\n",
       "   -0.011455126106739044,\n",
       "   -0.0014199867146089673,\n",
       "   -0.002447138773277402,\n",
       "   -0.007074424531310797,\n",
       "   0.01711466535925865,\n",
       "   0.006394191645085812,\n",
       "   0.010890532284975052,\n",
       "   0.02821607142686844,\n",
       "   0.0021903507877141237,\n",
       "   0.00959128700196743,\n",
       "   0.01642082817852497,\n",
       "   -0.02213478647172451,\n",
       "   0.024964556097984314,\n",
       "   -0.016121525317430496,\n",
       "   -0.012645534239709377,\n",
       "   -0.006360179744660854,\n",
       "   0.003761689178645611,\n",
       "   -0.014380128122866154,\n",
       "   -0.016774550080299377,\n",
       "   -0.0242979284375906,\n",
       "   -0.016257571056485176,\n",
       "   0.01844792254269123,\n",
       "   0.0008298844331875443,\n",
       "   0.01323733665049076,\n",
       "   -0.022991880774497986,\n",
       "   -0.03703189268708229,\n",
       "   0.026828395202755928,\n",
       "   0.005788784008473158,\n",
       "   -0.022787809371948242,\n",
       "   -0.0023297984153032303,\n",
       "   0.012006115168333054,\n",
       "   -0.011278265155851841,\n",
       "   -0.0018349288729950786,\n",
       "   0.015740593895316124,\n",
       "   -0.01825745776295662,\n",
       "   0.009836170822381973,\n",
       "   0.0127271618694067,\n",
       "   0.0059690456837415695,\n",
       "   0.003615439170971513,\n",
       "   0.004860265646129847,\n",
       "   -0.019155364483594894,\n",
       "   0.0082988440990448,\n",
       "   -0.016311990097165108,\n",
       "   -0.011257858015596867,\n",
       "   -0.012407452799379826,\n",
       "   -0.026066534221172333,\n",
       "   0.009904194623231888,\n",
       "   -0.012720359489321709,\n",
       "   0.0052752080373466015,\n",
       "   -0.022937461733818054,\n",
       "   0.03338584303855896,\n",
       "   0.007856693118810654,\n",
       "   0.000590102223213762,\n",
       "   -0.003812706796452403,\n",
       "   -0.026066534221172333,\n",
       "   0.003355249995365739,\n",
       "   -0.014679430983960629,\n",
       "   -0.021236877888441086,\n",
       "   -0.01686978153884411,\n",
       "   -0.004095003474503756,\n",
       "   0.033685144037008286,\n",
       "   0.00379910203628242,\n",
       "   0.04315399006009102,\n",
       "   -0.005503085907548666,\n",
       "   -0.00884983316063881,\n",
       "   -0.001613002852536738,\n",
       "   -0.007815878838300705,\n",
       "   0.00549968471750617,\n",
       "   0.01238024327903986,\n",
       "   0.015250827185809612,\n",
       "   0.015482106246054173,\n",
       "   0.03384840115904808,\n",
       "   0.019713155925273895,\n",
       "   -0.01605350151658058,\n",
       "   0.01983559876680374,\n",
       "   0.005958842113614082,\n",
       "   -0.0040711951442062855,\n",
       "   -0.013931174762547016,\n",
       "   -0.00947564747184515,\n",
       "   0.01719629392027855,\n",
       "   0.03722235560417175,\n",
       "   0.03779375180602074,\n",
       "   0.004986108746379614,\n",
       "   0.01385634858161211,\n",
       "   0.007108436431735754,\n",
       "   -0.013808732852339745,\n",
       "   0.021155251190066338,\n",
       "   0.007026808336377144,\n",
       "   0.031562816351652145,\n",
       "   -0.0011640489101409912,\n",
       "   0.0011291870614513755,\n",
       "   0.02535909228026867,\n",
       "   -0.0006292156758718193,\n",
       "   -0.03393002972006798,\n",
       "   -0.0027889558114111423,\n",
       "   0.021155251190066338,\n",
       "   0.011291869916021824,\n",
       "   0.010693265125155449,\n",
       "   0.0028603803366422653,\n",
       "   -0.013482220470905304,\n",
       "   0.0033773574978113174,\n",
       "   0.033086538314819336,\n",
       "   -0.020230133086442947,\n",
       "   0.005418057087808847,\n",
       "   -0.018842458724975586,\n",
       "   0.018284667283296585,\n",
       "   0.020719900727272034,\n",
       "   -0.0016325595788657665,\n",
       "   0.0038331137038767338,\n",
       "   -0.6351745128631592,\n",
       "   -0.010455183684825897,\n",
       "   0.015101175755262375,\n",
       "   -0.030855374410748482,\n",
       "   -0.02462443895637989,\n",
       "   -0.0011887074215337634,\n",
       "   -0.00529221398755908,\n",
       "   -0.002921601291745901,\n",
       "   -0.010842916555702686,\n",
       "   0.03572584316134453,\n",
       "   -0.011067393235862255,\n",
       "   -0.02157699503004551,\n",
       "   0.008795414119958878,\n",
       "   -0.024474788457155228,\n",
       "   -0.01821664348244667,\n",
       "   -0.02734537236392498,\n",
       "   0.021454554051160812,\n",
       "   -0.007945123128592968,\n",
       "   0.002203955315053463,\n",
       "   0.01854315586388111,\n",
       "   -0.016175944358110428,\n",
       "   0.017726875841617584,\n",
       "   -0.02236606553196907,\n",
       "   -0.0075029716826975346,\n",
       "   -0.015917455777525902,\n",
       "   0.02101920358836651,\n",
       "   0.002086615189909935,\n",
       "   -0.008380472660064697,\n",
       "   0.01159797515720129,\n",
       "   -0.004261660389602184,\n",
       "   -0.021128041669726372,\n",
       "   0.011189835146069527,\n",
       "   0.010992567054927349,\n",
       "   0.01696501486003399,\n",
       "   0.033358633518218994,\n",
       "   0.015182803384959698,\n",
       "   -0.0010892233112826943,\n",
       "   0.025644788518548012,\n",
       "   0.01316251140087843,\n",
       "   0.047425854951143265,\n",
       "   -0.03564421460032463,\n",
       "   -0.015876641497015953,\n",
       "   0.016380013898015022,\n",
       "   0.019345831125974655,\n",
       "   -0.00712204072624445,\n",
       "   0.02678758092224598,\n",
       "   0.003314435947686434,\n",
       "   0.02688281424343586,\n",
       "   -0.0048738704062998295,\n",
       "   -0.024774091318249702,\n",
       "   -0.00406779395416379,\n",
       "   -0.0061118947342038155,\n",
       "   -0.011135416105389595,\n",
       "   0.012876813299953938,\n",
       "   0.024229904636740685,\n",
       "   0.008523320779204369,\n",
       "   0.036406077444553375,\n",
       "   -0.029984677210450172,\n",
       "   0.0058976211585104465,\n",
       "   0.0043296837247908115,\n",
       "   0.009496054612100124,\n",
       "   -0.018175829201936722,\n",
       "   -0.005135760176926851,\n",
       "   0.000848590862005949,\n",
       "   -0.03243351727724075,\n",
       "   0.03205258399248123,\n",
       "   -0.000820531218778342,\n",
       "   0.02448839321732521,\n",
       "   -0.0010696665849536657,\n",
       "   -0.02323676459491253,\n",
       "   0.018883271142840385,\n",
       "   0.028896303847432137,\n",
       "   0.00712204072624445,\n",
       "   -0.01463861670345068,\n",
       "   -0.014352919533848763,\n",
       "   0.01877443492412567,\n",
       "   0.02954932674765587,\n",
       "   -0.012965243309736252,\n",
       "   -0.008400878868997097,\n",
       "   0.026134558022022247,\n",
       "   -0.00261719711124897,\n",
       "   0.0023076909128576517,\n",
       "   0.003243011422455311,\n",
       "   -0.01618954911828041,\n",
       "   0.024842113256454468,\n",
       "   -0.011842858977615833,\n",
       "   0.02586246468126774,\n",
       "   0.011278265155851841,\n",
       "   0.004445323720574379,\n",
       "   0.012196579948067665,\n",
       "   0.0001860012416727841,\n",
       "   0.014230477623641491,\n",
       "   -0.025508742779493332,\n",
       "   -0.04146701097488403,\n",
       "   0.004458928015083075,\n",
       "   0.023223159834742546,\n",
       "   0.004594974685460329,\n",
       "   0.01674734055995941,\n",
       "   0.018692806363105774,\n",
       "   -0.01070006750524044,\n",
       "   -0.02637944184243679,\n",
       "   -0.01769966632127762,\n",
       "   0.029358861967921257,\n",
       "   0.01599908247590065,\n",
       "   0.010053846053779125,\n",
       "   0.025141416117548943,\n",
       "   -0.002033896977081895,\n",
       "   -0.0020066876895725727,\n",
       "   0.05311260372400284,\n",
       "   -0.014720245264470577,\n",
       "   0.005972446873784065,\n",
       "   -0.015645362436771393,\n",
       "   -0.0027685489039868116,\n",
       "   0.0034385784529149532,\n",
       "   -0.011142218485474586,\n",
       "   -0.031154677271842957,\n",
       "   0.012604719959199429,\n",
       "   0.012298614718019962,\n",
       "   0.006999598816037178,\n",
       "   -0.02379455603659153,\n",
       "   0.005448667332530022,\n",
       "   0.0034929972607642412,\n",
       "   -0.0009710328304208815,\n",
       "   -0.013169313780963421,\n",
       "   0.0011427917052060366,\n",
       "   0.015386873856186867,\n",
       "   0.0032328080851584673,\n",
       "   0.015427687205374241,\n",
       "   -0.016380013898015022,\n",
       "   0.011448323726654053,\n",
       "   0.017998969182372093,\n",
       "   0.01136669609695673,\n",
       "   0.027427000924944878,\n",
       "   -0.008713786490261555,\n",
       "   0.009543671272695065,\n",
       "   0.01116262562572956,\n",
       "   0.029059559106826782,\n",
       "   -0.010727277025580406,\n",
       "   0.012312219478189945,\n",
       "   -0.021427344530820847,\n",
       "   -0.01982199400663376,\n",
       "   0.012795185670256615,\n",
       "   0.012142161838710308,\n",
       "   -0.014992338605225086,\n",
       "   -0.040460266172885895,\n",
       "   -0.009693321771919727,\n",
       "   -0.02369932271540165,\n",
       "   0.00046425912296399474,\n",
       "   0.007026808336377144,\n",
       "   -0.014842687174677849,\n",
       "   -0.006690092850476503,\n",
       "   0.0009395719971507788,\n",
       "   -0.00853012315928936,\n",
       "   0.011067393235862255,\n",
       "   0.0034079679753631353,\n",
       "   -0.005972446873784065,\n",
       "   -0.01254349946975708,\n",
       "   -0.024978160858154297,\n",
       "   -0.012625127099454403,\n",
       "   -0.01131907943636179,\n",
       "   -0.015141990035772324,\n",
       "   0.012162568047642708,\n",
       "   -0.017413968220353127,\n",
       "   0.026202579960227013,\n",
       "   -0.00740093644708395,\n",
       "   -0.03344026207923889,\n",
       "   0.009217158891260624,\n",
       "   0.01148233562707901,\n",
       "   -0.023508857935667038,\n",
       "   -0.01512838527560234,\n",
       "   -0.008883845061063766,\n",
       "   -0.03858282417058945,\n",
       "   0.0071152388118207455,\n",
       "   0.016393618658185005,\n",
       "   -0.007380529306828976,\n",
       "   0.01605350151658058,\n",
       "   -0.0024522405583411455,\n",
       "   -0.0008443393744528294,\n",
       "   -0.006217330694198608,\n",
       "   -0.01210814993828535,\n",
       "   0.0024012229405343533,\n",
       "   -0.0195907149463892,\n",
       "   -0.021617809310555458,\n",
       "   -0.009897392243146896,\n",
       "   0.01978117972612381,\n",
       "   0.029903048649430275,\n",
       "   0.02148176170885563,\n",
       "   0.011407509446144104,\n",
       "   -0.014216872863471508,\n",
       "   0.004622184205800295,\n",
       "   0.00784308835864067,\n",
       "   0.04658236354589462,\n",
       "   -0.004921486601233482,\n",
       "   -7.280620047822595e-05,\n",
       "   0.009774950332939625,\n",
       "   -0.008353263139724731,\n",
       "   0.032351888716220856,\n",
       "   -0.010373555123806,\n",
       "   6.48878631182015e-05,\n",
       "   0.03379398211836815,\n",
       "   -0.005615324713289738,\n",
       "   -0.008428088389337063,\n",
       "   0.022107576951384544,\n",
       "   -0.022515716031193733,\n",
       "   0.021100832149386406,\n",
       "   0.001799216610379517,\n",
       "   0.0008320101769641042,\n",
       "   -0.019903620705008507,\n",
       "   0.0106524508446455,\n",
       "   0.0022107576951384544,\n",
       "   0.01627117581665516,\n",
       "   -0.030773747712373734,\n",
       "   -0.0010399064049124718,\n",
       "   -0.02080152928829193,\n",
       "   -0.0130060575902462,\n",
       "   0.03942631185054779,\n",
       "   -0.006407795939594507,\n",
       "   0.049684226512908936,\n",
       "   -0.016706526279449463,\n",
       "   -0.005744569003582001,\n",
       "   0.011339486576616764,\n",
       "   -0.019998854026198387,\n",
       "   0.030746538192033768,\n",
       "   0.015849431976675987,\n",
       "   -0.009455240331590176,\n",
       "   -0.0020066876895725727,\n",
       "   -0.003714072983711958,\n",
       "   0.006003057584166527,\n",
       "   -0.0022549729328602552,\n",
       "   -0.048350971192121506,\n",
       "   0.006373784504830837,\n",
       "   0.016162339597940445,\n",
       "   0.0003941100731026381,\n",
       "   0.021645018830895424,\n",
       "   -0.0097953574731946,\n",
       "   -0.0029743195045739412,\n",
       "   0.007298901677131653,\n",
       "   0.007047215476632118,\n",
       "   0.04693608731031418,\n",
       "   0.014693035744130611,\n",
       "   0.003924945369362831,\n",
       "   0.013298558071255684,\n",
       "   -0.0031902934424579144,\n",
       "   0.002101920312270522,\n",
       "   0.0018638387555256486,\n",
       "   0.00201689125970006,\n",
       "   0.007026808336377144,\n",
       "   -0.0015738893998786807,\n",
       "   -0.04478655010461807,\n",
       "   0.009618496522307396,\n",
       "   -0.015101175755262375,\n",
       "   -0.0011291870614513755,\n",
       "   -0.011897278018295765,\n",
       "   0.003409668570384383,\n",
       "   0.010618438944220543,\n",
       "   -0.013455011881887913,\n",
       "   0.004788841120898724,\n",
       "   0.00379910203628242,\n",
       "   0.026120953261852264,\n",
       "   0.02813444286584854,\n",
       "   0.006060877349227667,\n",
       "   0.009965415112674236,\n",
       "   0.032025374472141266,\n",
       "   -0.010047043673694134,\n",
       "   -0.001022900571115315,\n",
       "   -0.01605350151658058,\n",
       "   -0.004975905176252127,\n",
       "   -0.01941385306417942,\n",
       "   -0.01835268922150135,\n",
       "   0.012080940417945385,\n",
       "   -0.019032923504710197,\n",
       "   -0.011734021827578545,\n",
       "   0.0015645362436771393,\n",
       "   -0.013366580940783024,\n",
       "   0.004901079926639795,\n",
       "   0.007863495498895645,\n",
       "   0.008121984079480171,\n",
       "   0.01256390567868948,\n",
       "   0.020869553089141846,\n",
       "   0.013971989043056965,\n",
       "   -0.03232467919588089,\n",
       "   -0.019441062584519386,\n",
       "   0.003561020363122225,\n",
       "   0.014094430953264236,\n",
       "   0.011693207547068596,\n",
       "   -0.01665210723876953,\n",
       "   -0.03292328491806984,\n",
       "   -0.00498950993642211,\n",
       "   -0.008360065519809723,\n",
       "   -0.01590385101735592,\n",
       "   -0.009863380342721939,\n",
       "   -0.0055098882876336575,\n",
       "   -0.0014046814758330584,\n",
       "   -0.02987583912909031,\n",
       "   -0.029440490528941154,\n",
       "   0.018597573041915894,\n",
       "   0.034936774522066116,\n",
       "   0.0033059329725801945,\n",
       "   -0.010067450813949108,\n",
       "   0.0032038979697972536,\n",
       "   0.011938091367483139,\n",
       "   0.018692806363105774,\n",
       "   -0.028433745726943016,\n",
       "   -0.012557103298604488,\n",
       "   0.03572584316134453,\n",
       "   0.010530008934438229,\n",
       "   -0.007176459766924381,\n",
       "   0.005832999013364315,\n",
       "   -0.013101289980113506,\n",
       "   -0.009774950332939625,\n",
       "   0.016148734837770462,\n",
       "   0.008550530299544334,\n",
       "   -0.008761403150856495,\n",
       "   0.001482057967223227,\n",
       "   0.018978504464030266,\n",
       "   0.005288812797516584,\n",
       "   -0.009053902700543404,\n",
       "   0.006870354525744915,\n",
       "   0.025971300899982452,\n",
       "   0.005622127093374729,\n",
       "   0.019250597804784775,\n",
       "   -0.00605067377910018,\n",
       "   0.007271692156791687,\n",
       "   0.006043871399015188,\n",
       "   0.05311260372400284,\n",
       "   0.032623980194330215,\n",
       "   -0.027835140004754066,\n",
       "   0.013577453792095184,\n",
       "   -0.0272501390427351,\n",
       "   0.0025933887809515,\n",
       "   -0.02070629596710205,\n",
       "   -0.013420999981462955,\n",
       "   0.02919560670852661,\n",
       "   -0.00043088517850264907,\n",
       "   -0.007890704087913036,\n",
       "   -0.002544071990996599,\n",
       "   0.0036664565559476614,\n",
       "   -0.016992224380373955,\n",
       "   -0.0055473013781011105,\n",
       "   0.01417605858296156,\n",
       "   0.007312506437301636,\n",
       "   -0.008326053619384766,\n",
       "   -0.010747683234512806,\n",
       "   -0.02199873887002468,\n",
       "   -0.015699781477451324,\n",
       "   0.00017707319057080895,\n",
       "   0.021740250289440155,\n",
       "   0.00841448362916708,\n",
       "   0.005265004467219114,\n",
       "   -0.002865482121706009,\n",
       "   0.021604204550385475,\n",
       "   0.04639190062880516,\n",
       "   -0.020298156887292862,\n",
       "   -0.0379025898873806,\n",
       "   -0.0020934175699949265,\n",
       "   0.019944434985518456,\n",
       "   0.010047043673694134,\n",
       "   0.005955440923571587,\n",
       "   -0.019481876865029335,\n",
       "   0.01812141016125679,\n",
       "   0.009305588901042938,\n",
       "   0.01463861670345068,\n",
       "   0.013931174762547016,\n",
       "   0.007645820267498493,\n",
       "   0.02209397219121456,\n",
       "   0.02678758092224598,\n",
       "   0.0019811789970844984,\n",
       "   0.00184173125308007,\n",
       "   0.003598433220759034,\n",
       "   -0.002749842358753085,\n",
       "   0.024093857035040855,\n",
       "   0.016298385336995125,\n",
       "   -0.009863380342721939,\n",
       "   -0.02066548354923725,\n",
       "   0.019060133025050163,\n",
       "   -0.016706526279449463,\n",
       "   -0.03695026412606239,\n",
       "   -0.0024097259156405926,\n",
       "   0.0048024458810687065,\n",
       "   -0.021563390269875526,\n",
       "   -0.005363638512790203,\n",
       "   -0.012257801368832588,\n",
       "   0.017604433000087738,\n",
       "   0.009904194623231888,\n",
       "   -0.029576536267995834,\n",
       "   0.0008668720838613808,\n",
       "   0.0027600459288805723,\n",
       "   -0.014747454784810543,\n",
       "   -0.0035916310735046864,\n",
       "   -0.01187687087804079,\n",
       "   -0.009659310802817345,\n",
       "   -0.0012609821278601885,\n",
       "   -0.03033839724957943,\n",
       "   0.0011053788475692272,\n",
       "   -0.011625184677541256,\n",
       "   -0.02632502280175686,\n",
       "   -0.05915307253599167,\n",
       "   -0.015210012905299664,\n",
       "   -0.0005297315656207502,\n",
       "   0.015223617665469646,\n",
       "   0.019441062584519386,\n",
       "   -0.031753282994031906,\n",
       "   0.0017286424990743399,\n",
       "   0.00011532077041920274,\n",
       "   -0.008366867899894714,\n",
       "   -0.016937805339694023,\n",
       "   -0.02973979339003563,\n",
       "   -0.04780678451061249,\n",
       "   0.004632387775927782,\n",
       "   -0.0021070220973342657,\n",
       "   -0.013332569040358067,\n",
       "   -0.013298558071255684,\n",
       "   -0.01927780732512474,\n",
       "   -0.005611923523247242,\n",
       "   0.025386299937963486,\n",
       "   0.011604777537286282,\n",
       "   0.011543556116521358,\n",
       "   -0.0025389702059328556,\n",
       "   -0.015563733875751495,\n",
       "   0.03205258399248123,\n",
       "   0.017958154901862144,\n",
       "   0.026120953261852264,\n",
       "   0.017903735861182213,\n",
       "   -0.025658393278717995,\n",
       "   0.01406722143292427,\n",
       "   0.00908111222088337,\n",
       "   -0.02730455808341503,\n",
       "   -0.01821664348244667,\n",
       "   0.00751657597720623,\n",
       "   -0.013774720951914787,\n",
       "   -0.0064758192747831345,\n",
       "   0.012815591879189014,\n",
       "   0.020964784547686577,\n",
       "   -0.0031171683222055435,\n",
       "   0.031834911555051804,\n",
       "   -0.021155251190066338,\n",
       "   0.010965358465909958,\n",
       "   0.0260121151804924,\n",
       "   0.012142161838710308,\n",
       "   0.009550473652780056,\n",
       "   0.005775179248303175,\n",
       "   0.01187687087804079,\n",
       "   -0.00636358093470335,\n",
       "   -0.031753282994031906,\n",
       "   0.002635903423652053,\n",
       "   -0.03240630775690079,\n",
       "   0.026637930423021317,\n",
       "   -0.0024692462757229805,\n",
       "   -0.0243387408554554,\n",
       "   0.027427000924944878,\n",
       "   -0.006458813790231943,\n",
       "   -0.004720817785710096,\n",
       "   -0.02661072090268135,\n",
       "   0.0060778832994401455,\n",
       "   -0.011101405136287212,\n",
       "   0.03425654023885727,\n",
       "   -0.0017601032741367817,\n",
       "   -0.026501882821321487,\n",
       "   -0.02217560075223446,\n",
       "   0.013747511431574821,\n",
       "   -0.0051085506565868855,\n",
       "   -0.009155938401818275,\n",
       "   -0.024325136095285416,\n",
       "   -0.023617694154381752,\n",
       "   0.0024879525881260633,\n",
       "   -0.0026818192563951015,\n",
       "   -0.00024679707712493837,\n",
       "   -0.024910137057304382,\n",
       "   0.034474216401576996,\n",
       "   -0.03749445080757141,\n",
       "   -0.020039668306708336,\n",
       "   0.03270560875535011,\n",
       "   0.0030848572496324778,\n",
       "   0.02522304467856884,\n",
       "   -0.012788383290171623,\n",
       "   0.009781752713024616,\n",
       "   -0.01609431579709053,\n",
       "   -0.022025948390364647,\n",
       "   -0.008768205530941486,\n",
       "   -0.03656933456659317,\n",
       "   0.022665368393063545,\n",
       "   0.020298156887292862,\n",
       "   0.030855374410748482,\n",
       "   0.012155766598880291,\n",
       "   0.0629623755812645,\n",
       "   -0.005067736841738224,\n",
       "   0.0007656874367967248,\n",
       "   -0.0034861948806792498,\n",
       "   0.005836400203406811,\n",
       "   -0.007734250742942095,\n",
       "   0.028460955247282982,\n",
       "   0.016475247219204903,\n",
       "   -0.025998510420322418,\n",
       "   -0.012972045689821243,\n",
       "   -0.00656084856018424,\n",
       "   0.0017618038691580296,\n",
       "   0.0034283751156181097,\n",
       "   0.03531770408153534,\n",
       "   0.029848629608750343,\n",
       "   0.03333142399787903,\n",
       "   -0.0022515717428177595,\n",
       "   -0.01473385002464056,\n",
       "   -0.007924715988337994,\n",
       "   -0.011679602786898613,\n",
       "   -0.023672113195061684,\n",
       "   0.03795700892806053,\n",
       "   0.014380128122866154,\n",
       "   0.0007941721705719829,\n",
       "   -0.026773976162075996,\n",
       "   0.022202810272574425,\n",
       "   0.03227026015520096,\n",
       "   0.002894392004236579,\n",
       "   0.008094774559140205,\n",
       "   -0.009094716981053352,\n",
       "   0.03735840320587158,\n",
       "   -0.002176746027544141,\n",
       "   0.006067679729312658,\n",
       "   0.004248056095093489,\n",
       "   0.0014854591572657228,\n",
       "   -0.02116885595023632,\n",
       "   -0.011448323726654053,\n",
       "   -0.046174224466085434,\n",
       "   0.01519640814512968,\n",
       "   0.016679316759109497,\n",
       "   -0.007339715491980314,\n",
       "   0.019930830225348473,\n",
       "   0.010944951325654984,\n",
       "   0.02412106655538082,\n",
       "   0.00795192550867796,\n",
       "   -0.018094200640916824,\n",
       "   -0.0008413633331656456,\n",
       "   -0.01572699099779129,\n",
       "   0.0016614694613963366,\n",
       "   -0.023168740794062614,\n",
       "   -0.019073737785220146,\n",
       "   -0.03809305652976036,\n",
       "   -0.020597459748387337,\n",
       "   -0.010353147983551025,\n",
       "   -0.011128613725304604,\n",
       "   0.019808389246463776,\n",
       "   -0.027971185743808746,\n",
       "   0.019495481625199318,\n",
       "   -0.007856693118810654,\n",
       "   -0.02402583509683609,\n",
       "   0.029903048649430275,\n",
       "   0.01233262661844492,\n",
       "   0.03485514596104622,\n",
       "   0.016978619620203972,\n",
       "   0.01540047861635685,\n",
       "   0.014216872863471508,\n",
       "   0.004169829189777374,\n",
       "   -0.02024373784661293,\n",
       "   -0.008632158860564232,\n",
       "   0.011727219447493553,\n",
       "   -0.003901137039065361,\n",
       "   0.0243387408554554,\n",
       "   0.015046756714582443,\n",
       "   -0.02886909432709217,\n",
       "   -0.018937690183520317,\n",
       "   0.021726645529270172,\n",
       "   0.010876927524805069,\n",
       "   -0.0035780263133347034,\n",
       "   -0.03218863159418106,\n",
       "   -0.0029726189095526934,\n",
       "   -0.0069349766708910465,\n",
       "   0.01503315195441246,\n",
       "   -0.011591172777116299,\n",
       "   0.012713557109236717,\n",
       "   -0.020733505487442017,\n",
       "   -0.0035474158357828856,\n",
       "   -0.009455240331590176,\n",
       "   -0.0004625585279427469,\n",
       "   -0.010353147983551025,\n",
       "   -0.01289041806012392,\n",
       "   -0.027726301923394203,\n",
       "   0.020461412146687508,\n",
       "   0.02250211127102375,\n",
       "   0.019359435886144638,\n",
       "   -0.021699437871575356,\n",
       "   -0.019305016845464706,\n",
       "   0.029358861967921257,\n",
       "   0.005526894237846136,\n",
       "   -0.013291755691170692,\n",
       "   0.016121525317430496,\n",
       "   -0.006887360475957394,\n",
       "   0.03997049853205681,\n",
       "   -0.008217216469347477,\n",
       "   0.031834911555051804,\n",
       "   0.01097896322607994,\n",
       "   -0.010938148945569992,\n",
       "   0.0066424766555428505,\n",
       "   -0.003873927751556039,\n",
       "   -0.009842973202466965,\n",
       "   0.0296037457883358,\n",
       "   -0.01742757298052311,\n",
       "   -0.010543613694608212,\n",
       "   -0.01457059383392334,\n",
       "   -0.010577625595033169,\n",
       "   0.0025015573482960463,\n",
       "   -0.027372581884264946,\n",
       "   -0.017550015822052956,\n",
       "   -0.0272501390427351,\n",
       "   -0.024474788457155228,\n",
       "   0.0005237795412540436,\n",
       "   0.02697804570198059,\n",
       "   -0.008434890769422054,\n",
       "   -0.006853349041193724,\n",
       "   -0.0030984620098024607,\n",
       "   0.012244196608662605,\n",
       "   -0.0005577911506406963,\n",
       "   0.007190064061433077,\n",
       "   -0.004737823735922575,\n",
       "   0.0074077388271689415,\n",
       "   -0.02684199996292591,\n",
       "   -0.011774835176765919,\n",
       "   0.008339658379554749,\n",
       "   0.002965816529467702,\n",
       "   0.020461412146687508,\n",
       "   -0.006407795939594507,\n",
       "   -0.02379455603659153,\n",
       "   -0.0013919270131736994,\n",
       "   0.029113978147506714,\n",
       "   -0.008244425989687443,\n",
       "   0.024787696078419685,\n",
       "   0.012400650419294834,\n",
       "   -0.005809191148728132,\n",
       "   -0.009482449851930141,\n",
       "   -0.0024981561582535505,\n",
       "   -0.016937805339694023,\n",
       "   -0.021223273128271103,\n",
       "   0.026488278061151505,\n",
       "   -0.010897334665060043,\n",
       "   -0.013638674281537533,\n",
       "   -0.008822623640298843,\n",
       "   -0.010067450813949108,\n",
       "   0.013516232371330261,\n",
       "   -0.014747454784810543,\n",
       "   0.013420999981462955,\n",
       "   0.0017686061328276992,\n",
       "   0.008598146960139275,\n",
       "   0.01692420057952404,\n",
       "   0.010713672265410423,\n",
       "   -0.021087227389216423,\n",
       "   0.011033381335437298,\n",
       "   0.006962186191231012,\n",
       "   -0.011924486607313156,\n",
       "   0.021699437871575356,\n",
       "   -0.02591688185930252,\n",
       "   -0.003152880584821105,\n",
       "   -0.008761403150856495,\n",
       "   -0.007870296947658062,\n",
       "   -0.011815649457275867,\n",
       "   -0.03137235343456268,\n",
       "   0.01696501486003399,\n",
       "   -0.024039439857006073,\n",
       "   -0.008360065519809723,\n",
       "   -0.03376677259802818,\n",
       "   -0.0007304003229364753,\n",
       "   -0.011611579917371273,\n",
       "   -0.011257858015596867,\n",
       "   0.006054074969142675,\n",
       "   0.025617580860853195,\n",
       "   -0.009428031742572784,\n",
       "   -0.014284895732998848,\n",
       "   0.008448495529592037,\n",
       "   -0.007747855503112078,\n",
       "   0.007856693118810654,\n",
       "   0.004697009921073914,\n",
       "   -0.02526385895907879,\n",
       "   -0.009094716981053352,\n",
       "   0.015645362436771393,\n",
       "   -0.00891785603016615,\n",
       "   -0.03991607949137688,\n",
       "   -0.012094545178115368,\n",
       "   -0.01546850148588419,\n",
       "   0.03371235355734825,\n",
       "   0.005693551152944565,\n",
       "   0.0037072706036269665,\n",
       "   -0.011359893716871738,\n",
       "   0.002763447118923068,\n",
       "   -0.05229632183909416,\n",
       "   0.011754428967833519,\n",
       "   0.0032770230900496244,\n",
       "   0.004622184205800295,\n",
       "   0.0003092935075983405,\n",
       "   0.005846603773534298,\n",
       "   -0.022678973153233528,\n",
       "   0.010802102275192738,\n",
       "   -0.003731078701093793,\n",
       "   0.008380472660064697,\n",
       "   -0.003992968704551458,\n",
       "   -0.01178163755685091,\n",
       "   -0.0033195377327501774,\n",
       "   -0.009611694142222404,\n",
       "   0.003771892748773098,\n",
       "   0.0019131556618958712,\n",
       "   -0.011414311826229095,\n",
       "   -0.008666169829666615,\n",
       "   0.03107305057346821,\n",
       "   -0.01021029893308878,\n",
       "   0.003351848805323243,\n",
       "   0.02730455808341503,\n",
       "   -0.010523206554353237,\n",
       "   7.806737994542345e-05,\n",
       "   -0.014298500493168831,\n",
       "   0.015931060537695885,\n",
       "   0.006509831175208092,\n",
       "   0.0037752939388155937,\n",
       "   0.02495095133781433,\n",
       "   -0.027263743802905083,\n",
       "   -0.002605292946100235,\n",
       "   0.0057343654334545135,\n",
       "   -0.007822681218385696,\n",
       "   -0.020134901627898216,\n",
       "   -0.007822681218385696,\n",
       "   -0.012121754698455334,\n",
       "   0.017413968220353127,\n",
       "   0.016992224380373955,\n",
       "   -0.02527746371924877,\n",
       "   0.0016325595788657665,\n",
       "   0.006866953335702419,\n",
       "   -0.02360408939421177,\n",
       "   0.026025719940662384,\n",
       "   -3.969798126490787e-05,\n",
       "   0.005724161863327026,\n",
       "   0.020692691206932068,\n",
       "   0.03425654023885727,\n",
       "   0.0015279736835509539,\n",
       "   -0.026705952361226082,\n",
       "   -0.01367268618196249,\n",
       "   -0.010591230355203152,\n",
       "   0.020325366407632828,\n",
       "   0.017454782500863075,\n",
       "   -0.013264546170830727,\n",
       "   0.0030712527222931385,\n",
       "   -0.014448151923716068,\n",
       "   0.03148118779063225,\n",
       "   -0.028188861906528473,\n",
       "   0.011400707066059113,\n",
       "   0.004598375875502825,\n",
       "   0.02813444286584854,\n",
       "   -0.02541350945830345,\n",
       "   0.029522117227315903,\n",
       "   -0.013380185700953007,\n",
       "   -0.03251514211297035,\n",
       "   0.0034215727355331182,\n",
       "   -0.005458870902657509,\n",
       "   0.00549968471750617,\n",
       "   -0.022243622690439224,\n",
       "   -0.006492825224995613,\n",
       "   0.00841448362916708,\n",
       "   -0.020175714045763016,\n",
       "   0.00016942057118285447,\n",
       "   -0.006492825224995613,\n",
       "   -0.010393962264060974,\n",
       "   0.019032923504710197,\n",
       "   -0.015563733875751495,\n",
       "   0.01599908247590065,\n",
       "   -0.009740938432514668,\n",
       "   0.000949775509070605,\n",
       "   0.1907917857170105,\n",
       "   -0.036542125046253204,\n",
       "   -0.006639075465500355,\n",
       "   0.0054962835274636745,\n",
       "   0.0018179230391979218,\n",
       "   -0.011359893716871738,\n",
       "   0.010659253224730492,\n",
       "   -0.002690321998670697,\n",
       "   -0.020543040707707405,\n",
       "   -0.005649336148053408,\n",
       "   0.007353320252150297,\n",
       "   0.010455183684825897,\n",
       "   -0.03564421460032463,\n",
       "   -0.004727620165795088,\n",
       "   0.0010348046198487282,\n",
       "   0.004843260161578655,\n",
       "   -0.018298272043466568,\n",
       "   -0.04312678053975105,\n",
       "   -0.025712812319397926,\n",
       "   0.03452863544225693,\n",
       "   0.007645820267498493,\n",
       "   -0.0006951132090762258,\n",
       "   0.012693149968981743,\n",
       "   -0.01669292151927948,\n",
       "   0.030011886730790138,\n",
       "   -0.011019776575267315,\n",
       "   0.004407910630106926,\n",
       "   0.006105092354118824,\n",
       "   -0.016665711998939514,\n",
       "   0.024229904636740685,\n",
       "   -0.014312105253338814,\n",
       "   -0.008679774589836597,\n",
       "   0.013611464761197567,\n",
       "   -0.0068057323805987835,\n",
       "   -0.023903392255306244,\n",
       "   -0.013917570002377033,\n",
       "   0.019658736884593964,\n",
       "   -0.02946770004928112,\n",
       "   0.008380472660064697,\n",
       "   0.004785439930856228,\n",
       "   0.020461412146687508,\n",
       "   -0.00624794140458107,\n",
       "   -0.006819337140768766,\n",
       "   -0.01802617870271206,\n",
       "   0.010121868923306465,\n",
       "   0.0201485063880682,\n",
       "   ...]],\n",
       " 'metadatas': [{'section_title': '5 Related work'},\n",
       "  {'section_title': '2 Problem Setting'},\n",
       "  {'section_title': 'Abstract'},\n",
       "  {'section_title': '1 Introduction'},\n",
       "  {'section_title': '4 Delayed Generalization: A Phase Diagram'},\n",
       "  {'section_title': '3 Why Generalization Occurs: Representations and Dynamics'},\n",
       "  {'section_title': '6 Conclusion'}],\n",
       " 'documents': ['Relatively few works have analyzed the phenomenon of grokking. [ 2] describe the circuit that\\ntransformers use to perform modular addition, track its formation over training, and broadly suggest\\nthat grokking is related to the phenomenon of “phase changes” in neural network training. [ 3,4]\\n9102103104105\\nOptimization Steps0.20.40.60.81.0AccuracyTrain Points: 1000 | Initialization Scale: 9.0\\ntrain\\nval(a)\\n1.00e-06 2.98e-05 8.86e-04 2.64e-02 7.85e-01\\nLast Layer Learning Rate1.00e-05\\n2.98e-04\\n8.86e-03\\n2.64e-01\\n7.85e+00Weight DecayMemorization\\nGrokking\\nComprehension\\nConfusion (b)\\nFigure 8: Left: Training curves for a run on MNIST, in the setting where we observe grokking. Right:\\nPhase diagram with the four phases of learning dynamics on MNIST.\\nprovided earlier speculative, informal conjectures on grokking [ 3,4]. Our work is related to the\\nfollowing broad research directions:\\nLearning mathematical structures [5] trains a neural network to learn arithmetic operation from\\npictures of digits, but they do not observe grokking due to their abundant training data. Beyond\\narithmetic relations, machine learning has been applied to learn other mathematical structures,\\nincluding geometry [6], knot theory [7] and group theory [8].\\nDouble descent Grokking is somewhat reminiscent of the phenomena of “epoch-wise” double\\ndescent [9], where generalization can improve after a period of overﬁtting. [ 10] ﬁnd that regularization\\ncan mitigate double descent, similar perhaps to how weight decay inﬂuences grokking.\\nRepresentation learning Representation learning lies at the core of machine learning [ 11–14].\\nRepresentation quality is usually measured by (perhaps vague) semantic meanings or performance on\\ndownstream tasks. In our study, the simplicity of arithmetic datasets allows us to deﬁne representation\\nquality and study evolution of representations in a quantitative way.\\nPhysics of learning Physics-inspired tools have proved to be useful in understanding deep learning\\nfrom a theoretical perspective. These tools include effective theories [ 15,16], conservation laws [ 17]\\nand free energy principle [ 18]. In addition, statistical physics has been identiﬁed as a powerful tool in\\nstudying generalization in neural networks [ 19–22]. Our work connects a low-level understanding of\\nmodels with their high-level performance. In a recent work, researchers at Anthropic [ 23], connect a\\nsudden decrease in loss during training with the emergence of induction heads within their models.\\nThey analogize their work to statistical physics , since it bridges a “microscopic”, mechanistic\\nunderstanding of networks with “macroscopic” facts about overall model performance.',\n",
       "  'Power et al. [ 1] observe grokking on a less common task – learning “algorithmic” binary operations.\\nGiven some binary operation ◦, a network is tasked with learning the map (a,b)↦→cwherec=a◦b.\\nThey use a decoder-only transformer to predict the second to last token in a tokenized equation of the\\nform “<lhs> <op> <rhs> <eq> <result> <eos>”. Each token is represented as a 256-dimensional\\nembedding vector. The embeddings are learnable and initialized randomly. After the transformer, a\\nﬁnal linear layer maps the output to class logits for each token.\\nToy Model We primarily study grokking in a simpler toy model, which still retains the key behaviors\\nfrom the setup of [ 1]. Although [ 1] treated this as a classiﬁcation task, we study both regression\\n(mean-squared error) and classiﬁcation (cross-entropy). The basic setup is as follows: our model\\ntakes as input the symbols a,band maps them to trainable embedding vectors Ea,Eb∈Rdin. It\\nthen sums Ea,Eband sends the resulting vector through a “decoder” MLP. The target output vector,\\ndenoted Yc∈Rdoutis a ﬁxed random vector (regression task) or a one-hot vector (classiﬁcation\\ntask). Our model architecture can therefore be compactly described as (a,b)↦→Dec(Ea+Eb),\\nwhere the embeddings E∗and the decoder are trainable. Despite its simplicity, this toy model can\\ngeneralize to all abelian groups (discussed in Appendix B). In sections 3-4.1, we consider only the\\nbinary operation of addition. We consider modular addition in Section 4.2 to generalize some of our\\nresults to a transformer architecture and study general non-abelian operations in Appendix H.\\nDataset In our toy setting, we are concerned with learning the addition operation. A data sample\\ncorresponding to i+jis denoted as (i,j)for simplicity. If i,j∈{0,...,p−1}, there are in total\\np(p+ 1)/2different samples since we consider i+jandj+ito be the same sample. A dataset D\\nis a set of non-repeating data samples. We denote the full dataset as D0and split it into a training\\ndatasetDand a validation dataset D′, i.e.,D⋃D′=D0,D⋂D′=∅. We deﬁne training data\\nfraction =|D|/|D0|where|·|denotes the cardinality of the set.\\n1Project code can be found at: https://github.com/ejmichaud/grokking-squared\\n2RQI: 0.0 — Accuracy - train: 1.0, validation: 0.1\\n02468101214161820(a) Memorization in toy addition\\nRQI: 0.6 — Accuracy - train: 1.0, validation: 0.9\\n02468101214161820 (b) Generalization in toy addition\\nAccuracy - train: 1.0, validation: 0.1\\n0246810\\n(c) Memorization in toy modular addition\\nAccuracy - train: 1.0, validation: 0.9\\n0246810 (d) Generalization in toy modular addition\\nFigure 2: Visualization of the learned set of embeddings ( p= 11 ) and the decoder function associated\\nwith it for the case of 2D embeddings. Axes refer to each dimension of the learned embeddings. The\\ndecoder is evaluated on a grid of points in embedding-space and the color at each point represents\\nthe highest probability class. For visualization purposes, the decoder is trained on inputs of the form\\n(Ei+Ej)/2. One can read off the output of the decoder when fed the operation i◦jfrom this ﬁgure\\nsimply by taking the midpoint between the respective embeddings of iandj.',\n",
       "  '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.',\n",
       "  'Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       "  'So far, we have (1) observed empirically that generalization on algorithmic datasets corresponds with\\nthe emergence of well-structured representations, (2) deﬁned a notion of representation quality in a\\ntoy setting and shown that it predicts generalization, and (3) developed an effective theory to describe\\n60.0 0.2 0.4 0.6 0.8 1.0\\ntraining data fraction0.00.10.20.30.4grokking rate λ3(a)\\n10−210−1100\\ngrokking rate λ3103104Steps to RQI >0.95tth= 1\\n/(2λ3η)\\nRuns that didn’t reach RQI >0.95 in 104steps\\nRuns that reached RQI >0.95 in 104steps (b)\\nFigure 5: Effective theory explains the dependence of grokking time on data size, for the addition\\ntask. (a) Dependence of λ3on training data fraction. Above the critical data fraction (around 0.4),\\nas data size becomes larger, λ3increases hence grokking time t∼1/λ3(predicted by our effective\\ntheory) decreases. (b) Comparing grokking steps (deﬁned as RQI>0.95) predicted by the effective\\ntheory with real neural network results. η= 10−3is the learning rate of the embeddings.\\nthe learning dynamics of the representations in the same toy setting. We now study how optimizer\\nhyperparameters affect high-level learning performance. In particular, we develop phase diagrams for\\nhow learning performance depends on the representation learning rate, decoder learning rate and the\\ndecoder weight decay. These parameters are of interest since they most explicitly regulate a kind of\\ncompetition between the encoder and decoder, as we elaborate below.',\n",
       "  'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.',\n",
       "  'We have shown how, in both toy models and general settings, that representation enables generalization\\nwhen it reﬂects structure in the data. We developed an effective theory of representation learning\\ndynamics (in a toy setting) which predicts the critical dependence of learning on the training data\\nfraction. We then presented four learning phases (comprehension, grokking, memorization and\\nconfusion) which depend on the decoder capacity and learning speed (given by, among other things,\\nlearning rate and weight decay) in decoder-only architectures. While we have mostly focused on a\\ntoy model, we ﬁnd preliminary evidence that our results generalize to the setting of [1].\\nOur work can be viewed as a step towards a statistical physics of deep learning , connecting the\\n“microphysics” of low-level network dynamics with the “thermodynamics” of high-level model\\nbehavior. We view the application of theoretical tools from physics, such as effective theories [ 24], to\\nbe a rich area for further work. The broader impact of such work, if successful, could be to make\\nmodels more transparent and predictable [ 23,25,26], crucial to the task of ensuring the safety of\\nadvanced AI systems.\\n10'],\n",
       " 'data': None,\n",
       " 'uris': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.peek() # returns a list of the first 10 items in the collection\n",
    "# collection.count() # returns the number of items in the collection\n",
    "# collection.modify(name=\"new_name\") # Rename the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.add(\n",
    "#     documents=[\"lorem ipsum...\", \"doc2\", \"doc3\", ...],\n",
    "#     metadatas=[{\"chapter\": \"3\", \"verse\": \"16\"}, {\"chapter\": \"3\", \"verse\": \"5\"}, {\"chapter\": \"29\", \"verse\": \"11\"}, ...],\n",
    "#     ids=[\"id1\", \"id2\", \"id3\", ...]\n",
    "# )\n",
    "\n",
    "import uuid\n",
    "\n",
    "for title, content in section_contents.items():\n",
    "  unique_id = str(uuid.uuid4())\n",
    "  if type(content) == dict:\n",
    "    for t, c in content.items():\n",
    "      collection.add(\n",
    "      documents = [c],\n",
    "      metadatas=[{\"section_title\": t}],\n",
    "      ids=[unique_id]\n",
    "    )\n",
    "  else:\n",
    "    collection.add(\n",
    "      documents = [content],\n",
    "      metadatas=[{\"section_title\": title}],\n",
    "      ids=[unique_id]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subQnPrompt = \"\"\"You are an AI language model assistant. Your task is to generate Five\n",
    "    different versions of the given user question to retrieve relevant documents from a vector\n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search.\n",
    "    Provide these alternative questions seperated by newlines only.\n",
    "\n",
    "    For example:\n",
    "    User question: \"What is the conclusion of the paper?\"\n",
    "\n",
    "    Generated questions:\n",
    "    What is the main takeaway from the paper?\n",
    "    What are the key findings of the paper?\n",
    "    What is the summary of the paper?\n",
    "    What is the final thought of the paper?\n",
    "    What is the ending of the paper?\n",
    "    \n",
    "    Output format should be as follows:\n",
    "\n",
    "    'What is Bill Gates known for?'\n",
    "│   \"Can you provide information about Bill Gates' background?\"\n",
    "\n",
    "    And not this format:\n",
    "\n",
    "    '1. What is Bill Gates known for?'\n",
    "│   \"2. Can you provide information about Bill Gates' background?\"\n",
    "    \"\"\"\n",
    "\n",
    "# subQnPrompt = \"\"\"\n",
    "# You are an AI language model assistant. Your task is to generate Five\n",
    "#     different versions of the given user question to retrieve relevant documents from a vector\n",
    "#     database. By generating multiple perspectives on the user question, your goal is to help\n",
    "#     the user overcome some of the limitations of the distance-based similarity search.\n",
    "#     Provide these alternative questions seperated by newlines.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-9FLxlLuUaRvL8vxswMcd2biqWeuvT\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"What triggers the process of generalization?\\nWhat is the reason behind generalization occurring?\\nCan you describe the causes of generalization?\\nWhat leads to the occurrence of generalization?\\nWhy is there a need for the process called generalization?\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": null\n",
      "      },\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1713446373,\n",
      "  \"model\": \"gpt-4-32k\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 47,\n",
      "    \"prompt_tokens\": 228,\n",
      "    \"total_tokens\": 275\n",
      "  },\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"prompt_index\": 0,\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "What triggers the process of generalization?\n",
      "What is the reason behind generalization occurring?\n",
      "Can you describe the causes of generalization?\n",
      "What leads to the occurrence of generalization?\n",
      "Why is there a need for the process called generalization?\n",
      "['What triggers the process of generalization?', 'What is the reason behind generalization occurring?', 'Can you describe the causes of generalization?', 'What leads to the occurrence of generalization?', 'Why is there a need for the process called generalization?']\n"
     ]
    }
   ],
   "source": [
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version = \"2024-02-01\",\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"cursor-gpt-4\", # model = \"deployment_name\".\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": subQnPrompt},\n",
    "        {\"role\": \"user\", \"content\": \"why does generalization occur?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "#print(response)\n",
    "print(response.model_dump_json(indent=2))\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "questions = response.choices[0].message.content.split(\"\\n\")\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs = collection.query(\n",
    "    query_texts=questions,\n",
    "    n_results=3,\n",
    "    # where={\"metadata_field\": \"is_equal_to_this\"},\n",
    "    # where_document={\"$contains\":\"search_string\"}\n",
    ")\n",
    "\n",
    "len(relevant_docs['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['502463a7-4688-4dd0-9d6f-9a54b0add4b7',\n",
       "   '3df1fa1f-b1a7-40b8-8fcb-51a0dab998ea',\n",
       "   'db9b51c0-a91e-4d1a-8042-4f5e21d308d5'],\n",
       "  ['502463a7-4688-4dd0-9d6f-9a54b0add4b7',\n",
       "   '3df1fa1f-b1a7-40b8-8fcb-51a0dab998ea',\n",
       "   'db9b51c0-a91e-4d1a-8042-4f5e21d308d5'],\n",
       "  ['502463a7-4688-4dd0-9d6f-9a54b0add4b7',\n",
       "   '3df1fa1f-b1a7-40b8-8fcb-51a0dab998ea',\n",
       "   'db9b51c0-a91e-4d1a-8042-4f5e21d308d5'],\n",
       "  ['502463a7-4688-4dd0-9d6f-9a54b0add4b7',\n",
       "   'db9b51c0-a91e-4d1a-8042-4f5e21d308d5',\n",
       "   '3df1fa1f-b1a7-40b8-8fcb-51a0dab998ea'],\n",
       "  ['502463a7-4688-4dd0-9d6f-9a54b0add4b7',\n",
       "   'db9b51c0-a91e-4d1a-8042-4f5e21d308d5',\n",
       "   '3df1fa1f-b1a7-40b8-8fcb-51a0dab998ea']],\n",
       " 'distances': [[0.3403434460452584, 0.38855066686549067, 0.4009697386823269],\n",
       "  [0.3219624988859576, 0.37492580737999054, 0.3816668843818757],\n",
       "  [0.3484505385848109, 0.400064797362986, 0.40610687158428693],\n",
       "  [0.32825513456780414, 0.37033589075312795, 0.3792920611799252],\n",
       "  [0.3349290299927769, 0.3809433774015685, 0.38528424963365987]],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [[{'section_title': '1 Introduction'},\n",
       "   {'section_title': 'Abstract'},\n",
       "   {'section_title': '3 Why Generalization Occurs: Representations and Dynamics'}],\n",
       "  [{'section_title': '1 Introduction'},\n",
       "   {'section_title': 'Abstract'},\n",
       "   {'section_title': '3 Why Generalization Occurs: Representations and Dynamics'}],\n",
       "  [{'section_title': '1 Introduction'},\n",
       "   {'section_title': 'Abstract'},\n",
       "   {'section_title': '3 Why Generalization Occurs: Representations and Dynamics'}],\n",
       "  [{'section_title': '1 Introduction'},\n",
       "   {'section_title': '3 Why Generalization Occurs: Representations and Dynamics'},\n",
       "   {'section_title': 'Abstract'}],\n",
       "  [{'section_title': '1 Introduction'},\n",
       "   {'section_title': '3 Why Generalization Occurs: Representations and Dynamics'},\n",
       "   {'section_title': 'Abstract'}]],\n",
       " 'documents': [['Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       "   '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.',\n",
       "   'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.'],\n",
       "  ['Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       "   '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.',\n",
       "   'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.'],\n",
       "  ['Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       "   '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.',\n",
       "   'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.'],\n",
       "  ['Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       "   'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.',\n",
       "   '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.'],\n",
       "  ['Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       "   'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.',\n",
       "   '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provided_context = relevant_docs\n",
    "provided_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTemplate:\n",
    "    def __init__(self, input_variables, template):\n",
    "        self.input_variables = input_variables\n",
    "        self.template = template\n",
    "\n",
    "    def format(self, **kwargs):\n",
    "        return self.template.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qaPrompt = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "#     \\n --- \\n {query_str} \\n --- \\n\n",
    "\n",
    "#     Here is any available background question + answer pairs:\n",
    "\n",
    "#     \\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "#     Here is additional context relevant to the question:\n",
    "\n",
    "#     \\n --- \\n {context_str} \\n --- \\n\n",
    "\n",
    "#     Use the above context and any background question + answer pairs to answer the question: \\n {query_str}\n",
    "#     \"\"\"\n",
    "\n",
    "# TODO: q_a_pairs from FT-ed model\n",
    "\n",
    "qaSysPrompt = \"\"\"\n",
    "Your task is to use the context provided to answer questions. \n",
    "Answer questions precisely and succinctly but provide full lists of steps and stipulations if the response requires it.\n",
    "If you cannot provide answers based on the context provided, answer with \"I cannot answer questions not related to JigsawStack\".\n",
    "\n",
    "Answer questions with as much detail as possible. Think logically and take it step by step. If the content explains where to find more information, please include that in your answer.\n",
    "\n",
    "DO NOT provide information other than what you have in the CONTENT.\n",
    "Do NOT mention \"the context\" in your answer.\n",
    "Do NOT create new context or use external sources to provide answers. \n",
    "\n",
    "Use ONLY the context provided to answer the question.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_query_template = \"\"\"\n",
    "    Here is the context relevant to the question:\n",
    "    ###{context_str}###\n",
    "\n",
    "    Answer the following question using the context above ONLY:\n",
    "    ###{query_str}###\n",
    "    \"\"\"\n",
    "\n",
    "qa_prompt = PromptTemplate(input_variables=[\"context_str\", \"query_str\"], template=context_query_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Here is the context relevant to the question:\n",
      "    ###only PyPDF library can be used to access Large Language Models###\n",
      "\n",
      "    Answer the following question using the context above ONLY:\n",
      "    ###What library can be used to access Large Language Models?###\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "formatted_qa_prompt = qa_prompt.format(context_str=\"only PyPDF library can be used to access Large Language Models\", query_str=\"What library can be used to access Large Language Models?\")\n",
    "print(formatted_qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_context = [{'role': 'user', 'content': 'Content:\\nDerbent, a city in Dagestan, Russia, claims to be the oldest city in Russia. Archaeological excavations have confirmed that Derbent has been continuously inhabited for nearly 2,000 years. Historical documentation dates back to the 8th century BC, making it one of the oldest continuously inhabited cities in the world.\\n\\nAnswer the following question using the content above:\\nHow old is the oldest city in Russia?'},\n",
    "#                   {'role': 'assistant', 'content':'The oldest city in Russia, Derbent, is nearly 2,000 years old.'},\n",
    "#                   {'role': 'user', 'content': 'Content:\\nJoan is 42 and John is 55\\n\\nAnswer the following question using the content above:\\nWhat is the age difference between Joan and John?'},\n",
    "#                   {'role': 'assistant', 'content':'The age difference between Joan and John is 13 years.'},\n",
    "#                   {'role': 'user', 'content': 'Content:\\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]\\n\\nAnswer the following question using the content above:\\nHow high do the High Plains go?'},\n",
    "#                   {'role': 'assistant', 'content':'The High Plains reach an elevation of up to 7,000ft '}]\n",
    "\n",
    "in_context = [\n",
    "    {'role': 'user', 'content': 'Here is the context relevant to the question:\\n###Derbent, a city in Dagestan, Russia, claims to be the oldest city in Russia. Archaeological excavations have confirmed that Derbent has been continuously inhabited for nearly 2,000 years. Historical documentation dates back to the 8th century BC, making it one of the oldest continuously inhabited cities in the world.###\\n\\nAnswer the following question using the context above ONLY:\\n###How old is the oldest city in Russia? How long has it been inhabited?###'},\n",
    "    {'role': 'assistant', 'content':'The oldest city in Russia, Derbent, is nearly 2,000 years old.'},\n",
    "    {'role': 'user', 'content': 'Here is the context relevant to the question:\\n###Joan is 42 years old and John is 55 years old.###\\n\\nAnswer the following question using the context above ONLY:\\n###What is the age gap between Joan and John? How much older is John than Joan?###'},\n",
    "    {'role': 'assistant', 'content': 'The age difference between Joan and John is 13 years. John is 13 years older than Joan.'},\n",
    "    {'role': 'user', 'content': 'Here is the context relevant to the question:\\n###The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).###\\n\\nAnswer the following question using the context above ONLY:\\n###What is the elevation range of the High Plains, specifically their highest point?###'},\n",
    "    {'role': 'assistant', 'content': 'The High Plains reach a maximum elevation of 7,000 feet.'}\n",
    "]\n",
    "\n",
    "# for o in in_context:\n",
    "#   print(o['content'])\n",
    "#   print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-9FD08XeJiktxA104qXfbulIP8DD97\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"The PyPDF library can be used to access Large Language Models.\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": null\n",
      "      },\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1713411924,\n",
      "  \"model\": \"gpt-4-32k\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 13,\n",
      "    \"prompt_tokens\": 531,\n",
      "    \"total_tokens\": 544\n",
      "  },\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"prompt_index\": 0,\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "The PyPDF library can be used to access Large Language Models.\n"
     ]
    }
   ],
   "source": [
    "formatted_user_prompt = qa_prompt.format(context_str=\"ONLY PyPDF library can be used to access Large Language Models\", query_str=\"What library can be used to access Large Language Models?\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"cursor-gpt-4\", # model = \"deployment_name\".\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": qaSysPrompt},\n",
    "    ] + in_context + [{\"role\": \"user\", \"content\": formatted_user_prompt}]\n",
    ")\n",
    "\n",
    "#print(response)\n",
    "print(response.model_dump_json(indent=2))\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "### check Distance metric/Similarity Score for Vector DB\n",
    "- Chroma L2>Cosine sim\n",
    "try BM25 relevance\n",
    "\n",
    "### Retrieveal:\n",
    "Find sweet spot between signal:noise ratio. \n",
    "Don't overcomplicate chunking (though smaller chunks have proven better results)\n",
    "but don't rely on long context length to retrieve multiple needles (recency bias of models do not attend well to earlier context)\n",
    "\n",
    "* Query Translation (Multi-Query)\n",
    "  * simplest unique context fetched by 5 queries\n",
    "* Query Translation (RAG Fusion)\n",
    "  * Parallel QnA Answering then synthesise, rerank to get top 5 most relevant docs of all queries\n",
    "* Query Translation (Decomposition)\n",
    "  * sequential summarisation of intermediary step-by-step QA then synthesise final answer\n",
    "* Query Translation (Step Back)\n",
    "  * generalise query in higher abstraction, query based on that\n",
    "* Query Translation (HyDE)\n",
    "  * generate hypothetical \"similar doc\" based on internal LLM knowledge and PromptE, use doc to retrieve\n",
    "* Routing\n",
    "  * define \"paths\" for classifier to choose\n",
    "* Query Construction\n",
    "  * metadata filtering or query-to-SQL/Cypher\n",
    "* Indexing (Multi Representation)\n",
    "  * Generate Proposition (concise summary) to retrieve on, but set Full Doc as context (ignore context length)\n",
    "* Indexing (RAPTOR**)\n",
    "  * sometimes answer comes from facts in multiple disparate docs\n",
    "  * Embd WHOLE docs/chunks and cluster **with long cxt models**, generate summary of cluster. (mid-level summary)\n",
    "    * Embd summaries of clusters (across different docs) used for similarity fetching later\n",
    "  * repeat recursively until only SINGLE cluster (one high level summary of ALL docs)\n",
    "    * becomes \"doc tree\" of varying levels of abstraction summaries with different docs at each abstraction level\n",
    "  * retrieval done on against all abstraction summaries generated, AND full docs (retrieved docs will range from long context to shorter context summaries)\n",
    "  * don't want to deal with chunking\n",
    "  * [code](https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb)\n",
    "  * [video for more deets](https://www.youtube.com/watch?v=jbGchdTL7d0)\n",
    "  * most relevant when Model to be used has context length limit like mistral 7b 32K\n",
    "* Indexing (ColBERT) \n",
    "  * Tokenise every single letter into embedding and find similarity at token level\n",
    "\n",
    "### (Corrective) CRAG\n",
    "introduces idea of State Machine RAGs. Prompt Chaining with State. \n",
    "Langgraph/Prompt Chaining with global state, with Relevant Docs Grading and Hallucination/Grounding Grading using LLM\n",
    "Retrieve Docs and grade them -> all ambiguous or irrelevant, use Web Search for Context (Tavily + Query rewriting optimised for web search) \n",
    "replace web search with some other data retriever like graphDB or SQLDB\n",
    "\n",
    "### Adaptive RAG aka Query Analysis (Router) + Self-Reflexion Rag*** -> like LLM \"guardrails\" with Agentic properties\n",
    "extension of CRAG, with self reflexion (grading of doc relevance and grounded/not answer retrieved)\n",
    "Cohere command-r (32B) optimised for picking tool use (based on defined rules in description) and output a \"path\" based on classification (output web_search or vector_store or \"False\" -> requires fallback)\n",
    "then Grade retrieved docs using command-r as \"grader\" (optimised for) -> relevant or not (output yes/no)\n",
    "and similarly Grade for answer grounded or not. \n",
    "feeding feedback from retrievals to self-correct\n",
    "LangGraph with RAG for more constrained, predictable, close-ended flows compared to Open-ended Agents\n",
    "\n",
    "**Definition of RAG**:\n",
    "reasoning AND retrieval on multiple chunks of information\n",
    "\n",
    "**Assumption of Retrieval**:\n",
    "rely on precise retrieval of relevant chunks! (small sized, high signal:noise ratio chunks)\n",
    "\n",
    "**Question to Ponder**: what is the Pareto Optimum granularity of docs to fetch? \n",
    "* maybe inclusion at document leve prioritised by some kind of heuristic like similarity/keyword search (IR techniques?)\n",
    "* Proposition Indexing (summary of actual doc) with Multi-Representation Retrieval (query simlarity of Proposition, return full doc as full context for RAG)\n",
    "  * something like Parent-Document Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "# Get your cohere API key on: www.cohere.com\n",
    "co = cohere.Client(os.environ[\"COHERE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relevant_docs[\"documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_documents = set([doc for docs in relevant_docs[\"documents\"] for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.',\n",
       " 'Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       " 'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       "  '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.',\n",
       "  'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.'],\n",
       " ['Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       "  '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.',\n",
       "  'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.'],\n",
       " ['Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       "  '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.',\n",
       "  'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.'],\n",
       " ['Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       "  'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.',\n",
       "  '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.'],\n",
       " ['Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       "  'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.',\n",
       "  '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.390488862991333 seconds to re-rank documents with Cohere.\n"
     ]
    }
   ],
   "source": [
    "query = \"why does generalization occur?\"\n",
    "# Example query and passages\n",
    "start = time.time()\n",
    "\n",
    "results = co.rerank(query=query, documents=unique_documents, top_n=4, model=\"rerank-english-v3.0\")\n",
    "print(f\"Took {time.time() - start} seconds to re-rank documents with Cohere.\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerank with reciprocal rank fusion instead of Cohere API\n",
    "# https://docs.llamaindex.ai/en/stable/examples/low_level/fusion_retriever/#step-2-perform-vector-search-for-each-query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try text-embedding-3-large with 256 dims to get indexing performance of text-ada-002\n",
    "# https://www.pinecone.io/learn/openai-embeddings-v3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_l_o_l(nested_list):\n",
    "    \"\"\"Flatten a list of lists into a single list.\n",
    "\n",
    "    Args:\n",
    "        nested_list (list): \n",
    "            – A list of lists (or iterables) to be flattened.\n",
    "\n",
    "    Returns:\n",
    "        list: A flattened list containing all items from the input list of lists.\n",
    "    \"\"\"\n",
    "    nested_list = [x if type(x) is list else [x,] for x in nested_list]\n",
    "    return [item for sublist in nested_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert at problem solving, knowledge extraction and reading comprehension.\n",
      "You are excellent at identifying the requirements, breaking a task down, and solving the problem step-by-step.\n",
      "\n",
      "For your task, you will be given a question that you must solve. \n",
      "You will solve this question by thinking things through step by step, generating a stepwise solution trace.\n",
      "After generating the solution trace, you will extract the final answer as simply as possible and return it. \n",
      "\n",
      "The following is the schema you must follow:\n",
      "\n",
      "-----\n",
      "SCHEMA -- The output (Reasoning ans Answer) should be formatted as a JSON object.\n",
      "-----\n",
      "\n",
      "Question: {Question on arithmetic or commonsense reasoning will go here. This is provided to you.}\n",
      "{\n",
      "    \"Reasoning\": \"{This is where you will break the question down and solve it step by step with explained reasoning. You generate this.}\",\n",
      "    \"Answer\": \"{The final answer, returned as simply as possible. You generate this.}\"\n",
      "}\n",
      "\n",
      "-----\n",
      "\n",
      "-----\n",
      "EXAMPLES - Shows how you should structure your outputs and examples of reasoning resulting in a simple answer.\n",
      "-----\n",
      "\n",
      "Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "{\n",
      "    \"Reasoning\": \"To find out how many trees were planted, we subtract the initial number of trees (15) from the final number of trees (21). Therefore, the calculation is 21 - 15, which equals 6 trees.\",\n",
      "    \"Answer\": \"6\"\n",
      "}\n",
      "\n",
      "Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "{\n",
      "    \"Reasoning\": \"Initially, there are 3 cars in the parking lot. With the arrival of 2 more cars, we simply add these to the initial count. So, the calculation is 3 cars + 2 cars, totaling 5 cars.\",\n",
      "    \"Answer\": \"5\"\n",
      "}\n",
      "\n",
      "Question: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
      "{\n",
      "    \"Reasoning\": \"Leah and her sister started with a total of 32 + 42 = 74 chocolates. After eating 35 chocolates, we subtract this number from the total. Thus, the calculation is 74 - 35, which results in 39 chocolates remaining.\",\n",
      "    \"Answer\": \"39\"\n",
      "}\n",
      "\n",
      "Question: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
      "{\n",
      "    \"Reasoning\": \"Jason originally had 20 lollipops and ended up with 12. To find out how many he gave to Denny, we subtract the final count from the original count: 20 - 12. Therefore, Jason gave Denny 8 lollipops.\",\n",
      "    \"Answer\": \"8\"\n",
      "}\n",
      "\n",
      "Question: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
      "{\n",
      "    \"Reasoning\": \"Shawn initially had 5 toys. He received 2 toys from his mom and 2 from his dad, making it 4 additional toys. Adding these to his initial count, we get 5 + 4, which equals 9 toys in total.\",\n",
      "    \"Answer\": \"9\"\n",
      "}\n",
      "\n",
      "Question: There were nine computers in the server room. Five more computers were installed each day, from Monday to Thursday. How many computers are now in the server room?\n",
      "{\n",
      "    \"Reasoning\": \"The installation occurred over 4 days (Monday to Thursday) with 5 computers added each day. This totals to 4 days * 5 computers/day = 20 computers. Adding these to the initial 9, we get 9 + 20 = 29 computers.\",\n",
      "    \"Answer\": \"29\"\n",
      "}\n",
      "\n",
      "Question: Michael had 58 golf balls. On Tuesday, he lost 23 golf balls. On Wednesday, he lost 2 more. How many golf balls did he have at the end of Wednesday?\n",
      "{\n",
      "    \"Reasoning\": \"Michael started with 58 balls. After losing 23 on Tuesday, he had 58 - 23 = 35 balls. Losing 2 more on Wednesday leaves him with 35 - 2 = 33 balls.\",\n",
      "    \"Answer\": \"33\"\n",
      "}\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "Here we go, you are an expert and you will do a good job by thinking things through step-by-step. \n",
      "Ensure you adhere to schema above and make sure the generated output is valid JSON.\n",
      "\n",
      "-----\n",
      "\n",
      "Question: whoami?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "structured_cot_cs_prompt = \"\"\"You are an expert at problem solving, knowledge extraction and reading comprehension.\n",
    "You are excellent at identifying the requirements, breaking a task down, and solving the problem step-by-step.\n",
    "\n",
    "For your task, you will be given a question that you must solve. \n",
    "You will solve this question by thinking things through step by step, generating a stepwise solution trace.\n",
    "After generating the solution trace, you will extract the final answer as simply as possible and return it. \n",
    "\n",
    "The following is the schema you must follow:\n",
    "\n",
    "-----\n",
    "SCHEMA -- The output (Reasoning ans Answer) should be formatted as a JSON object.\n",
    "-----\n",
    "\n",
    "Question: {{Question on arithmetic or commonsense reasoning will go here. This is provided to you.}}\n",
    "{{\n",
    "    \"Reasoning\": \"{{This is where you will break the question down and solve it step by step with explained reasoning. You generate this.}}\",\n",
    "    \"Answer\": \"{{The final answer, returned as simply as possible. You generate this.}}\"\n",
    "}}\n",
    "\n",
    "-----\n",
    "\n",
    "-----\n",
    "EXAMPLES - Shows how you should structure your outputs and examples of reasoning resulting in a simple answer.\n",
    "-----\n",
    "\n",
    "Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "{{\n",
    "    \"Reasoning\": \"To find out how many trees were planted, we subtract the initial number of trees (15) from the final number of trees (21). Therefore, the calculation is 21 - 15, which equals 6 trees.\",\n",
    "    \"Answer\": \"6\"\n",
    "}}\n",
    "\n",
    "Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "{{\n",
    "    \"Reasoning\": \"Initially, there are 3 cars in the parking lot. With the arrival of 2 more cars, we simply add these to the initial count. So, the calculation is 3 cars + 2 cars, totaling 5 cars.\",\n",
    "    \"Answer\": \"5\"\n",
    "}}\n",
    "\n",
    "Question: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "{{\n",
    "    \"Reasoning\": \"Leah and her sister started with a total of 32 + 42 = 74 chocolates. After eating 35 chocolates, we subtract this number from the total. Thus, the calculation is 74 - 35, which results in 39 chocolates remaining.\",\n",
    "    \"Answer\": \"39\"\n",
    "}}\n",
    "\n",
    "Question: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
    "{{\n",
    "    \"Reasoning\": \"Jason originally had 20 lollipops and ended up with 12. To find out how many he gave to Denny, we subtract the final count from the original count: 20 - 12. Therefore, Jason gave Denny 8 lollipops.\",\n",
    "    \"Answer\": \"8\"\n",
    "}}\n",
    "\n",
    "Question: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
    "{{\n",
    "    \"Reasoning\": \"Shawn initially had 5 toys. He received 2 toys from his mom and 2 from his dad, making it 4 additional toys. Adding these to his initial count, we get 5 + 4, which equals 9 toys in total.\",\n",
    "    \"Answer\": \"9\"\n",
    "}}\n",
    "\n",
    "Question: There were nine computers in the server room. Five more computers were installed each day, from Monday to Thursday. How many computers are now in the server room?\n",
    "{{\n",
    "    \"Reasoning\": \"The installation occurred over 4 days (Monday to Thursday) with 5 computers added each day. This totals to 4 days * 5 computers/day = 20 computers. Adding these to the initial 9, we get 9 + 20 = 29 computers.\",\n",
    "    \"Answer\": \"29\"\n",
    "}}\n",
    "\n",
    "Question: Michael had 58 golf balls. On Tuesday, he lost 23 golf balls. On Wednesday, he lost 2 more. How many golf balls did he have at the end of Wednesday?\n",
    "{{\n",
    "    \"Reasoning\": \"Michael started with 58 balls. After losing 23 on Tuesday, he had 58 - 23 = 35 balls. Losing 2 more on Wednesday leaves him with 35 - 2 = 33 balls.\",\n",
    "    \"Answer\": \"33\"\n",
    "}}\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "Here we go, you are an expert and you will do a good job by thinking things through step-by-step. \n",
    "Ensure you adhere to schema above and make sure the generated output is valid JSON.\n",
    "\n",
    "-----\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt = PromptTemplate(input_variables=[\"question\"], template=structured_cot_cs_prompt)\n",
    "print(qa_prompt.format(question=\"whoami?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
