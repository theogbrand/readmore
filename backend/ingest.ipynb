{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pinecone-client\n",
    "# %pip install python-dotenv\n",
    "# %pip install openai\n",
    "# %pip install chromadb-client\n",
    "# %pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI, AzureOpenAI\n",
    "import os\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pinecone import Pinecone\n",
    "\n",
    "# pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "# index = pc.Index(\"test-pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "\n",
    "# response = client.embeddings.create(\n",
    "#     input=\"Your text string goes here\",\n",
    "#     model=\"text-embedding-3-large\"\n",
    "# )\n",
    "\n",
    "# print(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version = \"2024-02-01\",\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=\"Your text string goes here\",\n",
    "    model=\"ada_gcal\"\n",
    ")\n",
    "\n",
    "print(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text, model=\"ada_gcal\"): # model = \"deployment_name\"\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "# df_bills['ada_v2'] = df_bills[\"text\"].apply(lambda x : generate_embeddings (x, model = 'text-embedding-ada-002'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"./pdf/grokking.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pages))\n",
    "print()\n",
    "\n",
    "full_document = \"\"\n",
    "\n",
    "for page in pages:\n",
    "  full_document += page.page_content\n",
    "\n",
    "print(full_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# first, extract Section Headers\n",
    "pattern = r\"\\n\\d+ [A-Z][a-zA-Z\\s:]+\\n\"\n",
    "\n",
    "# Find all matches\n",
    "matches = re.findall(pattern, full_document)\n",
    "\n",
    "matches.insert(0, \"Abstract\")\n",
    "\n",
    "# Print all section titles\n",
    "for match in matches:\n",
    "    print(match.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"\\n\\d+ [A-Z][a-zA-Z\\s:]+\\n\"\n",
    "\n",
    "# Split the text into sections\n",
    "sections = re.split(pattern, full_document)\n",
    "\n",
    "# number of sections should match with number of section titles (aka matches of regex pattern)\n",
    "# print(sections[0])\n",
    "# print(sections[2])\n",
    "# print(len(sections))\n",
    "\n",
    "# Create a dictionary to store section titles and contents\n",
    "section_contents = {}\n",
    "\n",
    "# Use zip to iterate over matches and sections simultaneously\n",
    "for match, section in zip(matches, sections):\n",
    "    section_title = match.strip()\n",
    "    content = section.strip()\n",
    "    section_contents[section_title] = content\n",
    "\n",
    "section_contents.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_contents[\"6 Conclusion\"] #TODO: process this to ONLY the abstract content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, content in section_contents.items():\n",
    "    print(title)\n",
    "    print(len(content))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsection = section_contents[\"3 Why Generalization Occurs: Representations and Dynamics\"]\n",
    "subsection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then extract subsections from sections\n",
    "pattern = r\"\\n\\d+\\.\\d+ [A-Z][a-zA-Z\\s:]+\\n\"\n",
    "\n",
    "subsections = re.findall(pattern, subsection)\n",
    "print(len(subsections))\n",
    "for match in subsections:\n",
    "    print(match.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract references from conclusion\n",
    "abstract = section_contents[\"Abstract\"]\n",
    "section_contents[\"Abstract\"] = abstract.split(\"Abstract\")[1]\n",
    "\n",
    "section_contents[\"Abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract references from conclusion\n",
    "conclusion = section_contents[\"6 Conclusion\"]\n",
    "section_contents[\"6 Conclusion\"] = conclusion.split(\"References\")[0]\n",
    "# conclusion.split(\"References\")[1]\n",
    "\n",
    "section_contents[\"6 Conclusion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run subsection splitting algo\n",
    "\n",
    "for title, section in section_contents.items():\n",
    "  pattern = r\"\\n\\d+\\.\\d+ [A-Z][a-zA-Z\\s:]+\\n\"\n",
    "  subsections_headers = re.findall(pattern, section)\n",
    "  \n",
    "  if len(subsections_headers) > 0:\n",
    "    subsection_map = {}\n",
    "    subsections_content = re.split(pattern, section)\n",
    "\n",
    "    subsections_headers.insert(0, title)\n",
    "    for subsection_header, subsection in zip(subsections_headers, subsections_content):\n",
    "      subsection_title = subsection_header.strip()\n",
    "      content = subsection.strip()\n",
    "      subsection_map[subsection_title] = content\n",
    "\n",
    "    print(subsection_map.keys())\n",
    "    section_contents[title] = subsection_map\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(section_contents[\"3 Why Generalization Occurs: Representations and Dynamics\"])\n",
    "len(section_contents[\"4 Delayed Generalization: A Phase Diagram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this for section title-based splitting\n",
    "section_contents[\"4 Delayed Generalization: A Phase Diagram\"]['4.1 Phase diagram of a toy model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text, model=\"ada_gcal\"): # model = \"deployment_name\"\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "# df_bills['ada_v2'] = df_bills[\"text\"].apply(lambda x : generate_embeddings (x, model = 'text-embedding-ada-002'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "azure_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "                api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "                api_type=\"azure\",\n",
    "                api_version = \"2024-02-01\",\n",
    "                model_name=\"ada_gcal\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "# Example setup of the client to connect to your chroma server\n",
    "client = chromadb.HttpClient(host='localhost', port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_collection(name=\"test_pdf_2\") \n",
    "# Delete a collection and all associated embeddings, documents, and metadata. ⚠️ This is destructive and not reversible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(\n",
    "        embedding_function=azure_ef,\n",
    "        name=\"test_pdf_2\",\n",
    "        metadata={\"hnsw:space\": \"l2\"} # l2 is the default\n",
    "    )\n",
    "collection = client.get_collection(name=\"test_pdf_2\", embedding_function=azure_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [],\n",
       " 'embeddings': [],\n",
       " 'metadatas': [],\n",
       " 'documents': [],\n",
       " 'data': None,\n",
       " 'uris': None}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.peek() # returns a list of the first 10 items in the collection\n",
    "# collection.count() # returns the number of items in the collection\n",
    "# collection.modify(name=\"new_name\") # Rename the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.add(\n",
    "#     documents=[\"lorem ipsum...\", \"doc2\", \"doc3\", ...],\n",
    "#     metadatas=[{\"chapter\": \"3\", \"verse\": \"16\"}, {\"chapter\": \"3\", \"verse\": \"5\"}, {\"chapter\": \"29\", \"verse\": \"11\"}, ...],\n",
    "#     ids=[\"id1\", \"id2\", \"id3\", ...]\n",
    "# )\n",
    "\n",
    "import uuid\n",
    "\n",
    "for title, content in section_contents.items():\n",
    "  unique_id = str(uuid.uuid4())\n",
    "  if type(content) == dict:\n",
    "    for t, c in content.items():\n",
    "      collection.add(\n",
    "      documents = [c],\n",
    "      metadatas=[{\"section_title\": t}],\n",
    "      ids=[unique_id]\n",
    "    )\n",
    "  else:\n",
    "    collection.add(\n",
    "      documents = [content],\n",
    "      metadatas=[{\"section_title\": title}],\n",
    "      ids=[unique_id]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "subQnPrompt = \"\"\"You are an AI language model assistant. Your task is to generate Five\n",
    "    different versions of the given user question to retrieve relevant documents from a vector\n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search.\n",
    "    Provide these alternative questions seperated by newlines only.\n",
    "\n",
    "    For example:\n",
    "    User question: \"What is the conclusion of the paper?\"\n",
    "\n",
    "    Generated questions:\n",
    "    What is the main takeaway from the paper?\n",
    "    What are the key findings of the paper?\n",
    "    What is the summary of the paper?\n",
    "    What is the final thought of the paper?\n",
    "    What is the ending of the paper?\n",
    "    \n",
    "    Output format should be as follows:\n",
    "\n",
    "    'What is Bill Gates known for?'\n",
    "│   \"Can you provide information about Bill Gates' background?\"\n",
    "\n",
    "    And not this format:\n",
    "\n",
    "    '1. What is Bill Gates known for?'\n",
    "│   \"2. Can you provide information about Bill Gates' background?\"\n",
    "    \"\"\"\n",
    "\n",
    "# subQnPrompt = \"\"\"\n",
    "# You are an AI language model assistant. Your task is to generate Five\n",
    "#     different versions of the given user question to retrieve relevant documents from a vector\n",
    "#     database. By generating multiple perspectives on the user question, your goal is to help\n",
    "#     the user overcome some of the limitations of the distance-based similarity search.\n",
    "#     Provide these alternative questions seperated by newlines.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-9FDHSyVfUe7jjzVmRoc1szlbwSFCg\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"What are the reasons behind the occurrence of generalization?\\nCan you explain the factors leading to the phenomenon of generalization?\\nWhat causes the generalization process to happen?\\nWhat are the underlying mechanisms that result in generalization?\\nWhat triggers the process of generalization?\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": null\n",
      "      },\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1713412998,\n",
      "  \"model\": \"gpt-4-32k\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 52,\n",
      "    \"prompt_tokens\": 228,\n",
      "    \"total_tokens\": 280\n",
      "  },\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"prompt_index\": 0,\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "What are the reasons behind the occurrence of generalization?\n",
      "Can you explain the factors leading to the phenomenon of generalization?\n",
      "What causes the generalization process to happen?\n",
      "What are the underlying mechanisms that result in generalization?\n",
      "What triggers the process of generalization?\n",
      "['What are the reasons behind the occurrence of generalization?', 'Can you explain the factors leading to the phenomenon of generalization?', 'What causes the generalization process to happen?', 'What are the underlying mechanisms that result in generalization?', 'What triggers the process of generalization?']\n"
     ]
    }
   ],
   "source": [
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version = \"2024-02-01\",\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"cursor-gpt-4\", # model = \"deployment_name\".\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": subQnPrompt},\n",
    "        {\"role\": \"user\", \"content\": \"why does generalization occur?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "#print(response)\n",
    "print(response.model_dump_json(indent=2))\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "questions = response.choices[0].message.content.split(\"\\n\")\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs = collection.query(\n",
    "    query_texts=questions,\n",
    "    n_results=3,\n",
    "    # where={\"metadata_field\": \"is_equal_to_this\"},\n",
    "    # where_document={\"$contains\":\"search_string\"}\n",
    ")\n",
    "\n",
    "len(relevant_docs['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['502463a7-4688-4dd0-9d6f-9a54b0add4b7',\n",
       "   '3df1fa1f-b1a7-40b8-8fcb-51a0dab998ea',\n",
       "   'db9b51c0-a91e-4d1a-8042-4f5e21d308d5'],\n",
       "  ['502463a7-4688-4dd0-9d6f-9a54b0add4b7',\n",
       "   '3df1fa1f-b1a7-40b8-8fcb-51a0dab998ea',\n",
       "   'db9b51c0-a91e-4d1a-8042-4f5e21d308d5'],\n",
       "  ['502463a7-4688-4dd0-9d6f-9a54b0add4b7',\n",
       "   '3df1fa1f-b1a7-40b8-8fcb-51a0dab998ea',\n",
       "   'db9b51c0-a91e-4d1a-8042-4f5e21d308d5'],\n",
       "  ['502463a7-4688-4dd0-9d6f-9a54b0add4b7',\n",
       "   '3df1fa1f-b1a7-40b8-8fcb-51a0dab998ea',\n",
       "   'db9b51c0-a91e-4d1a-8042-4f5e21d308d5'],\n",
       "  ['502463a7-4688-4dd0-9d6f-9a54b0add4b7',\n",
       "   '3df1fa1f-b1a7-40b8-8fcb-51a0dab998ea',\n",
       "   'db9b51c0-a91e-4d1a-8042-4f5e21d308d5']],\n",
       " 'distances': [[0.32714956092188724, 0.37943399695904956, 0.3891698747188948],\n",
       "  [0.3296414970979333, 0.36564393459354577, 0.38067940901124264],\n",
       "  [0.350467581296492, 0.3885351457047193, 0.41402199947094376],\n",
       "  [0.30511288407608245, 0.3532983574575262, 0.3605980527486859],\n",
       "  [0.3403434460452584, 0.38855066686549067, 0.4009697386823269]],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [[{'section_title': '1 Introduction'},\n",
       "   {'section_title': 'Abstract'},\n",
       "   {'section_title': '3 Why Generalization Occurs: Representations and Dynamics'}],\n",
       "  [{'section_title': '1 Introduction'},\n",
       "   {'section_title': 'Abstract'},\n",
       "   {'section_title': '3 Why Generalization Occurs: Representations and Dynamics'}],\n",
       "  [{'section_title': '1 Introduction'},\n",
       "   {'section_title': 'Abstract'},\n",
       "   {'section_title': '3 Why Generalization Occurs: Representations and Dynamics'}],\n",
       "  [{'section_title': '1 Introduction'},\n",
       "   {'section_title': 'Abstract'},\n",
       "   {'section_title': '3 Why Generalization Occurs: Representations and Dynamics'}],\n",
       "  [{'section_title': '1 Introduction'},\n",
       "   {'section_title': 'Abstract'},\n",
       "   {'section_title': '3 Why Generalization Occurs: Representations and Dynamics'}]],\n",
       " 'documents': [['Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       "   '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.',\n",
       "   'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.'],\n",
       "  ['Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       "   '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.',\n",
       "   'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.'],\n",
       "  ['Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       "   '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.',\n",
       "   'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.'],\n",
       "  ['Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       "   '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.',\n",
       "   'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.'],\n",
       "  ['Perhaps thecentral challenge of a scientiﬁc understanding of deep learning lies in accounting for neu-\\nral network generalization. Power et al. [ 1] recently added a new puzzle to the task of understanding\\ngeneralization with their discovery of grokking . Grokking refers to the surprising phenomenon of\\ndelayed generalization where neural networks, on certain learning problems, generalize long after\\noverﬁtting their training set. It is a rare albeit striking phenomenon that violates common machine\\nlearning intuitions, raising three key puzzles:\\nQ1The origin of generalization : When trained on the algorithmic datasets where grokking occurs,\\nhow do models generalize at all?\\nQ2The critical training size : Why does the training time needed to “grok” (generalize) diverge as\\nthe training set size decreases toward a critical point?\\nQ3Delayed generalization : Under what conditions does delayed generalization occur?\\nWe provide evidence that representation learning is central to answering each of these questions. Our\\nanswers can be summarized as follows:\\nA1Generalization can be attributed to learning a good representation of the input embeddings,\\ni.e., a representation that has the appropriate structure for the task and which can be predicted\\nfrom the theory in Section 3. See Figures 1 and 2.\\nA2The critical training set size corresponds to the least amount of training data that can determine\\nsuch a representation (which, in some cases, is unique up to linear transformations).\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.10343v2  [cs.LG]  14 Oct 2022Initialization (0 iterations)\\ntrain acc: 0.0 — val acc: 0.0Overﬁtting (1000 iterations)\\ntrain acc: 1.0 — val acc: 0.1Representation Learning (20000 iterations)\\ntrain acc: 1.0 — val acc: 1.0Figure 1: Visualization of the ﬁrst two principal components of the learned input embeddings at\\ndifferent training stages of a transformer learning modular addition. We observe that generalization\\ncoincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.\\nA3Grokking is a phase between “comprehension” and “memorization” phases and it can be\\nremedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.\\nThis paper is organized as follows: In Section 2, we introduce the problem setting and build a\\nsimpliﬁed toy model. In Section 3, we will use an effective theory approach, a useful tool from\\ntheoretical physics, to shed some light on questions Q1andQ2and show the relationship between\\ngeneralization and the learning of structured representations. In Section 4, we explain Q3by\\ndisplaying phase diagrams from a grid search of hyperparameters and show how we can “de-delay”\\ngeneralization by following intuition developed from the phase diagram. We discuss related work in\\nSection 5, followed by conclusions in Section 6.1',\n",
       "   '\\nWe aim to understand grokking , a phenomenon where models generalize long after\\noverﬁtting their training set. We present both a microscopic analysis anchored by an\\neffective theory and a macroscopic analysis of phase diagrams describing learning\\nperformance across hyperparameters. We ﬁnd that generalization originates from\\nstructured representations whose training dynamics and dependence on training set\\nsize can be predicted by our effective theory in a toy setting. We observe empirically\\nthe presence of four learning phases: comprehension ,grokking ,memorization , and\\nconfusion . We ﬁnd representation learning to occur only in a “Goldilocks zone”\\n(including comprehension and grokking) between memorization and confusion.\\nWe ﬁnd on transformers the grokking phase stays closer to the memorization phase\\n(compared to the comprehension phase), leading to delayed generalization. The\\nGoldilocks phase is reminiscent of “intelligence from starvation” in Darwinian\\nevolution, where resource limitations drive discovery of more efﬁcient solutions.\\nThis study not only provides intuitive explanations of the origin of grokking, but\\nalso highlights the usefulness of physics-inspired tools, e.g., effective theories and\\nphase diagrams, for understanding deep learning.',\n",
       "   'We can see that generalization appears to be linked to the emergence of highly-structured embeddings\\nin Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a\\ncircle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a\\nnotion of representation quality and show that it predicts the model’s performance. We then develop a\\nphysics-inspired effective theory of learning which can accurately predict the critical training set size\\nand training trajectories of representations. The concept of an effective theory in physics is similar\\nto model reduction in computational methods in that it aims to describe complex phenomena with\\nsimple yet intuitive pictures. In our effective theory, we will model the dynamics of representation\\nlearning not as gradient descent of the true task loss but rather a simpler effective loss function ℓeff\\nwhich depends only on the representations in embedding space and not on the decoder.']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provided_context = relevant_docs\n",
    "provided_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTemplate:\n",
    "    def __init__(self, input_variables, template):\n",
    "        self.input_variables = input_variables\n",
    "        self.template = template\n",
    "\n",
    "    def format(self, **kwargs):\n",
    "        return self.template.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qaPrompt = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "#     \\n --- \\n {query_str} \\n --- \\n\n",
    "\n",
    "#     Here is any available background question + answer pairs:\n",
    "\n",
    "#     \\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "#     Here is additional context relevant to the question:\n",
    "\n",
    "#     \\n --- \\n {context_str} \\n --- \\n\n",
    "\n",
    "#     Use the above context and any background question + answer pairs to answer the question: \\n {query_str}\n",
    "#     \"\"\"\n",
    "\n",
    "# TODO: q_a_pairs from FT-ed model\n",
    "\n",
    "qaSysPrompt = \"\"\"\n",
    "Your task is to use the context provided to answer questions. \n",
    "Answer questions precisely and succinctly but provide full lists of steps and stipulations if the response requires it.\n",
    "If you cannot provide answers based on the context provided, answer with \"I cannot answer questions not related to JigsawStack\".\n",
    "\n",
    "Answer questions with as much detail as possible. Think logically and take it step by step. If the content explains where to find more information, please include that in your answer.\n",
    "\n",
    "DO NOT provide information other than what you have in the CONTENT.\n",
    "Do NOT mention \"the context\" in your answer.\n",
    "Do NOT create new context or use external sources to provide answers. \n",
    "\n",
    "Use ONLY the context provided to answer the question.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_query_template = \"\"\"\n",
    "    Here is the context relevant to the question:\n",
    "    ###{context_str}###\n",
    "\n",
    "    Answer the following question using the context above ONLY:\n",
    "    ###{query_str}###\n",
    "    \"\"\"\n",
    "\n",
    "qa_prompt = PromptTemplate(input_variables=[\"context_str\", \"query_str\"], template=context_query_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Here is the context relevant to the question:\n",
      "    ###only PyPDF library can be used to access Large Language Models###\n",
      "\n",
      "    Answer the following question using the context above ONLY:\n",
      "    ###What library can be used to access Large Language Models?###\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "formatted_qa_prompt = qa_prompt.format(context_str=\"only PyPDF library can be used to access Large Language Models\", query_str=\"What library can be used to access Large Language Models?\")\n",
    "print(formatted_qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_context = [{'role': 'user', 'content': 'Content:\\nDerbent, a city in Dagestan, Russia, claims to be the oldest city in Russia. Archaeological excavations have confirmed that Derbent has been continuously inhabited for nearly 2,000 years. Historical documentation dates back to the 8th century BC, making it one of the oldest continuously inhabited cities in the world.\\n\\nAnswer the following question using the content above:\\nHow old is the oldest city in Russia?'},\n",
    "#                   {'role': 'assistant', 'content':'The oldest city in Russia, Derbent, is nearly 2,000 years old.'},\n",
    "#                   {'role': 'user', 'content': 'Content:\\nJoan is 42 and John is 55\\n\\nAnswer the following question using the content above:\\nWhat is the age difference between Joan and John?'},\n",
    "#                   {'role': 'assistant', 'content':'The age difference between Joan and John is 13 years.'},\n",
    "#                   {'role': 'user', 'content': 'Content:\\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]\\n\\nAnswer the following question using the content above:\\nHow high do the High Plains go?'},\n",
    "#                   {'role': 'assistant', 'content':'The High Plains reach an elevation of up to 7,000ft '}]\n",
    "\n",
    "in_context = [\n",
    "    {'role': 'user', 'content': 'Here is the context relevant to the question:\\n###Derbent, a city in Dagestan, Russia, claims to be the oldest city in Russia. Archaeological excavations have confirmed that Derbent has been continuously inhabited for nearly 2,000 years. Historical documentation dates back to the 8th century BC, making it one of the oldest continuously inhabited cities in the world.###\\n\\nAnswer the following question using the context above ONLY:\\n###How old is the oldest city in Russia? How long has it been inhabited?###'},\n",
    "    {'role': 'assistant', 'content':'The oldest city in Russia, Derbent, is nearly 2,000 years old.'},\n",
    "    {'role': 'user', 'content': 'Here is the context relevant to the question:\\n###Joan is 42 years old and John is 55 years old.###\\n\\nAnswer the following question using the context above ONLY:\\n###What is the age gap between Joan and John? How much older is John than Joan?###'},\n",
    "    {'role': 'assistant', 'content': 'The age difference between Joan and John is 13 years. John is 13 years older than Joan.'},\n",
    "    {'role': 'user', 'content': 'Here is the context relevant to the question:\\n###The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).###\\n\\nAnswer the following question using the context above ONLY:\\n###What is the elevation range of the High Plains, specifically their highest point?###'},\n",
    "    {'role': 'assistant', 'content': 'The High Plains reach a maximum elevation of 7,000 feet.'}\n",
    "]\n",
    "\n",
    "# for o in in_context:\n",
    "#   print(o['content'])\n",
    "#   print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-9FD08XeJiktxA104qXfbulIP8DD97\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"The PyPDF library can be used to access Large Language Models.\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": null\n",
      "      },\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1713411924,\n",
      "  \"model\": \"gpt-4-32k\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 13,\n",
      "    \"prompt_tokens\": 531,\n",
      "    \"total_tokens\": 544\n",
      "  },\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"prompt_index\": 0,\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "The PyPDF library can be used to access Large Language Models.\n"
     ]
    }
   ],
   "source": [
    "formatted_user_prompt = qa_prompt.format(context_str=\"ONLY PyPDF library can be used to access Large Language Models\", query_str=\"What library can be used to access Large Language Models?\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"cursor-gpt-4\", # model = \"deployment_name\".\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": qaSysPrompt},\n",
    "    ] + in_context + [{\"role\": \"user\", \"content\": formatted_user_prompt}]\n",
    ")\n",
    "\n",
    "#print(response)\n",
    "print(response.model_dump_json(indent=2))\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "Query Translation (Multi-Query) -> simplest unique context fetched by 5 queries\n",
    "⌨️ (0:28:20) Query Translation (RAG Fusion) -> Parallel QnA Answering then synthesise, rerank to get top 5 most relevant docs of all queries\n",
    "⌨️ (0:33:57) Query Translation (Decomposition) -> sequential summarisation of intermediary step-by-step QA then synthesise final answer\n",
    "⌨️ (0:40:31) Query Translation (Step Back) -> generalise query in higher abstraction, query based on that\n",
    "⌨️ (0:47:24) Query Translation (HyDE) -> generate hypothetical \"similar doc\" based on internal LLM knowledge and PromptE, use doc to retrieve\n",
    "⌨️ (0:52:07) Routing -> define \"paths\" for classifier to choose\n",
    "⌨️ (0:59:08) Query Construction -> metadata filtering or query-to-SQL/Cypher\n",
    "⌨️ (1:05:05) Indexing (Multi Representation) -> Generate Proposition (concise summary) to retrieve on, but set Full Doc as context (ignore context length)\n",
    "⌨️ (1:11:39) Indexing (RAPTOR) -> \n",
    "⌨️ (1:19:19) Indexing (ColBERT) -> Tokenise every single letter into embedding and find similarity at token level\n",
    "\n",
    "(Corrective) CRAG\n",
    "Langgraph/Prompt Chaining with global state, with Relevant Docs Grading and Hallucination/Grounding Grading using LLM\n",
    "\n",
    "Adaptive RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_lzma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/pdfai/backend/venv/lib/python3.9/site-packages/fastavro/read.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _read\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32mfastavro/_read.pyx:10\u001b[0m, in \u001b[0;36minit fastavro._read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/lzma.py:27\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_lzma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_lzma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _encode_filter_properties, _decode_filter_properties\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_lzma'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcohere\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Get your cohere API key on: www.cohere.com\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# co = cohere.Client(os.environ[\"COHERE_API_KEY\"])\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/pdfai/backend/venv/lib/python3.9/site-packages/cohere/__init__.py:131\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    122\u001b[0m     BadRequestError,\n\u001b[1;32m    123\u001b[0m     ForbiddenError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     UnauthorizedError,\n\u001b[1;32m    129\u001b[0m )\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m connectors, datasets, embed_jobs, finetuning, models\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncClient, Client\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    133\u001b[0m     DatasetsCreateResponse,\n\u001b[1;32m    134\u001b[0m     DatasetsCreateResponseDatasetParts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m     DatasetsListResponse,\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membed_jobs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CreateEmbedJobRequestTruncate\n",
      "File \u001b[0;32m~/projects/pdfai/backend/venv/lib/python3.9/site-packages/cohere/client.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanually_maintained\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tokenizers \u001b[38;5;28;01mas\u001b[39;00m local_tokenizers\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_overrides\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wait, async_wait, merge_embed_responses, SyncSdkUtils, AsyncSdkUtils\n\u001b[1;32m     22\u001b[0m run_overrides()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Use NoReturn as Never type for compatibility\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/pdfai/backend/venv/lib/python3.9/site-packages/cohere/utils.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastavro\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_schema, reader, writer\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmbedResponse, EmbedResponse_EmbeddingsFloats, EmbedResponse_EmbeddingsByType, ApiMeta, \\\n\u001b[1;32m     12\u001b[0m     EmbedByTypeResponseEmbeddings, ApiMetaBilledUnits, EmbedJob, CreateEmbedJobResponse, Dataset\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetsCreateResponse, DatasetsGetResponse\n",
      "File \u001b[0;32m~/projects/pdfai/backend/venv/lib/python3.9/site-packages/fastavro/__init__.py:47\u001b[0m\n\u001b[1;32m     43\u001b[0m __version_info__ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     44\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m __version_info__\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfastavro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mread\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfastavro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrite\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfastavro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/pdfai/backend/venv/lib/python3.9/site-packages/fastavro/read.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _read\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _read_py \u001b[38;5;28;01mas\u001b[39;00m _read  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m json_read\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logical_readers\n",
      "File \u001b[0;32m~/projects/pdfai/backend/venv/lib/python3.9/site-packages/fastavro/_read_py.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlzma\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzlib\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime, timezone\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/lzma.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_lzma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_lzma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _encode_filter_properties, _decode_filter_properties\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01m_compression\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_lzma'"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "\n",
    "# Get your cohere API key on: www.cohere.com\n",
    "# co = cohere.Client(os.environ[\"COHERE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [doc.page_content for doc in docs]\n",
    "\n",
    "# Example query and passages\n",
    "start = time.time()\n",
    "\n",
    "results = co.rerank(query=query, documents=documents, top_n=4, model=\"rerank-english-v2.0\")\n",
    "print(f\"Took {time.time() - start} seconds to re-rank documents with Cohere.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
