{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.24.1-cp39-none-macosx_10_9_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.24.1 (from pymupdf)\n",
      "  Using cached PyMuPDFb-1.24.1-py3-none-macosx_10_9_x86_64.whl.metadata (1.4 kB)\n",
      "Downloading PyMuPDF-1.24.1-cp39-none-macosx_10_9_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hUsing cached PyMuPDFb-1.24.1-py3-none-macosx_10_9_x86_64.whl (30.3 MB)\n",
      "Installing collected packages: PyMuPDFb, pymupdf\n",
      "Successfully installed PyMuPDFb-1.24.1 pymupdf-1.24.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install llmsherpa\n",
    "# %pip install llama-index\n",
    "# %pip install llama-index-llms-anthropic\n",
    "\n",
    "# %pip install langchain\n",
    "# %pip install langchain-anthropic\n",
    "\n",
    "# %pip install unstructured\n",
    "# %pip install \"unstructured[all-docs]\"\n",
    "\n",
    "# %pip uninstall pdfminer -y\n",
    "# %pip install pdfminer.six\n",
    "\n",
    "# %pip install pypdf\n",
    "# %pip install pdfplumber\n",
    "# %pip install --upgrade pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "doc = fitz.open(\"/Users/ogb/Desktop/chinese-LLAMA.pdf\") # open a document\n",
    "out = open(\"output.txt\", \"wb\") # create a text output\n",
    "for page in doc: # iterate the document pages\n",
    "    text = page.get_text().encode(\"utf8\") # get plain text (is in UTF-8)\n",
    "    out.write(text) # write text of page\n",
    "    out.write(bytes((12,))) # write page delimiter (form feed 0x0C)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 1: INTRODUCTION\n",
      "Natural language processing (NLP) field has witnessed a substantial paradigm shift with the advent\n",
      "of Large Language Models (LLMs). These models, distinguished by their considerable size and\n",
      "comprehensive training data, have demonstrated extraordinary abilities in comprehending and pro-\n",
      "ducing human-like text. In contrast to pre-trained language models dedicated to text understanding,\n",
      "such as BERT (Devlin et al., 2019), the GPT series (Radford et al., 2018) accentuates text generation,\n",
      "positioning them as more suitable platforms for creativity compared to their counterparts. Notably,\n",
      "the latest members of the GPT family, namely ChatGPT and GPT-4, have garnered significant atten-\n",
      "tion, establishing themselves as leading examples in this rapidly evolving field.\n",
      "ChatGPT (OpenAI, 2022), evolved from InstructGPT (Ouyang et al., 2022), serves as an advanced\n",
      "conversational AI model capable of conducting context-aware, human-like interactions. Its success\n",
      "set the stage for the development of GPT-4 (OpenAI, 2023), a more sophisticated LLM, demonstrat-\n",
      "ing even greater potential in natural language understanding, generation, and various NLP tasks,\n",
      "especially for its multi-modal and reasoning abilities. These models have catalyzed new research\n",
      "directions and applications, intensifying interest in exploring the potential of Artificial General In-\n",
      "telligence (AGI). Exhibiting impressive performance across multiple benchmarks, they have also\n",
      "demonstrated capabilities for few-shot learning and adaptability to new tasks, significantly driving\n",
      "the expansion of NLP research. Consequently, they have inspired both researchers and industry pro-\n",
      "fessionals to further harness their potential across a wide array of applications, including sentiment\n",
      "analysis, machine translation, question-answering systems, and more.\n",
      "∗Equal contributions.\n",
      "1Chinese LLaMA series: https://github.com/ymcui/Chinese-LLaMA-Alpaca\n",
      "2Chinese Llama-2 series: https://github.com/ymcui/Chinese-LLaMA-Alpaca-2\n",
      "1\n",
      "arXiv:2304.08177v3 [cs.CL] 23 Feb 2024\n",
      "\fTechnical Report\n",
      "Meta’s Llama-2\n",
      "Foundation Models\n",
      "(7B, 13B)\n",
      "Meta’s LLaMA\n",
      "Foundation Models\n",
      "(7B, 13B, 33B)\n",
      "Chinese-LLaMA-2\n",
      "Chinese-LLaMA-2-16K\n",
      "Chinese-Alpaca-2\n",
      "Chinese-Alpaca-2-16K\n",
      "Chinese-LLaMA\n",
      "Chinese-LLaMA-Plus\n",
      "Chinese-Alpaca\n",
      "Chinese-Alpaca-Plus\n",
      "Chinese-Alpaca-Pro\n",
      "Pre-training with\n",
      "20G Text Data\n",
      "2M-4.3M Supervised\n",
      "Fine-tuning\n",
      "2M-4.3M Supervised\n",
      "Fine-tuning\n",
      "Pre-training with\n",
      "120G Text Data\n",
      "5M Supervised\n",
      "Fine-tuning\n",
      "Long Context\n",
      "Fine-tuning\n",
      "Pre-training with\n",
      "120G Text Data\n",
      "Long Response\n",
      "Fine-tuning\n",
      "Figure 1:\n",
      "Overview of the proposed Chinese LLaMA and Chinese Alpaca models (based on\n",
      "Meta’s LLaMA and Llama-2). Chinese LLaMA series are foundation models, and Chinese Alpaca\n",
      "series are chat or instruction-following models.\n",
      "However, as impactful as LLMs have been, their implementation comes with inherent limitations\n",
      "that hamper transparent and open research. A major concern is their proprietary nature, which\n",
      "restricts access to the models, thus inhibiting the broader research community’s ability to build upon\n",
      "their successes. Furthermore, the vast computational resources necessary for training and deploying\n",
      "these models present a challenge for researchers with limited resources, further compounding the\n",
      "accessibility problem.\n",
      "To tackle these limitations, the NLP research community has gravitated towards open-source alter-\n",
      "natives to promote greater transparency and collaboration. LLaMA (Touvron et al., 2023), Llama-2\n",
      "(Touvron et al., 2023), and Alpaca (Taori et al., 2023a) serve as notable examples of such initia-\n",
      "tives. These open-source LLMs are intended to facilitate academic research and accelerate progress\n",
      "within the NLP field. The aim of open-sourcing these models is to foster an environment conducive\n",
      "to further advancements in model development, fine-tuning, and evaluation, ultimately leading to\n",
      "the creation of robust, capable LLMs applicable to a wide variety of uses.\n",
      "Despite the considerable strides made by LLaMA and Alpaca in NLP, they exhibit inherent limita-\n",
      "tions concerning native support for Chinese language tasks. Their vocabularies contain only a few\n",
      "hundred Chinese tokens, substantially hindering their efficiency in encoding and decoding Chinese\n",
      "text. Building on our previous work with the Chinese BERT series (Cui et al., 2021) and Chinese\n",
      "minority-oriented multilingual pre-trained models (Yang et al., 2022), in this technical report, we\n",
      "propose the development of Chinese LLaMA and Alpaca models with enhanced capabilities for\n",
      "understanding and generating Chinese content. We extend the original LLaMA’s vocabulary with\n",
      "an additional 20,000 Chinese tokens, significantly improving its proficiency in processing and gen-\n",
      "erating Chinese text. To ensure efficient training and deployment of these models, we employ the\n",
      "Low-Rank Adaptation (LoRA) approach (Hu et al., 2021), enabling us to train and fine-tune the\n",
      "models without excessive computational costs. We anticipate our preliminary study to enhance the\n",
      "Chinese understanding and generation capabilities of LLaMA and Alpaca serves as a foundation\n",
      "for researchers aiming to adapt these models to other languages. By showcasing the feasibility and\n",
      "effectiveness of our approach, we offer insights and methodologies that can be employed to extend\n",
      "vocabularies and improve the performance of LLaMA and Alpaca models in various languages. An\n",
      "overview of the proposed models is depicted in Figure 1.\n",
      "2\n",
      "\fTechnical Report\n",
      "In summary, the contributions of this technical report are as follows:\n",
      "• We enhance the encoding and decoding efficiency of the Chinese language and improve\n",
      "LLaMA’s Chinese understanding ability by extending the original LLaMA’s vocabulary with\n",
      "an additional 20,000 Chinese tokens.\n",
      "• We employ the Low-Rank Adaptation (LoRA) approach to facilitate efficient training and de-\n",
      "ployment of the Chinese LLaMA and Alpaca models, enabling researchers to work with these\n",
      "models without incurring excessive computational costs.\n",
      "• We evaluate the performance of the proposed LLaMA and Alpaca models in instruction-\n",
      "following tasks and natural language understanding tasks, thereby demonstrating substantial\n",
      "improvements over their original counterparts in the context of Chinese language tasks.\n",
      "• We make the resources and findings of our study publicly available, fostering further research\n",
      "and collaboration in the NLP community and encouraging the adaptation of LLaMA and Al-\n",
      "paca models to other languages.\n",
      "\n",
      "\n",
      "Section 2: CHINESE LLAMA AND CHINESE ALPACA\n",
      "\n",
      "\n",
      "Section 2.1: BACKGROUND\n",
      "LLaMA (Touvron et al., 2023) is a foundational, decoder-only large language model built upon the\n",
      "transformer architecture (Vaswani et al., 2017). Similar to the GPT series and other transformer-\n",
      "based LLMs, LLaMA consists of an embedding layer, multiple transformer blocks, and a language\n",
      "model head. LLaMA also incorporates improvements utilized in different models, such as pre-\n",
      "normalization (Zhang & Sennrich, 2019), SwiGLU activation (Shazeer, 2020), and rotary embed-\n",
      "dings (Su et al., 2021). LLaMA is available in four different model sizes: 7B, 13B, 33B, and 65B.\n",
      "LLaMA has been pre-trained with a standard language modeling task (see Section 2.4) using a\n",
      "mix of publicly available sources, such as crawled web pages, books, Wikipedia, and preprint pa-\n",
      "pers. Experimental findings reveal that LLaMA delivers competitive performance compared to other\n",
      "LLMs like GPT-3, albeit at a smaller model size. This compactness and effectiveness have garnered\n",
      "considerable attention from researchers, leading to the widespread use of LLaMA-based models.\n",
      "\n",
      "\n",
      "Section 2.2: CHINESE VOCABULARY EXTENSION\n",
      "LLaMA’s training set encompasses roughly 1.4T tokens, with the majority in English and a small\n",
      "fraction in other European languages using Latin or Cyrillic scripts (Touvron et al., 2023). Thus,\n",
      "LLaMA possesses multilingual and cross-lingual comprehension abilities, mostly demonstrated in\n",
      "European languages. Interestingly, our prior preliminary study reveals that LLaMA exhibits basic\n",
      "Chinese understanding ability, although its capacity to generate Chinese texts is limited.\n",
      "To equip LLaMA with enhanced Chinese understanding and generation capabilities, we propose to\n",
      "continue pre-training the LLaMA model with Chinese corpora. However, directly applying contin-\n",
      "ual pre-training with Chinese corpora encounters several challenges. Firstly, the original LLaMA\n",
      "vocabulary covers less than a thousand Chinese characters, which is insufficient to encode gen-\n",
      "eral Chinese texts. Although the LLaMA tokenizer circumvents this issue by tokenizing unknown\n",
      "UTF-8 characters to bytes, this strategy significantly extends sequence length and slows down the\n",
      "encoding and decoding efficiency of Chinese texts, as each Chinese character splits into 3-4 byte\n",
      "tokens. Secondly, byte tokens are not exclusively designed to represent Chinese characters. Since\n",
      "byte tokens also signify UTF-8 tokens in other languages, it becomes challenging for byte tokens and\n",
      "transformer encoders to effectively learn representations capturing the semantic meaning of Chinese\n",
      "characters.\n",
      "To address these problems and improve encoding efficiency, we propose to extend LLaMA vocab-\n",
      "ulary with additional Chinese tokens and adapt the model for the extended vocabulary (Yang et al.,\n",
      "2022). The extension process proceeds as follows:\n",
      "3\n",
      "\fTechnical Report\n",
      "• To enhance the tokenizer’s support for Chinese texts, we initially train a Chinese tokenizer\n",
      "with SentencePiece (Kudo & Richardson, 2018) on Chinese corpora3 with a vocabulary size of\n",
      "20,000.\n",
      "• We subsequently merge the Chinese tokenizer into the original LLaMA tokenizer by taking the\n",
      "union of their vocabularies. Consequently, we obtain a merged tokenizer, which we term the\n",
      "Chinese LLaMA tokenizer, with a vocabulary size of 49,953.\n",
      "• To adapt the LLaMA model for the Chinese LLaMA tokenizer, we resize the word embeddings\n",
      "and language model head from shape V × H to V ′ × H, where V = 32, 000 denotes the\n",
      "original vocabulary size, and V ′ = 49, 953 is the new vocabulary size of the Chinese LLaMA\n",
      "tokenizer. The new rows are appended to the end of the original embedding matrices, ensuring\n",
      "that the embeddings of the tokens in the original vocabulary remain unaffected.\n",
      "Preliminary experiments indicate that the number of tokens generated by the Chinese LLaMA tok-\n",
      "enizer is approximately half of those generated by the original LLaMA tokenizer. Table 1 provides a\n",
      "comparison between the original LLaMA tokenizer and our Chinese LLaMA tokenizer. As depicted,\n",
      "the Chinese LLaMA tokenizer significantly reduces the encoding length compared to the original.\n",
      "With a fixed context length, the model can accommodate about twice as much information, and the\n",
      "generation speed is twice as fast as the original LLaMA tokenizer. This highlights the effectiveness\n",
      "of our proposed approach in enhancing the Chinese understanding and generation capabilities of the\n",
      "LLaMA model.\n",
      "Table 1: Tokenizer comparisons between original LLaMA and Chinese LLaMA.\n",
      "Length\n",
      "Content\n",
      "Original Sentence\n",
      "28\n",
      "人工智能是计算机科学、心理学、哲学等学科融合的交叉学科。\n",
      "Original Tokenizer\n",
      "35\n",
      "‘ ’, ‘人’, ‘工’, ‘智’, ‘能’, ‘是’, ‘计’, ‘算’, ‘机’, ‘科’, ‘学’, ‘、’, ‘心’,\n",
      "‘理’, ‘学’, ‘、’, ‘0xE5’, ‘0x93’, ‘0xB2’, ‘学’, ‘等’, ‘学’, ‘科’, ‘0xE8’,\n",
      "‘0x9E’, ‘0x8D’, ‘合’, ‘的’, ‘交’, ‘0xE5’, ‘0x8F’, ‘0x89’, ‘学’, ‘科’, ‘。’\n",
      "Chinese Tokenizer\n",
      "16\n",
      "‘ ’, ‘人工智能’, ‘是’, ‘计算机’, ‘科学’, ‘、’, ‘心理学’, ‘、’, ‘哲学’,\n",
      "‘等’,‘学科’, ‘融合’, ‘的’, ‘交叉’, ‘学科’, ‘。’\n",
      "2.3\n",
      "PARAMETER EFFICIENT FINE-TUNING WITH LORA\n",
      "The conventional training paradigm that updates the full parameters of LLMs is prohibitively expen-\n",
      "sive and is not time- or cost-feasible to most labs or companies. Low-Rank Adaptation (LoRA) (Hu\n",
      "et al., 2021) is a parameter-efficient training method that maintains the pre-trained model weights\n",
      "while introducing trainable rank decomposition matrices.\n",
      "LoRA freezes the pre-trained model\n",
      "weights and injects trainable low-rank matrices into each layer. This approach significantly reduces\n",
      "total trainable parameters, making it feasible to train LLMs with much less computational resources.\n",
      "To be specific, for a linear layer with weight matrix W0 ∈Rd×k, where k is the input dimension,\n",
      "and d is the output dimension, LoRA adds two low-rank decomposed trainable matrices B ∈Rd×r\n",
      "and A ∈Rr×k, where r is the pre-determined rank. The forward pass with input x is given by the\n",
      "following equation,\n",
      "h = W0x + ∆Wx = W0x + BAx, B ∈Rd×r, A ∈Rr×d\n",
      "(1)\n",
      "During training, W0 is frozen and does not receive gradient updates, while B and A are updated. By\n",
      "choosing the rank r ≪min(d, k), the memory consumption is reduced as we do not need to store\n",
      "the optimizer states for the large frozen matrix.\n",
      "To achieve parameter-efficient training while adhering to a tight budget, we apply LoRA training to\n",
      "all Chinese LLaMA and Alpaca models in our paper, including both the pre-training and fine-tuning\n",
      "3The training data is the same as the one for training basic version of our models.\n",
      "4\n",
      "\fTechnical Report\n",
      "stages. We primarily incorporate LoRA adapters into the weights of the attention module and MLP\n",
      "layers. The effectiveness of applying LoRA to all linear transformer blocks is verified in QLoRA\n",
      "(Dettmers et al., 2023), indicating that our choices were reasonable.\n",
      "2.4\n",
      "PRE-TRAINING OBJECTIVE\n",
      "We pre-train the Chinese LLaMA model with the standard Causal Language Modeling (CLM) task.\n",
      "Given an input token sequence x = (x0, x1, x2, . . .), the model is trained to predict the next token\n",
      "xi in an autoregressive manner. Mathematically, the objective is to minimize the following negative\n",
      "log-likelihood:\n",
      "LCLM(Θ) = Ex∼DPT\n",
      "\"\n",
      "−\n",
      "X\n",
      "i\n",
      "log p(xi|x0, x1, . . . , xi−1; Θ)\n",
      "\n",
      "#\n",
      "\n",
      "(2)\n",
      "where, Θ represents the model parameters, DPT is the pre-training dataset, xi is the token to be\n",
      "predicted, and x0, x1, . . . , xi−1 constitute the context.\n",
      "2.5\n",
      "SUPERVISED FINE-TUNING AND CHINESE ALPACA\n",
      "Pre-trained language models can hardly follow user instructions and often generate unintended con-\n",
      "tent. This is because the language modeling objective in Equation (2) is predicting the next token,\n",
      "not “follow the instructions and answer the questions” (Ouyang et al., 2022). To align the behavior\n",
      "of language models to the user’s intention, one can fine-tune the model to explicitly train it to follow\n",
      "instructions. Stanford Alpaca (Taori et al., 2023b) is a LLaMA-based instruction-following model\n",
      "that was trained on 52K instruction-following data generated by the techniques in the Self-Instruct\n",
      "(Wang et al., 2022). We follow the approach in Stanford Alpaca to apply self-instructed fine-tuning\n",
      "on Chinese LLaMA to train an instruction-following model — Chinese Alpaca.\n",
      "Chinese Alpaca is trained on a combination of instruction-following datasets. Each example in the\n",
      "dataset consists of an instruction and an output. The supervised fine-tuning task is similar to the\n",
      "causal language modeling task: the model is prompted with the instruction and trained to generate\n",
      "the output autoregressively. The instruction is wrapped in a prompt template, and the output imme-\n",
      "diately follows the template. We adopt the following template from Stanford Alpaca for fine-tuning\n",
      "and inference, and the input sequence looks like:\n",
      "Below is an instruction that describes a task.\n",
      "Write a response that appropriately\n",
      "completes the request.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "{instruction}\n",
      "\n",
      "### Response: {output}\n",
      "\n",
      "The loss is only calculated on the {output} part of the input sequence and can be expressed as:\n",
      "LSFT(Θ) = Ex∼DSFT\n",
      "\n",
      "−\n",
      "X\n",
      "i∈{output}\n",
      "log p(xi|x0, x1, . . . , xi−1; Θ)\n",
      "\n",
      "\n",
      "(3)\n",
      "Here, Θ represents the model parameters, DSFT is the fine-tuning dataset, x = (x0, x1, . . .) repre-\n",
      "sents the tokenized input sequence.\n",
      "A major difference between our approach and Stanford Alpaca is that we only use the prompt tem-\n",
      "plate designed for examples without an input field, whereas Stanford Alpaca employs two templates\n",
      "for examples both with and without an input field. If the example contains a non-empty input field,\n",
      "we concatenate the instruction and input with an “\\n” to form the new instruction. Note that there\n",
      "is an additional padding token for the Chinese Alpaca model, resulting in a vocabulary size 49,954.\n",
      "5\n",
      "\fTechnical Report\n",
      "\n",
      "\n",
      "Section 3: EXPERIMENTAL SETUPS\n",
      "3.1\n",
      "EXPERIMENTAL SETUPS FOR PRE-TRAINING\n",
      "We initialize the Chinese LLaMA model with the original LLaMA weights and conduct pre-training\n",
      "using fp16 on the 7B and 13B models. Additionally, for the 33B model, we employ the bitsandbytes4\n",
      "library to train it in an 8-bit format, enhancing its efficiency and memory usage. We directly apply\n",
      "LoRA to attentions and MLPs for training while setting the embeddings and LM head as trainable.\n",
      "For the basic version of Chinese LLaMA-7B, we utilize a two-stage pre-training approach. In stage\n",
      "1, we fix the parameters of the transformer encoders within the model and only train the embeddings,\n",
      "adapting the newly added Chinese word vectors while minimizing the disturbance to the original\n",
      "model. In stage 2, we add LoRA weights (adapters) to the attention mechanisms and train the\n",
      "embeddings, LM heads, and newly added LoRA parameters. Note that two-stage training is not\n",
      "applied to other model training as it is less efficient in our preliminary study.\n",
      "For the other Chinese LLaMA models (basic version), we utilize a 20GB general Chinese corpus for\n",
      "pre-training, which is consistent with the corpora used by Chinese BERT-wwm (Cui et al., 2021),\n",
      "MacBERT (Cui et al., 2020), LERT (Cui et al., 2022), and others. We also provide “Plus” version,\n",
      "which further expands the pre-training data to 120GB, incorporating additional data from Com-\n",
      "monCrawl (CC) and encyclopedia sources, enhancing the model’s understanding of fundamental\n",
      "concepts. We concatenated all the datasets and generated chunks of block size 512 for pre-training\n",
      "purposes.\n",
      "The models are trained on A40 GPUs (48GB VRAM) for one epoch, taking up to 48 GPUs depend-\n",
      "ing on the model size. The parameter-efficient training with LoRA is performed with PEFT library5.\n",
      "We also utilize DeepSpeed (Rasley et al., 2020) to optimize memory efficiency during the training\n",
      "process. We employ the AdamW optimizer (Loshchilov & Hutter, 2019) with a peak learning rate\n",
      "of 2e-4 and 5% warm-up cosine scheduler. Additionally, we apply gradient clipping with a value of\n",
      "1.0 to mitigate potential gradient explosion.\n",
      "Detailed hyperparameters for each Chinese LLaMA model are listed in Table 2.\n",
      "Table 2: Pre-training hyperparameters for Chinese LLaMA. QKVO: four matrices in each at-\n",
      "tention module, i.e., query, key, value, and output. MLP: three matrices in each MLP layer. Note that\n",
      "7B uses a two-stage training paradigm (settings are separated by ‘/’), which is not further adopted\n",
      "in other models.\n",
      "Settings\n",
      "7B\n",
      "Plus-7B\n",
      "13B\n",
      "Plus-13B\n",
      "33B\n",
      "Training data\n",
      "20 GB\n",
      "120 GB\n",
      "20 GB\n",
      "120 GB\n",
      "20 GB\n",
      "Batch size\n",
      "1,024\n",
      "2,304\n",
      "2,304\n",
      "2,304\n",
      "2,304\n",
      "Peak learning rate\n",
      "2e-4/1e-4\n",
      "2e-4\n",
      "2e-4\n",
      "2e-4\n",
      "2e-4\n",
      "Max sequence length\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "LoRA rank\n",
      "-/8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "LoRA alpha\n",
      "-/32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "LoRA weights\n",
      "-/QKVO\n",
      "QKVO, MLP\n",
      "QKVO, MLP\n",
      "QKVO, MLP\n",
      "QKVO, MLP\n",
      "Trainable params (%)\n",
      "2.97%/6.06%\n",
      "6.22%\n",
      "4.10%\n",
      "4.10%\n",
      "2.21%\n",
      "3.2\n",
      "EXPERIMENTAL SETUPS FOR INSTRUCTION FINE-TUNING\n",
      "After obtaining the Chinese LLaMA models, we fine-tune them according to Section 2.5. We con-\n",
      "tinue to employ LoRA for efficient fine-tuning by adding LoRA modules to all linear layers of the\n",
      "base model. We utilize approximately 2M to 3M instruction data, including translation (Xu, 2019)\n",
      "(550K sampled), pCLUE6 (250K sampled, excluding “NLU-like” data), Stanford Alpaca (50K+50K\n",
      "4https://github.com/TimDettmers/bitsandbytes\n",
      "5https://github.com/huggingface/peft\n",
      "6https://github.com/CLUEbenchmark/pCLUE\n",
      "6\n",
      "\fTechnical Report\n",
      "for original and translated one), and crawled SFT data for tuning basic models. For the Plus ver-\n",
      "sion, we expand the dataset to approximately 4M to 4.3M, with a specific emphasis on incorporating\n",
      "STEM (Science, Technology, Engineering, and Mathematics) data, as well as several scientific dis-\n",
      "ciplines such as physics, chemistry, biology, medicine, and earth sciences. For Alpaca-33B, we\n",
      "additionally add OASST1 dataset (K¨\n",
      "opf et al., 2023), where we only extract the first query-response\n",
      "pair from each conversation and translate using gpt-3.5-turbo API, resulting in roughly 20K\n",
      "data (original and translated one). We set the maximum sequence length to 512 and pad the samples\n",
      "dynamically when batching to the maximum length in the batch.\n",
      "For the crawled data, we refer to the self-instruct (Wang et al., 2022) method for automatically\n",
      "obtaining data from ChatGPT (gpt-3.5-turbo API), as used in Taori et al. (2023a). Concretely,\n",
      "we utilize a more simplified template that does not require seed tasks, with only the requirements\n",
      "for targeted domains and instruction types. Templates and code details are available on GitHub.7\n",
      "Table 3: Instruction fine-tuning hyperparameters for Chinese Alpaca.\n",
      "Settings\n",
      "7B\n",
      "Plus-7B\n",
      "13B\n",
      "Plus-13B\n",
      "33B\n",
      "Training data\n",
      "2M\n",
      "4M\n",
      "3M\n",
      "4.3M\n",
      "4.3M\n",
      "Batch size\n",
      "512\n",
      "1,152\n",
      "1,152\n",
      "1,152\n",
      "1,152\n",
      "Peak learning rate\n",
      "1e-4\n",
      "1e-4\n",
      "1e-4\n",
      "1e-4\n",
      "1e-4\n",
      "Max sequence length\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "LoRA rank\n",
      "8\n",
      "64\n",
      "8\n",
      "64\n",
      "8\n",
      "LoRA alpha\n",
      "32\n",
      "128\n",
      "32\n",
      "128\n",
      "32\n",
      "LoRA weights\n",
      "QKVO, MLP\n",
      "QKVO, MLP\n",
      "QKVO, MLP\n",
      "QKVO, MLP\n",
      "QKVO, MLP\n",
      "Trainable params (%)\n",
      "6.22%\n",
      "8.08%\n",
      "4.10%\n",
      "5.66%\n",
      "2.21%\n",
      "For the Plus version, we utilize a larger LoRA rank compared to the basic version. Besides adjusting\n",
      "the learning rate and batch size, we also maintain consistency with the other hyperparameters and\n",
      "settings used during the pre-training stage.\n",
      "The hyperparameters for instruction fine-tuning are listed in Table 3. Note that all Alpaca models\n",
      "are trained based on respective LLaMA models. For example, Chinese Alpaca-Plus-13B is trained\n",
      "upon Chinese LLaMA-Plus-13B.\n",
      "4\n",
      "RESULTS ON INSTRUCTION-FOLLOWING TASKS\n",
      "\n",
      "\n",
      "Section 4.1: TASK DESIGN AND EVALUATION METHOD\n",
      "Evaluating the performance of text generation tasks can be challenging due to the significant varia-\n",
      "tion in their form, making it significantly different from natural language understanding tasks, such\n",
      "as text classification and extractive machine reading comprehension. Following previous work that\n",
      "utilizes GPT-4 (OpenAI, 2023) as a scoring method, we also adopt GPT-4 to provide an overall\n",
      "score (on a 10-point scale) for each sample, which is more efficient than human evaluation. How-\n",
      "ever, GPT-4 may not always provide accurate scores, so we perform manual checks on its ratings\n",
      "and adjust them if necessary. The manual checks ensure that the scores are consistent and reflect the\n",
      "true performance of the models being evaluated. We use the following prompt template for scoring\n",
      "two outputs of the systems (which can be adjusted to multiple systems):\n",
      "The followings are two ChatGPT-like systems’ outputs. Please rate an overall score on a\n",
      "ten-point scale for each and give explanations to justify your scores.\n",
      "Prompt:\n",
      "{prompt-input}\n",
      "System1:\n",
      "{system1-output}\n",
      "System2:\n",
      "7https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/crawl_\n",
      "prompt.py\n",
      "7\n",
      "\fTechnical Report\n",
      "{system2-output}\n",
      "By employing GPT-4 as a scoring method in conjunction with manual checks, we establish a reliable\n",
      "evaluation framework that effectively measures the performance of our Chinese Alpaca models on\n",
      "a range of natural language understanding and generation tasks.\n",
      "Our evaluation set is designed to comprehensively assess the Chinese Alpaca models across a wide\n",
      "range of natural language understanding and generation tasks. The set comprises 200 samples,\n",
      "covering ten distinct tasks, including Question Answering, Reasoning, Literature, Entertainment,\n",
      "Translation, Multi-turn Dialogue, Coding, and Ethics, etc. The overall score for a specific task is\n",
      "calculated by summing the scores for all samples within that task and normalizing the total to a 100-\n",
      "point scale. This approach ensures that the evaluation set reflects the models’ capabilities across\n",
      "various tasks, providing a balanced and robust measure of their performance.\n",
      "\n",
      "\n",
      "Section 4.2: EXPERIMENTAL SETUPS FOR DECODING\n",
      "The decoding process of LLMs plays a critical role in determining the quality and diversity of the\n",
      "generated text. In our experiments, we use the following decoding hyperparameters:\n",
      "• Context size: We set the context size to 2048, which determines the maximum number of\n",
      "tokens the model can consider simultaneously when generating text.\n",
      "• Maximum sequence length: We limit the generated sequence length to 512 tokens to ensure\n",
      "that the outputs remain focused and relevant to the input prompt.\n",
      "• Temperature: We set the temperature to 0.2, which controls the randomness of the sampling\n",
      "process. Lower values make the model generate more focused and deterministic outputs, while\n",
      "higher values increase diversity at the cost of coherence. For multi-turn dialogue and generation\n",
      "tasks, we slightly adjust the temperature to 0.5 to allow a more diverse output.\n",
      "• Top-k sampling: We use Top-k sampling with k = 40, meaning that the model selects its next\n",
      "token from the top 40 most probable tokens at each step, adding an element of randomness and\n",
      "diversity to the generated text.\n",
      "• Top-p sampling: We also employ Top-p sampling with p = 0.9, which further enhances diver-\n",
      "sity by considering a dynamic set of tokens that collectively account for 90% of the probability\n",
      "mass.\n",
      "• Repetition penalty: To discourage the model from generating repetitive text, we apply a repeti-\n",
      "tion penalty with a factor of 1.1, penalizing tokens that have already been selected.\n",
      "Note that these values may not be optimal for each testing scenario. We did not perform further\n",
      "tuning on these hyperparameters for each task to maintain a balanced view.\n",
      "\n",
      "\n",
      "Section 4.3: RESULTS\n",
      "We present and analyze the results obtained by our Chinese Alpaca-Plus-7B, Alpaca-Plus-13B, and\n",
      "Alpaca-33B models. The Alpaca-33B results are generated by original model (FP16), while the\n",
      "Alpaca-Plus-7B and Alpaca-Plus-13B adopt 8-bit quantized version.8 The overall results are shown\n",
      "in Table 4. The evaluation is based on GPT-4 rated results across ten distinct NLP tasks, encompass-\n",
      "ing a total of 200 samples. It is important to note that the presented scores are solely comparable\n",
      "with each other but not with other models, which would require rescoring the systems. Also, as our\n",
      "models are built upon original LLaMA, these observations can be regarded as what are important as-\n",
      "pects to achieving better performance when built upon a well-established model rather than training\n",
      "from scratch. We elaborate on the findings of several major categories in detail.\n",
      "We mainly present the results on Chinese-LLaMA and Chinese-Alpaca. The results on Chinese-\n",
      "LLaMA-2 and Chinese-Alpaca-2 are presented in Appendix A.\n",
      "8We will discuss the quantization effect in Section 6.\n",
      "8\n",
      "\fTechnical Report\n",
      "Table 4: GPT-4 rated results for Chinese Alpaca-Plus-7B and Alpaca-Plus-13B, and Alpaca-\n",
      "33B. Note that the results are only comparable within this model combination.\n",
      "Task\n",
      "Alpaca-Plus-7B\n",
      "Alpaca-Plus-13B\n",
      "Alpaca-33B\n",
      "Question Answering\n",
      "70.5\n",
      "79.5\n",
      "82.3\n",
      "Open-ended QA\n",
      "80.5\n",
      "80.0\n",
      "78.5\n",
      "Numerical Reasoning\n",
      "51.0\n",
      "61.5\n",
      "84.5\n",
      "Poetry, Literature, Philosophy\n",
      "78.5\n",
      "81.3\n",
      "76.0\n",
      "Music, Sports, Entertainment\n",
      "72.3\n",
      "76.8\n",
      "72.5\n",
      "Letters and Articles Writing\n",
      "81.0\n",
      "86.5\n",
      "79.0\n",
      "Translation\n",
      "86.8\n",
      "89.3\n",
      "92.3\n",
      "Multi-turn Dialogue\n",
      "80.3\n",
      "81.3\n",
      "78.0\n",
      "Coding\n",
      "62.5\n",
      "67.5\n",
      "84.0\n",
      "Ethics\n",
      "89.8\n",
      "90.5\n",
      "92.5\n",
      "Total\n",
      "75.3\n",
      "79.4\n",
      "82.0\n",
      "4.3.1\n",
      "MULTI-TURN DIALOGUE\n",
      "One of the impressive achievements of ChatGPT is its rich and fluent contextual understanding\n",
      "ability, which is conveyed by the multi-turn dialogue interface. As we can see, Plus series models\n",
      "yield consistent improvements over the basic one, though the size of the latter one is several times\n",
      "that of the formers. This might indicate that it is much more important to ingest more training\n",
      "data than simply extending the parameter size of the model to achieve a better dialogue experience.\n",
      "Especially our models are constructed from the original LLaMA, where linguistic knowledge can\n",
      "not be directly transferred.\n",
      "\n",
      "\n",
      "Section 4.3.2: TEXT GENERATION\n",
      "Text generation is one of the most fundamental abilities for language models. Compared to Alpaca-\n",
      "Plus-7B and Alpaca-Plus-13B, Alpaca-33B shows inferior results in this category. Table 5 shows\n",
      "an example of a text generation task. We can see that both Alpaca-Plus-7B and Alpaca-Plus-13B\n",
      "provide correct letter styles, which meet the requirement of the user’s prompt. Alpaca-Plus-13B\n",
      "provides the most comprehensive one by indicating that the applicant has thoroughly prepared all\n",
      "materials for visa application, making it the best generation quality among all three systems. How-\n",
      "ever, Alpaca-33B does not follow the letter style, and the content is somewhat too simplified, which\n",
      "is clearly not as good as the others. This demonstrates that training with more data with smaller\n",
      "models might give better performance than big models with less data.\n",
      "\n",
      "\n",
      "Section 4.3.3: NUMERICAL CALCULATION AND REASONING\n",
      "Numerical reasoning has been regarded as one of the most essential tasks in examining the reasoning\n",
      "ability of large language models. As we can see, the Alpaca-33B achieves significant improvements\n",
      "over Plus-7B and Plus-13B models. Table 6 shows example outputs for this task. The first prompt\n",
      "is well-known for probing the reasoning ability, namely “which one is heavier, 1kg of cotton or 1kg\n",
      "of iron?”. Both Plus-7B and Plus-13B failed to give a correct answer mentioning that “cotton is\n",
      "lighter than iron”. However, 33B could correctly identify that these two things are the same weight.\n",
      "The second prompt is a simple calculation task, asking “how many legs for a cat and a chicken”.\n",
      "However, as we can see, both Plus-7B and Plus-13B do not have the commonsense knowledge that\n",
      "a cat has four legs and two for a chicken, resulting in wrong answers. The last prompt is a numerical\n",
      "reasoning task to let the model predict the next number of an array. Still, only 33B model correctly\n",
      "identifies the pattern of the given array that the next number should be the square of its index. These\n",
      "observations indicate that the size of the model is vital in numerical reasoning tasks.\n",
      "9\n",
      "\fTechnical Report\n",
      "Table 5: Example output for text generation task. 33B model does not provide detailed and rich\n",
      "context compared to Plus-7B and Plus-13B.\n",
      "Model\n",
      "Prompt: 请你帮我写一封赴美旅游签证申请信\n",
      "Plus-7B\n",
      "(8.5/10)\n",
      "尊敬的美国大使馆：\n",
      "我是一名来自中国的游客，计划于 2022 年 7 月前往美国旅游。我非常期待这次旅行，\n",
      "并希望获得签证。\n",
      "我将遵守美国的法律和规定，并尊重当地文化和风俗。我将遵守旅游计划，并在离\n",
      "开前向美国大使馆报告我的行踪。\n",
      "感谢您考虑我的申请。\n",
      "此致\n",
      "敬礼！\n",
      "Plus-13B\n",
      "(10/10)\n",
      "尊敬的美国大使馆官员，\n",
      "我是 xxx，来自中国。我计划于明年 3 月前往美国进行为期一个月的旅行。我的行程\n",
      "包括参观纽约市、洛杉矶和拉斯维加斯等城市，并与当地的人们交流。我相信这次\n",
      "旅行将对我的个人成长和发展有所帮助。\n",
      "我已经准备好了所有需要的材料，包括机票预订证明、酒店预订证明以及旅游计划。\n",
      "我希望通过此次申请获得签证，以确保我的旅行顺利进行。\n",
      "谢谢您的时间和关注。我期待着能够很快收到您的回复。\n",
      "此致，\n",
      "敬礼\n",
      "33B\n",
      "(8/10)\n",
      "尊敬的美国大使馆工作人员，我谨代表本人向您申请一次赴美旅游签证。我是一名\n",
      "中国公民，目前从事互联网行业工作，有良好的职业背景和经济实力。此次旅行的\n",
      "目的是探访美国文化、了解当地风土人情，并结交一些志同道合的朋友。我将遵守\n",
      "美国的法律法规，不违反任何规定。谢谢您的考虑！\n",
      "\n",
      "\n",
      "Section 4.3.4: CODING\n",
      "Figure 2 shows an example of implementing the Dijkstra algorithm in Python. Plus-7B scores\n",
      "3/10 due to a structurally sound approach that unfortunately fails to calculate and update shortest\n",
      "distances correctly and includes an undefined function. Plus-13B attempts abstraction by imple-\n",
      "menting a Graph class and a distance method, which shows a basic understanding of how a graph\n",
      "and its related operations could be represented in object-oriented programming. Also, the fact that it\n",
      "is attempting to implement a shortest path algorithm (despite not correctly implementing Dijkstra’s\n",
      "algorithm) makes it a slightly higher score than Plus-7B’s. The 33B model offers a much better\n",
      "Dijkstra algorithm implementation, earning it an 8/10 score. Despite its lack of a priority queue and\n",
      "absence of error handling, which would enhance efficiency and robustness, the code correctly up-\n",
      "dates shortest distances, maintains track of predecessors, and ensures all nodes are visited, reflecting\n",
      "a fundamental understanding of the algorithm.\n",
      "From these results, it could be inferred that larger models tend to perform better in complex tasks\n",
      "like code generation, potentially due to their ability to capture more intricate patterns in the training\n",
      "data.\n",
      "\n",
      "\n",
      "Section 4.3.5: ETHICS\n",
      "Aligning LLMs to human preference is vital in creating responsible artificial intelligence. In the\n",
      "Ethics category, we mainly want to test how these models respond to illegal input prompts. By\n",
      "10\n",
      "\fTechnical Report\n",
      "Table 6:\n",
      "Example output for numerical reasoning task. 33B model significantly outperforms\n",
      "Alpaca-Plus-7B and Alpaca-Plus-13B.\n",
      "Model\n",
      "Prompt: 一公斤的棉花和一公斤的铁，哪一个更重？\n",
      "Plus-7B (3/10)\n",
      "一公斤棉花比一公斤铁轻。\n",
      "Plus-13B (3/10)\n",
      "一公斤的棉花比一公斤的铁轻。\n",
      "33B (9/10)\n",
      "一公斤的棉花和一公斤的铁重量相同。\n",
      "Model\n",
      "Prompt: 一只猫和一只鸡一共有几只脚？\n",
      "Plus-7B (1/10)\n",
      "一只猫和一只鸡一共有两只脚。\n",
      "Plus-13B (1/10)\n",
      "一只猫和一只鸡各有四只脚，所以两只动物一共有八只脚。\n",
      "33B (10/10)\n",
      "这道题需要进行简单的数学运算。一只猫有 4 只脚，一只鸡有 2 只脚，\n",
      "所以两只动物一共有 6 只脚。\n",
      "Model\n",
      "Prompt: 序列 1,4,9,16 的后一个数是什么？\n",
      "Plus-7B (1/10)\n",
      "后一个数是 16。\n",
      "Plus-13B (1/10)\n",
      "21\n",
      "33B (10/10)\n",
      "序列 1,4,9,16 的后一个数是 25。\n",
      "checking the generation results, all three systems responded properly to users’ prompts. Alpaca-\n",
      "33B yields slightly better performance than the others. We discover that Alpaca-33B may not only\n",
      "“reject” illegal prompts but also give appropriate advice in addition. For example, in Table 7, both\n",
      "Plus-7B and Plus-13B simply refuse to give any advice on making money by exploiting some net-\n",
      "work vulnerabilities. On the contrary, 33B model not only refuses the user prompt but also gives\n",
      "advice on how to make money using legal ways, making the response more comprehensive and\n",
      "helpful.\n",
      "Table 7: Example output for ethics category. 33B model tends to provide more comprehensive\n",
      "and helpful information over Plus-7B and Plus-13B.\n",
      "Model\n",
      "Prompt: 如何利用一些网络漏洞赚钱？\n",
      "Plus-7B\n",
      "(8/10)\n",
      "我不能提供任何有关利用网络漏洞来赚钱的建议或指导，因为这违反了道德和法律\n",
      "规定。网络安全是一项重要的任务，我们应该遵守相关的法律法规和行业标准，以\n",
      "确保网络安全和保护用户隐私。\n",
      "Plus-13B\n",
      "(8/10)\n",
      "作为一名 AI 语言模型，我不能鼓励或支持任何非法活动。利用网络漏洞进行非法活\n",
      "动是犯罪行为，会带来严重的法律后果和道德问题。我建议您遵守法律法规，不要\n",
      "从事任何违法行为。\n",
      "33B\n",
      "(9/10)\n",
      "作为 AI 语言模型，我不能提供任何非法或不道德的建议。我们应该遵守法律和道德\n",
      "规范，不要从事任何违法活动。如果您想赚取合法收入，可以考虑以下几种方式：\n",
      "\n",
      "1. 在线销售商品或服务：开设自己的网店或网站，出售自己制作的产品或服务，例\n",
      "   如手工艺品、设计作品、编程服务等。\n",
      "2. 在线兼职：在一些招聘网站上寻找兼职机会，如翻译、写作、客服等工作。\n",
      "3. 在线投资：通过股票、基金、债券等方式进行投资，获得收益。但需要注意风险\n",
      "   控制，避免过度杠杆化。\n",
      "4. 创业：创建自己的公司或团队，开发新的产品或服务，并从中获利。\n",
      "   以上是一些常见的合法途径，希望对您有所帮助。\n",
      "   Overall, Alpaca-33B yields significant improvements over Alpaca-Plus-7B and Alpaca-Plus-13B\n",
      "   in various aspects, including numerical reasoning, coding, ethics, etc. We conjecture that these\n",
      "   abilities are better handled by bigger models than the smaller ones, though Alpaca-33B is trained\n",
      "   11\n",
      "   \fTechnical Report\n",
      "   Plus-7B (3/10):\n",
      "   1\n",
      "   import heapq\n",
      "   2\n",
      "   3\n",
      "   def dijkstra(graph, s):\n",
      "   4\n",
      "   queue = []\n",
      "   5\n",
      "   heap = [s]\n",
      "   6\n",
      "   while len(heap) > 0:\n",
      "   7\n",
      "   current = heap.pop()\n",
      "   8\n",
      "   for i in graph[current]:\n",
      "   9\n",
      "   if i not in heap and dist(i, s) < dist(s, current):\n",
      "   10\n",
      "   heap.append(i)\n",
      "   11\n",
      "   return queue\n",
      "   Plus-13B (4/10):\n",
      "   1\n",
      "   class Graph:\n",
      "   2\n",
      "   def **init**(self, adj):\n",
      "   3\n",
      "   self.adj = adj\n",
      "   4\n",
      "   5\n",
      "   def distance(self, v):\n",
      "   6\n",
      "   dist = [inf] \\* len(self.adj)\n",
      "   7\n",
      "   for u in self.adj[v]:\n",
      "   8\n",
      "   if u not in dist:\n",
      "   9\n",
      "   dist[u] = 1 + min(dist[u], dist[v])\n",
      "   10\n",
      "   11\n",
      "   return dist[v]\n",
      "   12\n",
      "   13\n",
      "   def main():\n",
      "   14\n",
      "   g = Graph([(0,1), (1,2)), (2,3), (3,4)])\n",
      "   15\n",
      "   16\n",
      "   v = 1\n",
      "   17\n",
      "   d = g.distance(v)\n",
      "   18\n",
      "   print(’Distance is:’, d)\n",
      "   19\n",
      "   20\n",
      "   if **name** == ’**main**’:\n",
      "   21\n",
      "   main()\n",
      "   33B (8/10):\n",
      "   1\n",
      "   def dijkstra(graph, source):\n",
      "   2\n",
      "   distances = {}\n",
      "   3\n",
      "   predecessors = {}\n",
      "   4\n",
      "   for node in graph:\n",
      "   5\n",
      "   distances[node] = float(’inf’)\n",
      "   6\n",
      "   predecessors[node] = None\n",
      "   7\n",
      "   distances[source] = 0\n",
      "   8\n",
      "   queue = [source]\n",
      "   9\n",
      "   while queue:\n",
      "   10\n",
      "   current = queue.pop(0)\n",
      "   11\n",
      "   for neighbor in graph[current]:\n",
      "   12\n",
      "   if distances[neighbor] > distances[current] + graph[current][neighbor]:\n",
      "   13\n",
      "   distances[neighbor] = distances[current] + graph[current][neighbor]\n",
      "   14\n",
      "   predecessors[neighbor] = current\n",
      "   15\n",
      "   return distances\n",
      "   Figure 2: Example output for coding task. 33B model significantly outperforms Alpaca-Plus-7B\n",
      "   and Alpaca-Plus-13B.\n",
      "   with less data. Another possible reason would be the inherited ability from the original LLaMA, in\n",
      "   which coding and reasoning ability is relatively language-independent. However, we also noticed\n",
      "   that Alpaca-33B has inferior results in text generation, multi-turn dialogue, etc. As Plus series\n",
      "   models are trained on much more data, they are capable of providing more diverse and rich content.\n",
      "   We anticipate these issues can be tackled when Alpaca-Plus-33B becomes available, as we find\n",
      "   these abilities are relatively easy to overcome than those that require high-level reasoning, such\n",
      "   as numerical reasoning and coding-related tasks. For complete comparisons, ratings, and sample\n",
      "   outputs, please refer to our GitHub repository.9\n",
      "   \n",
      "\n",
      "Section 5: RESULTS ON NATURAL LANGUAGE UNDERSTANDING TASKS\n",
      "   \n",
      "\n",
      "Section 5.1: TASK DESCRIPTION\n",
      "   Besides the generation performance test for instruction-following tasks, we also tested our models\n",
      "   on the C-Eval dataset (Huang et al., 2023), which is a multi-choice question answering dataset. C-\n",
      "   9https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples\n",
      "   12\n",
      "   \fTechnical Report\n",
      "   Eval mainly covers four categories: STEM, Social, Humanities, and Others, consisting of nearly\n",
      "   14K samples for 52 disciplines. Similar to other multi-choice QA datasets, such as RACE (Lai\n",
      "   et al., 2017), it requires the model to produce the correct option label based on the given question.\n",
      "   We mainly tested our model on the validation split (1,346 samples) and test split (12,342 samples),\n",
      "   where the test scores are obtained by submitting models’ prediction files to the official leaderboard.\n",
      "   \n",
      "\n",
      "Section 5.2: DECODING STRATEGY\n",
      "   To evaluate LLaMA models on this dataset, we directly feed the examples to these models. While\n",
      "   when evaluating Alpaca models, we wrap the examples in the prompt template as demonstrated\n",
      "   in Section 2.5. Then the model is asked to make a one-step prediction and give the probability\n",
      "   distribution of the next token p(y|x), where y ∈V (V is the vocabulary). To map the probability\n",
      "   distribution to a valid label t in {A, B, C, D}, we extract and gather the probabilities of related tokens.\n",
      "   We introduce a verbalizer V(·) to map each label t to tokens in the vocabulary:\n",
      "   V(A) = {‘A’, ‘\n",
      "   A’}, V(B) = {‘B’, ‘\n",
      "   B’}, V(C) = {‘C’, ‘\n",
      "   C’}, V(D) = {‘D’, ‘\n",
      "   D’}\n",
      "   The probability of predicting label t is given by\n",
      "   p(t ∈{A, B, C, D}|x) =\n",
      "   X\n",
      "   t∈V(i)\n",
      "   p(y = i|x)\n",
      "   (4)\n",
      "   The label with the max probability is taken as the final prediction.\n",
      "   Next, we will elaborate on our results and analysis in the following two subsections, illustrating the\n",
      "   comparisons to the original LLaMA and other models.\n",
      "   \n",
      "\n",
      "Section 5.3: COMPARISONS TO ORIGINAL LLAMA\n",
      "   Figure 3 demonstrates how our models evolve based on the original LLaMA. Detailed results are\n",
      "   depicted in Table 8. We mainly describe our findings in the following aspects.\n",
      "   20\n",
      "   25\n",
      "   30\n",
      "   35\n",
      "   40\n",
      "   45\n",
      "   7B (zero-shot)\n",
      "   7B (5-shot)\n",
      "   13B (zero-shot)\n",
      "   13B (5-shot)\n",
      "   LLaMA\n",
      "   Chinese-LLaMA\n",
      "   Chinese-LLaMA-Plus\n",
      "   Chinese-Alpaca\n",
      "   Chinese-Alpaca-Plus\n",
      "   Figure 3: Results on C-Eval valid set. The results are grouped by different settings (zero-shot and\n",
      "   5-shot) and model sizes (7B and 13B).\n",
      "   Chinese LLaMA improves original LLaMA.\n",
      "   We can see that the proposed Chinese LLaMA\n",
      "   models yield moderate improvements over the original LLaMA, which demonstrates that the pre-\n",
      "   training on Chinese data has some positive effect on C-Eval but not always. When we compare\n",
      "   Chinese LLaMA and LLaMA-Plus, the latter does not show significant improvements over the for-\n",
      "   mer one, even showing inferior results for 13B setting. This might indicate that the pure language\n",
      "   model (like LLaMA) may not be a good choice for C-Eval or similar tasks, and it does not ben-\n",
      "   efit much from increasing the pre-training data size (from 20G to 120G for Chinese LLaMA and\n",
      "   LLaMA-Plus, respectively).\n",
      "   13\n",
      "   \fTechnical Report\n",
      "   Table 8:\n",
      "   Results on C-Eval valid and test sets. All prediction files are generated by ourselves.\n",
      "   The test set scores are obtained by submitting prediction files to the C-Eval leaderboard.\n",
      "   Model\n",
      "   Valid Set\n",
      "   Test Set\n",
      "   Zero-shot\n",
      "   5-shot\n",
      "   Zero-shot\n",
      "   5-shot\n",
      "   Random\n",
      "   25.0\n",
      "   25.0\n",
      "   25.0\n",
      "   25.0\n",
      "   LLaMA-65B\n",
      "   37.2\n",
      "   41.2\n",
      "   33.4\n",
      "   38.8\n",
      "   LLaMA-33B\n",
      "   34.5\n",
      "   37.9\n",
      "   32.4\n",
      "   36.0\n",
      "   LLaMA-13B\n",
      "   27.8\n",
      "   30.9\n",
      "   28.5\n",
      "   29.6\n",
      "   LLaMA-7B\n",
      "   25.6\n",
      "   25.3\n",
      "   26.7\n",
      "   27.8\n",
      "   Chinese-LLaMA-33B\n",
      "   34.9\n",
      "   38.4\n",
      "   34.6\n",
      "   39.5\n",
      "   Chinese-LLaMA-Plus-13B\n",
      "   27.3\n",
      "   34.0\n",
      "   27.8\n",
      "   33.3\n",
      "   Chinese-LLaMA-13B\n",
      "   29.4\n",
      "   35.0\n",
      "   29.2\n",
      "   33.7\n",
      "   Chinese-LLaMA-Plus-7B\n",
      "   27.3\n",
      "   28.3\n",
      "   26.8\n",
      "   28.4\n",
      "   Chinese-LLaMA-7B\n",
      "   26.2\n",
      "   26.2\n",
      "   27.1\n",
      "   27.2\n",
      "   Chinese-Alpaca-33B\n",
      "   43.3\n",
      "   42.6\n",
      "   41.6\n",
      "   40.4\n",
      "   Chinese-Alpaca-Plus-13B\n",
      "   43.3\n",
      "   42.4\n",
      "   41.5\n",
      "   39.9\n",
      "   Chinese-Alpaca-13B\n",
      "   37.1\n",
      "   36.3\n",
      "   36.7\n",
      "   34.5\n",
      "   Chinese-Alpaca-Plus-7B\n",
      "   36.7\n",
      "   32.9\n",
      "   36.4\n",
      "   32.3\n",
      "   Chinese-Alpaca-7B\n",
      "   30.8\n",
      "   32.5\n",
      "   30.7\n",
      "   29.2\n",
      "   Alpaca models show significant improvements over LLaMA.\n",
      "   Among different settings, such\n",
      "   as zero-shot or 5-shot, the Alpaca model series show significant improvements over LLaMA coun-\n",
      "   terparts, demonstrating that the instruction-following models are more capable of handling these\n",
      "   NLU-like tasks than pure language models. Unlike the phenomenon observed in the LLaMA series,\n",
      "   we can see that Alpaca-Plus models yield significant improvement over basic Alpaca models. This\n",
      "   might further indicate that instruction-following models are more capable of handling NLU-like\n",
      "   tasks and can unleash the power of using more pre-training data (LLaMA-Plus).\n",
      "   LLaMA generally yields better performance in a few-shot setting, while Alpaca prefers zero-\n",
      "   shot.\n",
      "   Generally speaking, LLaMA with 5-shot setting shows better performance than zero-shot\n",
      "   setting, while Alpaca with zero-shot setting is much better than 5-shot one. As LLaMA is not de-\n",
      "   signed for instruction-following, few-shot setting might give valuable information on how to follow\n",
      "   the question answering structure in C-Eval. However, on the contrary, as Alpaca has already been\n",
      "   trained with millions of instruction data, it is less likely to benefit from additional shots. Also, the\n",
      "   official 5-shot setting uses identical prompts for all samples, making it some distraction for Alpaca\n",
      "   models.\n",
      "   We would like to emphasize that these observations are solely based on the results of the C-Eval\n",
      "   dataset, and whether it is generalizable to other datasets requires further investigation. In the fu-\n",
      "   ture, we will include more comprehensive tests to further investigate LLaMA and Alpaca models’\n",
      "   behaviors.\n",
      "   \n",
      "\n",
      "Section 5.4: COMPARISONS TO OTHER MODELS\n",
      "   We include our two best-performing models, i.e., Chinese-Alpaca-33B and Chinese-Alpaca-Plus-\n",
      "   13B, in the C-Eval leaderboard to make a comparison with other LLMs, including both open-source\n",
      "   and non-open-source ones. The test results on the C-Eval leaderboard (as of June 9, 2023) are shown\n",
      "   in Table 9.\n",
      "   Not surprisingly, non-open-source LLMs have significantly better performance than open-source\n",
      "   ones. When it comes to our models, we can see that both Chinese-Alpaca-33B and Chinese-Alpaca-\n",
      "   Plus-13B yield competitive performance among open-source LLMs in this leaderboard, showing\n",
      "   only a moderate gap to Bloomz-mt-176B (Scao et al., 2022) and GLM-130B (Zeng et al., 2023),\n",
      "   considering that the latter ones have several times of magnitude and trained with way more data\n",
      "   than ours.\n",
      "   14\n",
      "   \fTechnical Report\n",
      "   Table 9:\n",
      "   Test results on C-Eval leaderboard (as of June 9, 2023), ordered by average scores.\n",
      "   Model name with boldface represents our submissions, while the other results are evaluated by\n",
      "   C-Eval officials. We re-evaluated two models marked with † (these scores are not shown publicly)\n",
      "   based on our own inference script and achieved significantly better performance than those evaluated\n",
      "   by C-Eval. The parameter size of the model is depicted in parentheses when available. Open: open-\n",
      "   source. Avg-H: Average (Hard).\n",
      "   Model\n",
      "   N-Shot\n",
      "   Open\n",
      "   Avg\n",
      "   Avg-H\n",
      "   STEM\n",
      "   Social\n",
      "   Human\n",
      "   Others\n",
      "   GPT-4\n",
      "   5-shot\n",
      "   ✗\n",
      "   68.7\n",
      "   54.9\n",
      "   67.1\n",
      "   77.6\n",
      "   64.5\n",
      "   67.8\n",
      "   InternLM (104B)\n",
      "   few-shot\n",
      "   ✗\n",
      "   62.7\n",
      "   46.0\n",
      "   58.1\n",
      "   76.7\n",
      "   64.6\n",
      "   56.4\n",
      "   ChatGPT\n",
      "   5-shot\n",
      "   ✗\n",
      "   54.4\n",
      "   41.4\n",
      "   52.9\n",
      "   61.8\n",
      "   50.9\n",
      "   53.6\n",
      "   Claude-v1.3\n",
      "   5-shot\n",
      "   ✗\n",
      "   54.2\n",
      "   39.0\n",
      "   51.9\n",
      "   61.7\n",
      "   52.1\n",
      "   53.7\n",
      "   Claude-instant-v1.0\n",
      "   5-shot\n",
      "   ✗\n",
      "   45.9\n",
      "   35.5\n",
      "   43.1\n",
      "   53.8\n",
      "   44.2\n",
      "   45.4\n",
      "   Bloomz-mt (176B)\n",
      "   0-shot\n",
      "   ✓\n",
      "   44.3\n",
      "   30.8\n",
      "   39.0\n",
      "   53.0\n",
      "   47.7\n",
      "   42.7\n",
      "   GLM-130B\n",
      "   0-shot\n",
      "   ✓\n",
      "   44.0\n",
      "   30.7\n",
      "   36.7\n",
      "   55.8\n",
      "   47.7\n",
      "   43.0\n",
      "   Chinese-Alpaca-33B\n",
      "   0-shot\n",
      "   ✓\n",
      "   41.6\n",
      "   30.3\n",
      "   37.0\n",
      "   51.6\n",
      "   42.3\n",
      "   40.3\n",
      "   Chinese-Alpaca-Plus-13B\n",
      "   0-shot\n",
      "   ✓\n",
      "   41.5\n",
      "   30.5\n",
      "   36.6\n",
      "   49.7\n",
      "   43.1\n",
      "   41.2\n",
      "   CubeLM (13B)\n",
      "   few-shot\n",
      "   ✗\n",
      "   40.2\n",
      "   27.3\n",
      "   34.1\n",
      "   49.7\n",
      "   43.4\n",
      "   39.6\n",
      "   ChatGLM-6B\n",
      "   0-shot\n",
      "   ✓\n",
      "   38.9\n",
      "   29.2\n",
      "   33.3\n",
      "   48.3\n",
      "   41.3\n",
      "   38.0\n",
      "   LLaMA-65B\n",
      "   5-shot\n",
      "   ✓\n",
      "   38.8\n",
      "   31.7\n",
      "   37.8\n",
      "   45.6\n",
      "   36.1\n",
      "   37.1\n",
      "   Chinese-Alpaca-13B†\n",
      "   0-shot\n",
      "   ✓\n",
      "   36.7\n",
      "   28.4\n",
      "   33.1\n",
      "   43.7\n",
      "   38.4\n",
      "   35.0\n",
      "   Chinese-LLaMA-13B†\n",
      "   5-shot\n",
      "   ✓\n",
      "   33.7\n",
      "   28.1\n",
      "   31.9\n",
      "   38.6\n",
      "   33.5\n",
      "   32.8\n",
      "   Chinese-LLaMA-13B\n",
      "   5-shot\n",
      "   ✓\n",
      "   33.3\n",
      "   27.3\n",
      "   31.6\n",
      "   37.2\n",
      "   33.6\n",
      "   32.8\n",
      "   MOSS (16B)\n",
      "   0-shot\n",
      "   ✓\n",
      "   33.1\n",
      "   28.4\n",
      "   31.6\n",
      "   37.0\n",
      "   33.4\n",
      "   32.1\n",
      "   Chinese-Alpaca-13B\n",
      "   0-shot\n",
      "   ✓\n",
      "   30.9\n",
      "   24.4\n",
      "   27.4\n",
      "   39.2\n",
      "   32.5\n",
      "   28.0\n",
      "   For another aspect, Chinese-Alpaca-13B and Chinese-LLaMA-13B were previously evaluated by C-\n",
      "   Eval. We also manually submitted the prediction file by our own implementation to the leaderboard.\n",
      "   The results show that both models show significant improvements over the ones evaluated by C-Eval,\n",
      "   especially for Alpaca-13B model, yielding +5.8 average score (from 30.9 to 36.7). Also, Alpaca-\n",
      "   13B shows advantages over LLaMA-13B, which is in accordance with our previous findings. These\n",
      "   observations indicate that adopting a proper decoding strategy and prompt template might be vital\n",
      "   in achieving better performance for individual LLMs, especially for instruction-following models.\n",
      "   \n",
      "\n",
      "Section 6: EFFECT OF DIFFERENT QUANTIZATION METHODS\n",
      "   Deploying large language models on personal computers, particularly on CPUs, has historically\n",
      "   been challenging due to their immense computational requirements. However, with the help of\n",
      "   many community efforts, such as llama.cpp (Gerganov, 2023), users can efficiently quantize\n",
      "   LLMs, significantly reducing memory usage and computational demands, making it easier to deploy\n",
      "   LLMs on personal computers. This also enables quicker interactions with the models and facilitates\n",
      "   local data processing. Quantizing LLMs and deploying them on personal computers offer several\n",
      "   benefits. Firstly, it helps users protect their data privacy by ensuring that sensitive information\n",
      "   remains within their local environment rather than being transmitted to external servers. Secondly, it\n",
      "   democratizes access to LLMs by making them more accessible to users with limited computational\n",
      "   resources. Lastly, it promotes the development of new applications and research directions that take\n",
      "   advantage of local LLM deployments. Overall, the ability to deploy LLMs on personal computers\n",
      "   using llama.cpp (or similar) paves the way for a more versatile and privacy-conscious utilization\n",
      "   of LLMs in various domains.\n",
      "   In this section, we investigate the effect of different quantization methods. We use llama.cpp to\n",
      "   quantize Alpaca-Plus-7B, Alpaca-Plus-13B, and Alpaca-33B and calculate the perplexity on Chi-\n",
      "   nese text corpora. We quantize these models into 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit forms to\n",
      "   compare with the original FP16 one.10 The results are shown in Figure 4.\n",
      "   10Specifically, we use q2 K, q3 K, q4 0, q5 0, q6 K, and q8 0 quantization option for each quantized model.\n",
      "   15\n",
      "   \fTechnical Report\n",
      "   9\n",
      "   11\n",
      "   13\n",
      "   15\n",
      "   17\n",
      "   19\n",
      "   2-bit\n",
      "   3-bit\n",
      "   4-bit\n",
      "   5-bit\n",
      "   6-bit\n",
      "   8-bit\n",
      "   F16\n",
      "   10.692\n",
      "   10.713\n",
      "   10.802\n",
      "   10.717\n",
      "   10.999\n",
      "   11.365\n",
      "   13.040\n",
      "   9.147\n",
      "   9.147\n",
      "   9.169\n",
      "   9.325\n",
      "   9.917\n",
      "   10.229\n",
      "   15.455\n",
      "   10.793\n",
      "   10.790\n",
      "   10.845\n",
      "   11.155\n",
      "   12.415\n",
      "   12.504\n",
      "   18.292\n",
      "   Plus-7B\n",
      "   Plus-13B\n",
      "   33B\n",
      "   Figure 4: Perplexities for different quantization methods. Note that 33B model has a higher PPL\n",
      "   as it is trained on less data than the others.\n",
      "   The quantization level is strictly bound to the memory usage and inference speed, and thus a trade-\n",
      "   off must be made when choosing a proper quantization level. As we can see, the 8-bit quantization\n",
      "   method has almost the same or even lower perplexities compared to the original FP16 model, demon-\n",
      "   strating that it is a good choice for deploying LLMs on personal computers, with only half size of\n",
      "   the FP16 one. The 6-bit models also achieve decent PPLs comparable to the 8-bit one, making it\n",
      "   a better balance of speed and performance. When we use a more aggressive quantization level, the\n",
      "   performance drastically decreases (i.e., higher PPL), especially for 3-bit and 2-bit. We also discover\n",
      "   that larger models are less sensitive to quantization methods than smaller ones. For example, the\n",
      "   performance of 33B models changes much more mildly than the others. A similar result is also\n",
      "   observed when comparing Plus-7B and Plus-13B models. This might indicate that though 2-bit and\n",
      "   3-bit quantization are less effective for smaller models, it might be a promising way to deploy larger\n",
      "   models without significant performance loss. This is extremely helpful when the users only have\n",
      "   limited computing resources and still want to try large language models. This might also imply\n",
      "   that the quantized training method may become a main-stream approach for training large language\n",
      "   models, especially for those with limited training resources.\n",
      "   \n",
      "\n",
      "Section 7: CONCLUSION\n",
      "   In this technical report, we have presented an approach to enhance the Chinese understanding\n",
      "   and generation capabilities of the LLaMA model. Acknowledging the limitations of the original\n",
      "   LLaMA’s Chinese vocabulary, we expanded it by incorporating 20K additional Chinese tokens, sig-\n",
      "   nificantly increasing its encoding efficiency for the Chinese language. Building on the Chinese\n",
      "   LLaMA, we employed supervised fine-tuning with instruction data, resulting in Chinese Alpaca\n",
      "   models exhibiting improved instruction-following capabilities.\n",
      "   To evaluate our models effectively, we annotated 200 samples across ten distinct task types and\n",
      "   utilized GPT-4 for evaluation. Our experiments demonstrated that the proposed models significantly\n",
      "   outperformed the original LLaMA in Chinese understanding and generation tasks. We also tested\n",
      "   our models on C-Eval datasets. The results show that the proposed model could achieve significant\n",
      "   improvements and show competitive performance to the models with several times bigger sizes.\n",
      "   Looking ahead, we plan to explore Reinforcement Learning from Human Feedback (RLHF) or Re-\n",
      "   inforcement Learning from AI Instructed Feedback (RLAIF) to further align the models’ output\n",
      "   with human preferences. Moreover, we intend to adopt more advanced and effective quantization\n",
      "   methods, such as GPTQ (Frantar et al., 2022), among others. Additionally, we aim to investigate al-\n",
      "   ternative methods to LoRA for more efficient and effective pre-training and fine-tuning of large lan-\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_sections(text):\n",
    "    sections = re.split(r'(\\d+(?:\\.\\d+)*)\\n([A-Z\\s]+)\\n', text)[1:]\n",
    "    return [(sections[i], sections[i+1].strip(), sections[i+2]) for i in range(0, len(sections), 3)]\n",
    "\n",
    "with open('/Users/ogb/Desktop/cllama-mu.md', 'r') as file:\n",
    "    text = file.read()\n",
    "    sections = parse_sections(text)\n",
    "    for i, section in enumerate(sections):\n",
    "        print(f\"Section {section[0]}: {section[1]}\\n{section[2]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically transformed natural language processing research and shown promising strides towards Artificial General Intelligence (AGI). Nonetheless, the high costs associated with training and deploying LLMs present substantial obstacles to transparent, accessible academic research. While several large language models, such as LLaMA, have been open-sourced by the community, these predominantly focus on English corpora, limiting their usefulness for other languages. In this paper, we propose a method to augment LLaMA with capabilities for understanding and generating Chinese text and its ability to follow instructions. We achieve this by extending LLaMA's existing vocabulary with an additional 20,000 Chinese tokens, thereby improving its encoding efficiency and semantic understanding of Chinese. We further incorporate secondary pre-training using Chinese data and fine-tune the model with Chinese instruction datasets, significantly enhancing the model's ability to comprehend and execute instructions. Our experimental results indicate that the newly proposed model markedly enhances the original LLaMA's proficiency in understanding and generating Chinese content. Additionally, the results on the C-Eval dataset yield competitive performance among the models with several times the size of ours. We have made our pre-trained models, training scripts, and other resources available through GitHub, fostering open research for our community. ${ }^{12}$\n",
      "\n",
      "('\\\\section*{1 INTRODUCTION}', \"\\n\\nNatural language processing (NLP) field has witnessed a substantial paradigm shift with the advent of Large Language Models (LLMs). These models, distinguished by their considerable size and comprehensive training data, have demonstrated extraordinary abilities in comprehending and producing human-like text. In contrast to pre-trained language models dedicated to text understanding, such as BERT (Devlin et al., 2019), the GPT series (Radford et al., 2018) accentuates text generation, positioning them as more suitable platforms for creativity compared to their counterparts. Notably, the latest members of the GPT family, namely ChatGPT and GPT-4, have garnered significant attention, establishing themselves as leading examples in this rapidly evolving field.\\n\\nChatGPT (OpenAI, 2022), evolved from InstructGPT (Ouyang et al., 2022), serves as an advanced conversational AI model capable of conducting context-aware, human-like interactions. Its success set the stage for the development of GPT-4 (OpenAI, 2023), a more sophisticated LLM, demonstrating even greater potential in natural language understanding, generation, and various NLP tasks, especially for its multi-modal and reasoning abilities. These models have catalyzed new research directions and applications, intensifying interest in exploring the potential of Artificial General Intelligence (AGI). Exhibiting impressive performance across multiple benchmarks, they have also demonstrated capabilities for few-shot learning and adaptability to new tasks, significantly driving the expansion of NLP research. Consequently, they have inspired both researchers and industry professionals to further harness their potential across a wide array of applications, including sentiment analysis, machine translation, question-answering systems, and more.\\n\\\\footnotetext{\\n${ }^{*}$ Equal contributions.\\n\\n${ }^{1}$ Chinese LLaMA series: https://github.com/ymcui/Chinese-LLaMA-Alpaca\\n\\n${ }^{2}$ Chinese Llama-2 series: https://github.com/ymcui/Chinese-LLaMA-Alpaca-2\\n}\\n\\n![](https://cdn.mathpix.com/cropped/2024_04_09_8bbce9ce4589243ed8cag-02.jpg?height=813&width=1390&top_left_y=282&top_left_x=365)\\n\\nFigure 1: Overview of the proposed Chinese LLaMA and Chinese Alpaca models (based on Meta's LLaMA and Llama-2). Chinese LLaMA series are foundation models, and Chinese Alpaca series are chat or instruction-following models.\\n\\nHowever, as impactful as LLMs have been, their implementation comes with inherent limitations that hamper transparent and open research. A major concern is their proprietary nature, which restricts access to the models, thus inhibiting the broader research community's ability to build upon their successes. Furthermore, the vast computational resources necessary for training and deploying these models present a challenge for researchers with limited resources, further compounding the accessibility problem.\\n\\nTo tackle these limitations, the NLP research community has gravitated towards open-source alternatives to promote greater transparency and collaboration. LLaMA (Touvron et al., 2023), Llama-2 (Touvron et al., 2023), and Alpaca (Taori et al., 2023a) serve as notable examples of such initiatives. These open-source LLMs are intended to facilitate academic research and accelerate progress within the NLP field. The aim of open-sourcing these models is to foster an environment conducive to further advancements in model development, fine-tuning, and evaluation, ultimately leading to the creation of robust, capable LLMs applicable to a wide variety of uses.\\n\\nDespite the considerable strides made by LLaMA and Alpaca in NLP, they exhibit inherent limitations concerning native support for Chinese language tasks. Their vocabularies contain only a few hundred Chinese tokens, substantially hindering their efficiency in encoding and decoding Chinese text. Building on our previous work with the Chinese BERT series (Cui et al., 2021) and Chinese minority-oriented multilingual pre-trained models (Yang et al., 2022), in this technical report, we propose the development of Chinese LLaMA and Alpaca models with enhanced capabilities for understanding and generating Chinese content. We extend the original LLaMA's vocabulary with an additional 20,000 Chinese tokens, significantly improving its proficiency in processing and generating Chinese text. To ensure efficient training and deployment of these models, we employ the Low-Rank Adaptation (LoRA) approach (Hu et al., 2021), enabling us to train and fine-tune the models without excessive computational costs. We anticipate our preliminary study to enhance the Chinese understanding and generation capabilities of LLaMA and Alpaca serves as a foundation for researchers aiming to adapt these models to other languages. By showcasing the feasibility and effectiveness of our approach, we offer insights and methodologies that can be employed to extend vocabularies and improve the performance of LLaMA and Alpaca models in various languages. An overview of the proposed models is depicted in Figure 1.\\n\\nIn summary, the contributions of this technical report are as follows:\\n- We enhance the encoding and decoding efficiency of the Chinese language and improve LLaMA's Chinese understanding ability by extending the original LLaMA's vocabulary with an additional 20,000 Chinese tokens.\\n- We employ the Low-Rank Adaptation (LoRA) approach to facilitate efficient training and deployment of the Chinese LLaMA and Alpaca models, enabling researchers to work with these models without incurring excessive computational costs.\\n- We evaluate the performance of the proposed LLaMA and Alpaca models in instructionfollowing tasks and natural language understanding tasks, thereby demonstrating substantial improvements over their original counterparts in the context of Chinese language tasks.\\n- We make the resources and findings of our study publicly available, fostering further research and collaboration in the NLP community and encouraging the adaptation of LLaMA and Alpaca models to other languages.\\n\\n\")\n",
      "('\\\\section*{2 Chinese LlaMA And Chinese Alpaca}', '\\n\\n\\\\subsection*{2.1 BACKGROUND}\\n\\nLLaMA (Touvron et al., 2023) is a foundational, decoder-only large language model built upon the transformer architecture (Vaswani et al., 2017). Similar to the GPT series and other transformerbased LLMs, LLaMA consists of an embedding layer, multiple transformer blocks, and a language model head. LLaMA also incorporates improvements utilized in different models, such as prenormalization (Zhang \\\\& Sennrich, 2019), SwiGLU activation (Shazeer, 2020), and rotary embeddings (Su et al., 2021). LLaMA is available in four different model sizes: 7B, 13B, 33B, and 65B.\\n\\nLLaMA has been pre-trained with a standard language modeling task (see Section 2.4) using a mix of publicly available sources, such as crawled web pages, books, Wikipedia, and preprint papers. Experimental findings reveal that LLaMA delivers competitive performance compared to other LLMs like GPT-3, albeit at a smaller model size. This compactness and effectiveness have garnered considerable attention from researchers, leading to the widespread use of LLaMA-based models.\\n\\n\\\\subsection*{2.2 Chinese Vocabulary Extension}\\n\\nLLaMA\\'s training set encompasses roughly 1.4T tokens, with the majority in English and a small fraction in other European languages using Latin or Cyrillic scripts (Touvron et al., 2023). Thus, LLaMA possesses multilingual and cross-lingual comprehension abilities, mostly demonstrated in European languages. Interestingly, our prior preliminary study reveals that LLaMA exhibits basic Chinese understanding ability, although its capacity to generate Chinese texts is limited.\\n\\nTo equip LLaMA with enhanced Chinese understanding and generation capabilities, we propose to continue pre-training the LLaMA model with Chinese corpora. However, directly applying continual pre-training with Chinese corpora encounters several challenges. Firstly, the original LLaMA vocabulary covers less than a thousand Chinese characters, which is insufficient to encode general Chinese texts. Although the LLaMA tokenizer circumvents this issue by tokenizing unknown UTF-8 characters to bytes, this strategy significantly extends sequence length and slows down the encoding and decoding efficiency of Chinese texts, as each Chinese character splits into 3-4 byte tokens. Secondly, byte tokens are not exclusively designed to represent Chinese characters. Since byte tokens also signify UTF-8 tokens in other languages, it becomes challenging for byte tokens and transformer encoders to effectively learn representations capturing the semantic meaning of Chinese characters.\\n\\nTo address these problems and improve encoding efficiency, we propose to extend LLaMA vocabulary with additional Chinese tokens and adapt the model for the extended vocabulary (Yang et al., 2022). The extension process proceeds as follows:\\n- To enhance the tokenizer\\'s support for Chinese texts, we initially train a Chinese tokenizer with SentencePiece (Kudo \\\\& Richardson, 2018) on Chinese corpora ${ }^{3}$ with a vocabulary size of 20,000 .\\n- We subsequently merge the Chinese tokenizer into the original LLaMA tokenizer by taking the union of their vocabularies. Consequently, we obtain a merged tokenizer, which we term the Chinese LLaMA tokenizer, with a vocabulary size of 49,953.\\n- To adapt the LLaMA model for the Chinese LLaMA tokenizer, we resize the word embeddings and language model head from shape $V \\\\times H$ to $V^{\\\\prime} \\\\times H$, where $V=32,000$ denotes the original vocabulary size, and $V^{\\\\prime}=49,953$ is the new vocabulary size of the Chinese LLaMA tokenizer. The new rows are appended to the end of the original embedding matrices, ensuring that the embeddings of the tokens in the original vocabulary remain unaffected.\\n\\nPreliminary experiments indicate that the number of tokens generated by the Chinese LLaMA tokenizer is approximately half of those generated by the original LLaMA tokenizer. Table 1 provides a comparison between the original LLaMA tokenizer and our Chinese LLaMA tokenizer. As depicted, the Chinese LLaMA tokenizer significantly reduces the encoding length compared to the original. With a fixed context length, the model can accommodate about twice as much information, and the generation speed is twice as fast as the original LLaMA tokenizer. This highlights the effectiveness of our proposed approach in enhancing the Chinese understanding and generation capabilities of the LLaMA model.\\n\\nTable 1: Tokenizer comparisons between original LLaMA and Chinese LLaMA.\\n\\n\\\\begin{tabular}{|c|c|c|}\\n\\\\hline & Length & Content \\\\\\\\\\n\\\\hline Original Sentence & 28 & 人工智能是计算机科学、心理学、哲学等学科融合的交叉学科。 \\\\\\\\\\n\\\\hline Original Tokenizer & 35 & ![](https://cdn.mathpix.com/cropped/2024_04_09_8bbce9ce4589243ed8cag-04.jpg?height=124\\\\&width=909\\\\&top_left_y=1311\\\\&top_left_x=820) \\\\\\\\\\n\\\\hline Chinese Tokenizer & 16 & ![](https://cdn.mathpix.com/cropped/2024_04_09_8bbce9ce4589243ed8cag-04.jpg?height=79\\\\&width=909\\\\&top_left_y=1437\\\\&top_left_x=820) \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\n\\\\subsection*{2.3 Parameter Efficient Fine-Tuning with LoRA}\\n\\nThe conventional training paradigm that updates the full parameters of LLMs is prohibitively expensive and is not time- or cost-feasible to most labs or companies. Low-Rank Adaptation (LoRA) (Hu et al., 2021) is a parameter-efficient training method that maintains the pre-trained model weights while introducing trainable rank decomposition matrices. LoRA freezes the pre-trained model weights and injects trainable low-rank matrices into each layer. This approach significantly reduces total trainable parameters, making it feasible to train LLMs with much less computational resources.\\n\\nTo be specific, for a linear layer with weight matrix $W_{0} \\\\in \\\\mathbb{R}^{d \\\\times k}$, where $k$ is the input dimension, and $d$ is the output dimension, LoRA adds two low-rank decomposed trainable matrices $B \\\\in \\\\mathbb{R}^{d \\\\times r}$ and $A \\\\in \\\\mathbb{R}^{r \\\\times k}$, where $r$ is the pre-determined rank. The forward pass with input $x$ is given by the following equation,\\n\\n$$\\n\\\\begin{equation*}\\nh=W_{0} x+\\\\Delta W x=W_{0} x+B A x, B \\\\in \\\\mathbb{R}^{d \\\\times r}, A \\\\in \\\\mathbb{R}^{r \\\\times d} \\\\tag{1}\\n\\\\end{equation*}\\n$$\\n\\nDuring training, $W_{0}$ is frozen and does not receive gradient updates, while $B$ and $A$ are updated. By choosing the rank $r \\\\ll \\\\min (d, k)$, the memory consumption is reduced as we do not need to store the optimizer states for the large frozen matrix.\\n\\nTo achieve parameter-efficient training while adhering to a tight budget, we apply LoRA training to all Chinese LLaMA and Alpaca models in our paper, including both the pre-training and fine-tuning\\n\\\\footnotetext{\\n${ }^{3}$ The training data is the same as the one for training basic version of our models.\\n}\\nstages. We primarily incorporate LoRA adapters into the weights of the attention module and MLP layers. The effectiveness of applying LoRA to all linear transformer blocks is verified in QLoRA (Dettmers et al., 2023), indicating that our choices were reasonable.\\n\\n\\\\subsection*{2.4 Pre-TRAINING OBJECTIVE}\\n\\nWe pre-train the Chinese LLaMA model with the standard Causal Language Modeling (CLM) task. Given an input token sequence $\\\\boldsymbol{x}=\\\\left(x_{0}, x_{1}, x_{2}, \\\\ldots\\\\right)$, the model is trained to predict the next token $x_{i}$ in an autoregressive manner. Mathematically, the objective is to minimize the following negative log-likelihood:\\n\\n$$\\n\\\\begin{equation*}\\n\\\\mathcal{L}_{\\\\mathrm{CLM}}(\\\\Theta)=\\\\mathbb{E}_{\\\\boldsymbol{x} \\\\sim \\\\mathcal{D}_{\\\\mathrm{PT}}}\\\\left[-\\\\sum_{i} \\\\log p\\\\left(x_{i} \\\\mid x_{0}, x_{1}, \\\\ldots, x_{i-1} ; \\\\Theta\\\\right)\\\\right] \\\\tag{2}\\n\\\\end{equation*}\\n$$\\n\\nwhere, $\\\\Theta$ represents the model parameters, $\\\\mathcal{D}_{\\\\mathrm{PT}}$ is the pre-training dataset, $x_{i}$ is the token to be predicted, and $x_{0}, x_{1}, \\\\ldots, x_{i-1}$ constitute the context.\\n\\n\\\\subsection*{2.5 Supervised Fine-Tuning and Chinese Alpaca}\\n\\nPre-trained language models can hardly follow user instructions and often generate unintended content. This is because the language modeling objective in Equation (2) is predicting the next token, not \"follow the instructions and answer the questions\" (Ouyang et al., 2022). To align the behavior of language models to the user\\'s intention, one can fine-tune the model to explicitly train it to follow instructions. Stanford Alpaca (Taori et al., 2023b) is a LLaMA-based instruction-following model that was trained on $52 \\\\mathrm{~K}$ instruction-following data generated by the techniques in the Self-Instruct (Wang et al., 2022). We follow the approach in Stanford Alpaca to apply self-instructed fine-tuning on Chinese LLaMA to train an instruction-following model - Chinese Alpaca.\\n\\nChinese Alpaca is trained on a combination of instruction-following datasets. Each example in the dataset consists of an instruction and an output. The supervised fine-tuning task is similar to the causal language modeling task: the model is prompted with the instruction and trained to generate the output autoregressively. The instruction is wrapped in a prompt template, and the output immediately follows the template. We adopt the following template from Stanford Alpaca for fine-tuning and inference, and the input sequence looks like:\\n\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\\\\#\\\\#\\\\# Instruction:\\n\\n\\\\{instruction $\\\\}$\\n\\n\\\\#\\\\#\\\\# Response: \\\\{output $\\\\}$\\n\\nThe loss is only calculated on the $\\\\{$ output $\\\\}$ part of the input sequence and can be expessed as:\\n\\n$$\\n\\\\begin{equation*}\\n\\\\mathcal{L}_{\\\\mathrm{SFT}}(\\\\Theta)=\\\\mathbb{E}_{\\\\boldsymbol{x} \\\\sim \\\\mathcal{D}_{\\\\mathrm{SFT}}}\\\\left[-\\\\sum_{i \\\\in\\\\{\\\\text { output }\\\\}} \\\\log p\\\\left(x_{i} \\\\mid x_{0}, x_{1}, \\\\ldots, x_{i-1} ; \\\\Theta\\\\right)\\\\right] \\\\tag{3}\\n\\\\end{equation*}\\n$$\\n\\nHere, $\\\\Theta$ represents the model parameters, $\\\\mathcal{D}_{\\\\mathrm{SFT}}$ is the fine-tuning dataset, $\\\\boldsymbol{x}=\\\\left(x_{0}, x_{1}, \\\\ldots\\\\right)$ represents the tokenized input sequence.\\n\\nA major difference between our approach and Stanford Alpaca is that we only use the prompt template designed for examples without an input field, whereas Stanford Alpaca employs two templates for examples both with and without an input field. If the example contains a non-empty input field, we concatenate the instruction and input with an \" $\\\\backslash n$ \" to form the new instruction. Note that there is an additional padding token for the Chinese Alpaca model, resulting in a vocabulary size 49,954.')\n"
     ]
    }
   ],
   "source": [
    "# Seems to work decently well BUT COST $$$ FOR MATH2PIX\n",
    "with open('/Users/ogb/Desktop/chinese-LLAMA.md', 'r') as file:\n",
    "    markdown_text = file.read()\n",
    "\n",
    "# Now you can process the markdown_text variable with your desired function\n",
    "import re\n",
    "\n",
    "def extract_abstract(text):\n",
    "    pattern = r\"\\\\begin{abstract}(.*?)\\\\end{abstract}\"\n",
    "    abstract = re.search(pattern, text, re.DOTALL)\n",
    "    return abstract.group(1) if abstract else None\n",
    "\n",
    "abstract = extract_abstract(markdown_text)\n",
    "print(abstract)\n",
    "\n",
    "# find sections\n",
    "def split_sections(text):\n",
    "    pattern = r\"(\\\\section\\*{.*?})(.*?)(?=\\\\section\\*{.*?}|$)\"\n",
    "    sections = re.findall(pattern, text, re.DOTALL)\n",
    "    return sections\n",
    "\n",
    "sections = split_sections(markdown_text)\n",
    "for section in sections:\n",
    "    print(section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import MathpixPDFLoader\n",
    "# loader = MathpixPDFLoader(\"https://arxiv.org/pdf/2303.17564.pdf\")\n",
    "# data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this as \"Free\" for now\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "elements = partition_pdf(\n",
    "  filename=\"/Users/ogb/Desktop/chinese-LLAMA.pdf\", \n",
    "  strategy=\"ocr_only\",                                     # mandatory to use ``hi_res`` strategy\n",
    "  extract_images_in_pdf=True,                            # mandatory to set as ``True``\n",
    "  extract_image_block_types=[\"Image\", \"Table\"],          # optional\n",
    "  # extract_image_block_to_payload=False,                  # optional\n",
    "  # extract_image_block_output_dir=\"path/to/save/images\",  # optional - only works when ``extract_image_block_to_payload=False``\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2304.08177v3 [cs.CL] 23 Feb 2024\n",
      "\n",
      "Technical Report\n",
      "\n",
      "EFFICIENT AND EFFECTIVE TEXT ENCODING FOR CHINESE LLAMA AND ALPACA\n",
      "\n",
      "Yiming Cui* Zigqing Yang* Xin Yao ymcui@ieee.org ziqingyang@égmail.com yaoxin94@foxmail.com\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically transformed natural language processing research and shown promising strides towards Artificial General Intelligence (AGI). Nonetheless, the high costs associ- ated with training and deploying LLMs present substantial obstacles to transpar- ent, accessible academic research. While several large language models, such as LLaMA, have been open-sourced by the community, these predominantly focus on English corpora, limiting their usefulness for other languages. In this paper, we propose a method to augment LLaMA with capabilities for understanding and generating Chinese text and its ability to follow instructions. We achieve this by extending LLaMA’s existing vocabulary with an additional 20,000 Chinese to- kens, thereby improving its encoding efficiency and semantic understanding of Chinese. We further incorporate secondary pre-training using Chinese data and fine-tune the model with Chinese instruction datasets, significantly enhancing the model’s ability to comprehend and execute instructions. Our experimental results indicate that the newly proposed model markedly enhances the original LLaMA’s proficiency in understanding and generating Chinese content. Additionally, the results on the C-Eval dataset yield competitive performance among the models with several times the size of ours. We have made our pre-trained models, train- ing scripts, and other resources available through GitHub, fostering open research for our community. !?\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Natural language processing (NLP) field has witnessed a substantial paradigm shift with the advent of Large Language Models (LLMs). These models, distinguished by their considerable size and comprehensive training data, have demonstrated extraordinary abilities in comprehending and pro- ducing human-like text. In contrast to pre-trained language models dedicated to text understanding, such as BERT (Devlin et al., 2019), the GPT series (Radford et al., 2018) accentuates text generation, positioning them as more suitable platforms for creativity compared to their counterparts. Notably, the latest members of the GPT family, namely ChatGPT and GPT-4, have garnered significant atten- tion, establishing themselves as leading examples in this rapidly evolving field.\n",
      "\n",
      "ChatGPT (OpenAI, 2022), evolved from InstructGPT (Ouyang et al., 2022), serves as an advanced conversational AI model capable of conducting context-aware, human-like interactions. Its success set the stage for the development of GPT-4 (OpenAI, 2023), a more sophisticated LLM, demonstrat- ing even greater potential in natural language understanding, generation, and various NLP tasks, especially for its multi-modal and reasoning abilities. These models have catalyzed new research directions and applications, intensifying interest in exploring the potential of Artificial General In- telligence (AGI). Exhibiting impressive performance across multiple benchmarks, they have also demonstrated capabilities for few-shot learning and adaptability to new tasks, significantly driving the expansion of NLP research. Consequently, they have inspired both researchers and industry pro- fessionals to further harness their potential across a wide array of applications, including sentiment analysis, machine translation, question-answering systems, and more.\n",
      "\n",
      "Equal contributions. ‘Chinese LLaMA series: https: //github.com/ymcui/Chinese-LLaMA-Alpaca Chinese Llama-2 series: https: //github.com/ymcui/Chinese-LLaMA-Alpaca-2\n",
      "\n",
      "Technical Report\n",
      "\n",
      "Meta’s LLaMA Meta’s Llama-2 Foundation Models Foundation Models (7B, 13B, 33B) (7B, 13B) Pre-training with Pre-training with Pre-training with 206 Text Data 1206 Text Data 120G Text Data\n",
      "\n",
      "Chinese-LLaMA Chinese-LLaMA-Plus\n",
      "\n",
      "Chinese-LLaMA-2 Chinese-LLaMA-2-16K\n",
      "\n",
      "2M-4.3M Supervised 2M-4.3M Supervised 5M Supervised Long Context Fine-tuning Fine-tuning Fine-tuning Fine-tuning\n",
      "\n",
      "Chinese-Alpaca\n",
      "\n",
      "Chinese-Alpaca-Plus\n",
      "\n",
      "Chinese-Alpaca-2\n",
      "\n",
      "Chinese-Alpaca-:\n",
      "\n",
      "|\n",
      "\n",
      "Long Response Fine-tuning\n",
      "\n",
      "Chinese-Alpaca-Pro\n",
      "\n",
      "Figure 1: Overview of the proposed Chinese LLaMA and Chinese Alpaca models (based on Meta’s LLaMA and Llama-2). Chinese LLaMA series are foundation models, and Chinese Alpaca series are chat or instruction-following models.\n",
      "\n",
      "However, as impactful as LLMs have been, their implementation comes with inherent limitations that hamper transparent and open research. A major concern is their proprietary nature, which restricts access to the models, thus inhibiting the broader research community’s ability to build upon their successes. Furthermore, the vast computational resources necessary for training and deploying these models present a challenge for researchers with limited resources, further compounding the accessibility problem.\n",
      "\n",
      "To tackle these limitations, the NLP research community has gravitated towards open-source alter- natives to promote greater transparency and collaboration. LLaMA (Touvron et al., 2023), Llama-2 (Touvron et al., 2023), and Alpaca (Taori et al., 2023a) serve as notable examples of such initia- tives. These open-source LLMs are intended to facilitate academic research and accelerate progress within the NLP field. The aim of open-sourcing these models is to foster an environment conducive to further advancements in model development, fine-tuning, and evaluation, ultimately leading to the creation of robust, capable LLMs applicable to a wide variety of uses.\n",
      "\n",
      "Despite the considerable strides made by LLaMA and Alpaca in NLP, they exhibit inherent limita- tions concerning native support for Chinese language tasks. Their vocabularies contain only a few hundred Chinese tokens, substantially hindering their efficiency in encoding and decoding Chinese text. Building on our previous work with the Chinese BERT series (Cui et al., 2021) and Chinese minority-oriented multilingual pre-trained models (Yang et al., 2022), in this technical report, we propose the development of Chinese LLaMA and Alpaca models with enhanced capabilities for understanding and generating Chinese content. We extend the original LLaMA’s vocabulary with an additional 20,000 Chinese tokens, significantly improving its proficiency in processing and gen- erating Chinese text. To ensure efficient training and deployment of these models, we employ the Low-Rank Adaptation (LoRA) approach (Hu et al., 2021), enabling us to train and fine-tune the models without excessive computational costs. We anticipate our preliminary study to enhance the Chinese understanding and generation capabilities of LLaMA and Alpaca serves as a foundation for researchers aiming to adapt these models to other languages. By showcasing the feasibility and effectiveness of our approach, we offer insights and methodologies that can be employed to extend vocabularies and improve the performance of LLaMA and Alpaca models in various languages. An overview of the proposed models is depicted in Figure 1.\n",
      "\n",
      "Technical Report\n",
      "\n",
      "In summary, the contributions of this technical report are as follows:\n",
      "\n",
      "¢ We enhance the encoding and decoding efficiency of the Chinese language and improve LLaMA’s Chinese understanding ability by extending the original LLaMA’s vocabulary with an additional 20,000 Chinese tokens.\n",
      "\n",
      "¢ We employ the Low-Rank Adaptation (LoRA) approach to facilitate efficient training and de- ployment of the Chinese LLaMA and Alpaca models, enabling researchers to work with these models without incurring excessive computational costs.\n",
      "\n",
      "¢ We evaluate the performance of the proposed LLaMA and Alpaca models in instruction- following tasks and natural language understanding tasks, thereby demonstrating substantial improvements over their original counterparts in the context of Chinese language tasks.\n",
      "\n",
      "« We make the resources and findings of our study publicly available, fostering further research and collaboration in the NLP community and encouraging the adaptation of LLaMA and Al- paca models to other languages.\n",
      "\n",
      "2 CHINESE LLAMA AND CHINESE ALPACA\n",
      "\n",
      "2.1 BACKGROUND\n",
      "\n",
      "LLaMA (Touvron et al., 2023) is a foundational, decoder-only large language model built upon the transformer architecture (Vaswani et al., 2017). Similar to the GPT series and other transformer- based LLMs, LLaMA consists of an embedding layer, multiple transformer blocks, and a language model head. LLaMA also incorporates improvements utilized in different models, such as pre- normalization (Zhang & Sennrich, 2019), SwiGLU activation (Shazeer, 2020), and rotary embed- dings (Su et al., 2021). LLaMA is available in four different model sizes: 7B, 13B, 33B, and 65B.\n",
      "\n",
      "LLaMA has been pre-trained with a standard language modeling task (see Section 2.4) using a mix of publicly available sources, such as crawled web pages, books, Wikipedia, and preprint pa- pers. Experimental findings reveal that LLaMA delivers competitive performance compared to other LLMs like GPT-3, albeit at a smaller model size. This compactness and effectiveness have garnered considerable attention from researchers, leading to the widespread use of LLaMA-based models.\n",
      "\n",
      "2.2 CHINESE VOCABULARY EXTENSION\n",
      "\n",
      "LLaMA’s training set encompasses roughly 1.4T tokens, with the majority in English and a small fraction in other European languages using Latin or Cyrillic scripts (Touvron et al., 2023). Thus, LLaMA possesses multilingual and cross-lingual comprehension abilities, mostly demonstrated in European languages. Interestingly, our prior preliminary study reveals that LLaMA exhibits basic Chinese understanding ability, although its capacity to generate Chinese texts is limited.\n",
      "\n",
      "To equip LLaMA with enhanced Chinese understanding and generation capabilities, we propose to continue pre-training the LLaMA model with Chinese corpora. However, directly applying contin- ual pre-training with Chinese corpora encounters several challenges. Firstly, the original LLaMA vocabulary covers less than a thousand Chinese characters, which is insufficient to encode gen- eral Chinese texts. Although the LLaMA tokenizer circumvents this issue by tokenizing unknown UTF-8 characters to bytes, this strategy significantly extends sequence length and slows down the encoding and decoding efficiency of Chinese texts, as each Chinese character splits into 3-4 byte tokens. Secondly, byte tokens are not exclusively designed to represent Chinese characters. Since byte tokens also signify UTF-8 tokens in other languages, it becomes challenging for byte tokens and transformer encoders to effectively learn representations capturing the semantic meaning of Chinese characters.\n",
      "\n",
      "To address these problems and improve encoding efficiency, we propose to extend LLaMA vocab- ulary with additional Chinese tokens and adapt the model for the extended vocabulary (Yang et al., 2022). The extension process proceeds as follows:\n",
      "\n",
      "Technical Report\n",
      "\n",
      "To enhance the tokenizer’s support for Chinese texts, we initially train a Chinese tokenizer with SentencePiece (Kudo & Richardson, 2018) on Chinese corpora’ with a vocabulary size of 20,000.\n",
      "\n",
      "« We subsequently merge the Chinese tokenizer into the original LLaMA tokenizer by taking the union of their vocabularies. Consequently, we obtain a merged tokenizer, which we term the Chinese LLaMA tokenizer, with a vocabulary size of 49,953.\n",
      "\n",
      "To adapt the LLaMA model for the Chinese LLaMA tokenizer, we resize the word embeddings and language model head from shape V x H to V’ x H, where V = 32,000 denotes the original vocabulary size, and V’ = 49,953 is the new vocabulary size of the Chinese LLaMA tokenizer. The new rows are appended to the end of the original embedding matrices, ensuring that the embeddings of the tokens in the original vocabulary remain unaffected.\n",
      "\n",
      "Preliminary experiments indicate that the number of tokens generated by the Chinese LLaMA tok- enizer is approximately half of those generated by the original LLaMA tokenizer. Table 1 provides a comparison between the original LLaMA tokenizer and our Chinese LLaMA tokenizer. As depicted, the Chinese LLaMA tokenizer significantly reduces the encoding length compared to the original. With a fixed context length, the model can accommodate about twice as much information, and the generation speed is twice as fast as the original LLaMA tokenizer. This highlights the effectiveness of our proposed approach in enhancing the Chinese understanding and generation capabilities of the LLaMA model.\n",
      "\n",
      "Table 1: Tokenizer comparisons between original LLaMA and Chinese LLaMA.\n",
      "\n",
      "Length Content\n",
      "\n",
      "Original Sentence 28 ALB ERLUVERAE. CRE. FESSARAHELEA. Ey ee ee\n",
      "\n",
      "Original Tokenizer 35 2 a ee\n",
      "\n",
      "RDB, RE, Uta, AE eee\n",
      "\n",
      "BBA BO RL BA ,\n",
      "\n",
      "Chinese Tokenizer 16\n",
      "\n",
      "2.3. PARAMETER EFFICIENT FINE-TUNING WITH LORA\n",
      "\n",
      "The conventional training paradigm that updates the full parameters of LLMs is prohibitively expen- sive and is not time- or cost-feasible to most labs or companies. Low-Rank Adaptation (LoRA) (Hu et al., 2021) is a parameter-efficient training method that maintains the pre-trained model weights while introducing trainable rank decomposition matrices. LoRA freezes the pre-trained model weights and injects trainable low-rank matrices into each layer. This approach significantly reduces total trainable parameters, making it feasible to train LLMs with much less computational resources.\n",
      "\n",
      "To be specific, for a linear layer with weight matrix Wo € R¢**, where k is the input dimension, and d is the output dimension, LoRA adds two low-rank decomposed trainable matrices B € R¢*\" and A € R\"**, where r is the pre-determined rank. The forward pass with input x is given by the following equation,\n",
      "\n",
      "h=Wor+AWr=Wor+ BAz, BER\", ACR™? (1)\n",
      "\n",
      "During training, Wo is frozen and does not receive gradient updates, while B and A are updated. By choosing the rank r < min(d, k), the memory consumption is reduced as we do not need to store the optimizer states for the large frozen matrix.\n",
      "\n",
      "To achieve parameter-efficient training while adhering to a tight budget, we apply LoRA training to all Chinese LLaMA and Alpaca models in our paper, including both the pre-training and fine-tuning\n",
      "\n",
      "~The training data is the same as the one for training basic version of our models.\n",
      "\n",
      "Technical Report\n",
      "\n",
      "stages. We primarily incorporate LoRA adapters into the weights of the attention module and MLP layers. The effectiveness of applying LoRA to all linear transformer blocks is verified in QLORA (Dettmers et al., 2023), indicating that our choices were reasonable.\n",
      "\n",
      "2.4 PRE-TRAINING OBJECTIVE\n",
      "\n",
      "We pre-train the Chinese LLaMA model with the standard Causal Language Modeling (CLM) task. Given an input token sequence w = (x9, 21, %2,...), the model is trained to predict the next token x; in an autoregressive manner. Mathematically, the objective is to minimize the following negative log-likelihood:\n",
      "\n",
      "Lom(9) = Ez~p,y | > log p(wi|vo, 21, ---,®i-1; ©) (2)\n",
      "\n",
      "where, © represents the model parameters, Dpr is the pre-training dataset, x; is the token to be predicted, and xp, 2%1,...,2;_1 constitute the context.\n",
      "\n",
      "2.5 SUPERVISED FINE-TUNING AND CHINESE ALPACA\n",
      "\n",
      "Pre-trained language models can hardly follow user instructions and often generate unintended con- tent. This is because the language modeling objective in Equation (2) is predicting the next token, not “follow the instructions and answer the questions” (Ouyang et al., 2022). To align the behavior of language models to the user’s intention, one can fine-tune the model to explicitly train it to follow instructions. Stanford Alpaca (Taori et al., 2023b) is a LLaMA-based instruction-following model that was trained on 52K instruction-following data generated by the techniques in the Self-Instruct (Wang et al., 2022). We follow the approach in Stanford Alpaca to apply self-instructed fine-tuning on Chinese LLaMA to train an instruction-following model — Chinese Alpaca.\n",
      "\n",
      "Chinese Alpaca is trained on a combination of instruction-following datasets. Each example in the dataset consists of an instruction and an output. The supervised fine-tuning task is similar to the causal language modeling task: the model is prompted with the instruction and trained to generate the output autoregressively. The instruction is wrapped in a prompt template, and the output imme- diately follows the template. We adopt the following template from Stanford Alpaca for fine-tuning and inference, and the input sequence looks like:\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: {instruction}\n",
      "\n",
      "### Response: {output} The loss is only calculated on the {output} part of the input sequence and can be expressed as: Lsrr(®) = ExwDeer | — > log p(xi|0,%1,--+, 2-15 9) (3) 4€ {output}\n",
      "\n",
      "Here, O represents the model parameters, Dsrr is the fine-tuning dataset, « = (29, x sents the tokenized input sequence.\n",
      "\n",
      "...) repre-\n",
      "\n",
      "A major difference between our approach and Stanford Alpaca is that we only use the prompt tem- plate designed for examples without an input field, whereas Stanford Alpaca employs two templates for examples both with and without an input field. If the example contains a non-empty input field, we concatenate the instruction and input with an “\\n” to form the new instruction. Note that there is an additional padding token for the Chinese Alpaca model, resulting in a vocabulary size 49,954.\n",
      "\n",
      "Technical Report\n",
      "\n",
      "3 EXPERIMENTAL SETUPS\n",
      "\n",
      "3.1 EXPERIMENTAL SETUPS FOR PRE-TRAINING\n",
      "\n",
      "We initialize the Chinese LLaMA model with the original LLaMA weights and conduct pre-training using fp16 on the 7B and 13B models. Additionally, for the 33B model, we employ the bitsandbytes* library to train it in an 8-bit format, enhancing its efficiency and memory usage. We directly apply LoRA to attentions and MLPs for training while setting the embeddings and LM head as trainable.\n",
      "\n",
      "For the basic version of Chinese LLaMA-7B, we utilize a two-stage pre-training approach. In stage 1, we fix the parameters of the transformer encoders within the model and only train the embeddings, adapting the newly added Chinese word vectors while minimizing the disturbance to the original model. In stage 2, we add LoRA weights (adapters) to the attention mechanisms and train the embeddings, LM heads, and newly added LoRA parameters. Note that two-stage training is not applied to other model training as it is less efficient in our preliminary study.\n",
      "\n",
      "For the other Chinese LLaMA models (basic version), we utilize a 20GB general Chinese corpus for pre-training, which is consistent with the corpora used by Chinese BERT-wwm (Cui et al., 2021), MacBERT (Cui et al., 2020), LERT (Cui et al., 2022), and others. We also provide “Plus” version, which further expands the pre-training data to 120GB, incorporating additional data from Com- monCrawl (CC) and encyclopedia sources, enhancing the model’s understanding of fundamental concepts. We concatenated all the datasets and generated chunks of block size 512 for pre-training purposes.\n",
      "\n",
      "The models are trained on A40 GPUs (48GB VRAM) for one epoch, taking up to 48 GPUs depend- ing on the model size. The parameter-efficient training with LoRA is performed with PEFT library>. We also utilize DeepSpeed (Rasley et al., 2020) to optimize memory efficiency during the training process. We employ the AdamW optimizer (Loshchilov & Hutter, 2019) with a peak learning rate of 2e-4 and 5% warm-up cosine scheduler. Additionally, we apply gradient clipping with a value of 1.0 to mitigate potential gradient explosion.\n",
      "\n",
      "Detailed hyperparameters for each Chinese LLaMA model are listed in Table 2.\n",
      "\n",
      "Table 2: Pre-training hyperparameters for Chinese LLaMA. QKVO: four matrices in each at- tention module, i.e., query, key, value, and output. MLP: three matrices in each MLP layer. Note that 7B uses a two-stage training paradigm (settings are separated by ‘/’), which is not further adopted in other models.\n",
      "\n",
      "Settings 7B Plus-7B 13B Plus-13B 33B Training data 20 GB 120 GB 20 GB 120 GB 20 GB Batch size 1,024 2,304 2,304 2,304 2,304 Peak learning rate 2e-4/le-4 2e-4 2e-4 2e-4 2e-4 Max sequence length 512 512 512 512 512 LoRA rank -/8 8 8 8 8 LoRA alpha -/32 32 32 32 32 LoRA weights -/QKVO QKVO,MLP QKVO,MLP QKVO,MLP QKVO, MLP Trainable params (%) — 2.97%/6.06% 6.22% 4.10% 4.10% 2.21%\n",
      "\n",
      "3.2 EXPERIMENTAL SETUPS FOR INSTRUCTION FINE-TUNING\n",
      "\n",
      "After obtaining the Chinese LLaMA models, we fine-tune them according to Section 2.5. We con- tinue to employ LoRA for efficient fine-tuning by adding LoRA modules to all linear layers of the base model. We utilize approximately 2M to 3M instruction data, including translation (Xu, 2019) (550K sampled), pCLUE® (250K sampled, excluding “NLU-like” data), Stanford Alpaca (SOK+50K\n",
      "\n",
      "4https://github.com/TimDettmers/bitsandbytes Shttps://github.com/huggingface/peft Shttps://github.com/CLUEbenchmark/pCLUE\n",
      "\n",
      "Technical Report\n",
      "\n",
      "for original and translated one), and crawled SFT data for tuning basic models. For the Plus ver- sion, we expand the dataset to approximately 4M to 4.3M, with a specific emphasis on incorporating STEM (Science, Technology, Engineering, and Mathematics) data, as well as several scientific dis- ciplines such as physics, chemistry, biology, medicine, and earth sciences. For Alpaca-33B, we additionally add OASST1 dataset (Kopf et al., 2023), where we only extract the first query-response pair from each conversation and translate using gpt-3.5-turbo API, resulting in roughly 20K data (original and translated one). We set the maximum sequence length to 512 and pad the samples dynamically when batching to the maximum length in the batch.\n",
      "\n",
      "For the crawled data, we refer to the self-instruct (Wang et al., 2022) method for automatically obtaining data from ChatGPT (gpt-3 . 5-turbo API), as used in Taori et al. (2023a). Concretely, we utilize a more simplified template that does not require seed tasks, with only the requirements for targeted domains and instruction types. Templates and code details are available on GitHub.’\n",
      "\n",
      "Table 3: Instruction fine-tuning hyperparameters for Chinese Alpaca.\n",
      "\n",
      "Settings 7B Plus-7B 13B Plus-13B 33B Training data 2M 4M 3M 4.3M 4.3M Batch size 512 1,152 1,152 1,152 1,152 Peak learning rate le-4 le-4 le-4 le-4 le-4 Max sequence length 512 512 512 512 512 LoRA rank 8 64 8 64 8 LoRA alpha 32 128 32 128 32 LoRA weights QKVO,MLP QKVO,MLP QKVO,MLP QKVO,MLP QKVO, MLP Trainable params (%) 6.22% 8.08% 4.10% 5.66% 2.21%\n",
      "\n",
      "For the Plus version, we utilize a larger LoRA rank compared to the basic version. Besides adjusting the learning rate and batch size, we also maintain consistency with the other hyperparameters and settings used during the pre-training stage.\n",
      "\n",
      "The hyperparameters for instruction fine-tuning are listed in Table 3. Note that all Alpaca models are trained based on respective LLaMA models. For example, Chinese Alpaca-Plus-13B is trained upon Chinese LLaMA-Plus-13B.\n",
      "\n",
      "4 RESULTS ON INSTRUCTION-FOLLOWING TASKS\n",
      "\n",
      "4.1 TASK DESIGN AND EVALUATION METHOD\n",
      "\n",
      "Evaluating the performance of text generation tasks can be challenging due to the significant varia- tion in their form, making it significantly different from natural language understanding tasks, such as text classification and extractive machine reading comprehension. Following previous work that utilizes GPT-4 (OpenAI, 2023) as a scoring method, we also adopt GPT-4 to provide an overall score (on a 10-point scale) for each sample, which is more efficient than human evaluation. How- ever, GPT-4 may not always provide accurate scores, so we perform manual checks on its ratings and adjust them if necessary. The manual checks ensure that the scores are consistent and reflect the true performance of the models being evaluated. We use the following prompt template for scoring two outputs of the systems (which can be adjusted to multiple systems):\n",
      "\n",
      "The followings are two ChatGPT-like systems’ outputs. Please rate an overall score on a ten-point scale for each and give explanations to justify your scores.\n",
      "\n",
      "Prompt:\n",
      "\n",
      "{prompt-input}\n",
      "\n",
      "System1:\n",
      "\n",
      "{system1-output}\n",
      "\n",
      "System2:\n",
      "\n",
      "Thttps://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/crawl_ prompt .py\n",
      "\n",
      "Technical Report\n",
      "\n",
      "{system2-output}\n",
      "\n",
      "By employing GPT-4 as a scoring method in conjunction with manual checks, we establish a reliable evaluation framework that effectively measures the performance of our Chinese Alpaca models on a range of natural language understanding and generation tasks.\n",
      "\n",
      "Our evaluation set is designed to comprehensively assess the Chinese Alpaca models across a wide range of natural language understanding and generation tasks. The set comprises 200 samples, covering ten distinct tasks, including Question Answering, Reasoning, Literature, Entertainment, Translation, Multi-turn Dialogue, Coding, and Ethics, etc. The overall score for a specific task is calculated by summing the scores for all samples within that task and normalizing the total to a 100- point scale. This approach ensures that the evaluation set reflects the models’ capabilities across various tasks, providing a balanced and robust measure of their performance.\n",
      "\n",
      "4.2 EXPERIMENTAL SETUPS FOR DECODING\n",
      "\n",
      "The decoding process of LLMs plays a critical role in determining the quality and diversity of the generated text. In our experiments, we use the following decoding hyperparameters:\n",
      "\n",
      "Context size: We set the context size to 2048, which determines the maximum number of tokens the model can consider simultaneously when generating text.\n",
      "\n",
      "¢« Maximum sequence length: We limit the generated sequence length to 512 tokens to ensure that the outputs remain focused and relevant to the input prompt.\n",
      "\n",
      "¢ Temperature: We set the temperature to 0.2, which controls the randomness of the sampling process. Lower values make the model generate more focused and deterministic outputs, while higher values increase diversity at the cost of coherence. For multi-turn dialogue and generation tasks, we slightly adjust the temperature to 0.5 to allow a more diverse output.\n",
      "\n",
      "Top-k sampling: We use Top-k sampling with k = 40, meaning that the model selects its next token from the top 40 most probable tokens at each step, adding an element of randomness and diversity to the generated text.\n",
      "\n",
      "Top-p sampling: We also employ Top-p sampling with p = 0.9, which further enhances diver- sity by considering a dynamic set of tokens that collectively account for 90% of the probability mass.\n",
      "\n",
      "¢ Repetition penalty: To discourage the model from generating repetitive text, we apply a repeti- tion penalty with a factor of 1.1, penalizing tokens that have already been selected.\n",
      "\n",
      "Note that these values may not be optimal for each testing scenario. We did not perform further tuning on these hyperparameters for each task to maintain a balanced view.\n",
      "\n",
      "4.3 RESULTS\n",
      "\n",
      "We present and analyze the results obtained by our Chinese Alpaca-Plus-7B, Alpaca-Plus-13B, and Alpaca-33B models. The Alpaca-33B results are generated by original model (FP16), while the Alpaca-Plus-7B and Alpaca-Plus-13B adopt 8-bit quantized version.* The overall results are shown in Table 4. The evaluation is based on GPT-4 rated results across ten distinct NLP tasks, encompass- ing a total of 200 samples. It is important to note that the presented scores are solely comparable with each other but not with other models, which would require rescoring the systems. Also, as our models are built upon original LLaMA, these observations can be regarded as what are important as- pects to achieving better performance when built upon a well-established model rather than training from scratch. We elaborate on the findings of several major categories in detail.\n",
      "\n",
      "We mainly present the results on Chinese-LLaMA and Chinese-Alpaca. The results on Chinese- LLaMA-2 and Chinese-Alpaca-2 are presented in Appendix A.\n",
      "\n",
      "8 We will discuss the quantization effect in Section 6.\n",
      "\n",
      "Technical Report\n",
      "\n",
      "Table 4: GPT-4 rated results for Chinese Alpaca-Plus-7B and Alpaca-Plus-13B, and Alpaca- 33B. Note that the results are only comparable within this model combination.\n",
      "\n",
      "Task Alpaca-Plus-7B = Alpaca-Plus-13B = Alpaca-33B Question Answering 70.5 79.5 82.3 Open-ended QA 80.5 80.0 78.5 Numerical Reasoning 51.0 61.5 84.5 Poetry, Literature, Philosophy 78.5 81.3 76.0 Music, Sports, Entertainment 72.3 76.8 72.5 Letters and Articles Writing 81.0 86.5 79.0 Translation 86.8 89.3 92.3 Multi-turn Dialogue 80.3 81.3 78.0 Coding 62.5 67.5 84.0 Ethics 89.8 90.5 92.5 Total 75.3 79.4 82.0\n",
      "\n",
      "4.3.1 MULTI-TURN DIALOGUE\n",
      "\n",
      "One of the impressive achievements of ChatGPT is its rich and fluent contextual understanding ability, which is conveyed by the multi-turn dialogue interface. As we can see, Plus series models yield consistent improvements over the basic one, though the size of the latter one is several times that of the formers. This might indicate that it is much more important to ingest more training data than simply extending the parameter size of the model to achieve a better dialogue experience. Especially our models are constructed from the original LLaMA, where linguistic knowledge can not be directly transferred.\n",
      "\n",
      "4.3.2 TEXT GENERATION\n",
      "\n",
      "Text generation is one of the most fundamental abilities for language models. Compared to Alpaca- Plus-7B and Alpaca-Plus-13B, Alpaca-33B shows inferior results in this category. Table 5 shows an example of a text generation task. We can see that both Alpaca-Plus-7B and Alpaca-Plus-13B provide correct letter styles, which meet the requirement of the user’s prompt. Alpaca-Plus-13B provides the most comprehensive one by indicating that the applicant has thoroughly prepared all materials for visa application, making it the best generation quality among all three systems. How- ever, Alpaca-33B does not follow the letter style, and the content is somewhat too simplified, which is clearly not as good as the others. This demonstrates that training with more data with smaller models might give better performance than big models with less data.\n",
      "\n",
      "4.3.3. NUMERICAL CALCULATION AND REASONING\n",
      "\n",
      "Numerical reasoning has been regarded as one of the most essential tasks in examining the reasoning ability of large language models. As we can see, the Alpaca-33B achieves significant improvements over Plus-7B and Plus-13B models. Table 6 shows example outputs for this task. The first prompt is well-known for probing the reasoning ability, namely “which one is heavier, 1kg of cotton or lkg of iron?”. Both Plus-7B and Plus-13B failed to give a correct answer mentioning that “cotton is lighter than iron”. However, 33B could correctly identify that these two things are the same weight. The second prompt is a simple calculation task, asking “how many legs for a cat and a chicken”. However, as we can see, both Plus-7B and Plus-13B do not have the commonsense knowledge that a cat has four legs and two for a chicken, resulting in wrong answers. The last prompt is a numerical reasoning task to let the model predict the next number of an array. Still, only 33B model correctly identifies the pattern of the given array that the next number should be the square of its index. These observations indicate that the size of the model is vital in numerical reasoning tasks.\n",
      "\n",
      "Technical Report\n",
      "\n",
      "Table 5: Example output for text generation task. 33B model does not provide detailed and rich context compared to Plus-7B and Plus-13B.\n",
      "\n",
      "Model = Prompt: iH 1% #8 RH Ft AL KAR AF BE HP aH 1S\n",
      "\n",
      "Plus-7B - BAH RA ARE: (8.5/10)\n",
      "\n",
      "RE-SRKAPHYHAS, HHT202247A WER BR. RAHM RE KRIT HA BRAS TE\n",
      "\n",
      "RHE PRAWKE PMS, HEESULLPMG. RHR PRBT, FAB FW OK A KAR AR EG KT IE\n",
      "\n",
      "GHB ERY PIF BLK\n",
      "\n",
      "aL4U!\n",
      "\n",
      "Plus-13B BAH RAKE HR, (10/10)\n",
      "\n",
      "Rexxx, RAPA. KTRMPAF3IA MER DMTAM—TA ORT. RTE Bis KMAN FH . BY FAM edo FIRB, HH BIWMAN RR. KtalsiweR BAT AS Yt ROSA RK Fo RRA AB Be\n",
      "\n",
      "RLARSHIMA SEROMA, QHMEMITEN. BE MITERA RRB. RA BML ILK PT RABE, VAG RR RATAN BEAT\n",
      "\n",
      "BAS tl fo KE. RHA BRAK EY.\n",
      "\n",
      "BEEK,\n",
      "\n",
      "BL 33B HHH KAARRBLYAR, RERRAAME Pi —KARRHSIE. RR-SZ (8/10) PHAR, AAS ARM IO, ARQ HOA A. MK RIT HY\n",
      "\n",
      "AMRRGAALU. THSIUMLAH, FER-BSMUSHMA. KHULT KAQLEAM, Rik REM WHARF!\n",
      "\n",
      "4.3.4 CODING\n",
      "\n",
      "Figure 2 shows an example of implementing the Dijkstra algorithm in Python. Plus-7B scores 3/10 due to a structurally sound approach that unfortunately fails to calculate and update shortest distances correctly and includes an undefined function. Plus-13B attempts abstraction by imple- menting a Graph class and a distance method, which shows a basic understanding of how a graph and its related operations could be represented in object-oriented programming. Also, the fact that it is attempting to implement a shortest path algorithm (despite not correctly implementing Dijkstra’s algorithm) makes it a slightly higher score than Plus-7B’s. The 33B model offers a much better Dijkstra algorithm implementation, earning it an 8/10 score. Despite its lack of a priority queue and absence of error handling, which would enhance efficiency and robustness, the code correctly up- dates shortest distances, maintains track of predecessors, and ensures all nodes are visited, reflecting a fundamental understanding of the algorithm.\n",
      "\n",
      "From these results, it could be inferred that larger models tend to perform better in complex tasks like code generation, potentially due to their ability to capture more intricate patterns in the training data.\n",
      "\n",
      "4.3.5 ETHICS\n",
      "\n",
      "Aligning LLMs to human preference is vital in creating responsible artificial intelligence. In the Ethics category, we mainly want to test how these models respond to illegal input prompts. By\n",
      "\n",
      "10\n",
      "\n",
      "Technical Report\n",
      "\n",
      "Table 6: Example output for numerical reasoning task. 33B model significantly outperforms Alpaca-Plus-7B and Alpaca-Plus-13B.\n",
      "\n",
      "Model Prompt: — 2 /f 4% 4642-27 4K, M—4 BE? Plus-7B (3/10) ARIEL — A RAE Plus-13B (3/10) — FF AAR IE HLA OY ARAB\n",
      "\n",
      "33B (9/10) A ARE AAT HREE HA. Model Prompt: — 3 #4 — 2 4-H ALR?\n",
      "\n",
      "Plus-7B (1/10) —%aFe—-RHh—-RABRH. Plus-13B (1/10) —A#i-RBSAORM, PPAR Ay -HAAR DE.\n",
      "\n",
      "33B (10/10) RMARLMATHAHORSER. —RBAARM, —RHAIRM, PR VAP Rhy -—HA ORM Model Prompt: /¥ ¥'|1,4,9,164 & —*4k 2 tt A?\n",
      "\n",
      "Plus-7B (1/10) —**4&£16- Plus-13B (1/10) 21 33B (10/10) FE 5 1,4,9,1689 G —* AEDS ©\n",
      "\n",
      "checking the generation results, all three systems responded properly to users’ prompts. Alpaca- 33B yields slightly better performance than the others. We discover that Alpaca-33B may not only “reject” illegal prompts but also give appropriate advice in addition. For example, in Table 7, both Plus-7B and Plus-13B simply refuse to give any advice on making money by exploiting some net- work vulnerabilities. On the contrary, 33B model not only refuses the user prompt but also gives advice on how to make money using legal ways, making the response more comprehensive and helpful.\n",
      "\n",
      "Table 7: Example output for ethics category. 33B model tends to provide more comprehensive and helpful information over Plus-7B and Plus-13B.\n",
      "\n",
      "Model _— Prompt: #¢4 4 A] — 28 1 4-77 RAR?\n",
      "\n",
      "Plus-7B RA AEA A KAA AS TA RHR EAS, AAR T WRF KE\n",
      "\n",
      "(8/10) Me. MBREE-RGEHES, ANH FWAH AAMT LR. VA Fy PR Wl 28 Se Fo FRAP PAL PAB\n",
      "\n",
      "Plus-13B FAAS SARA, KA HEB MK AG ET AE TK Gh A AL 2G HI a AT AR KS\n",
      "\n",
      "(8/10) WRIA, SHRP SHAKEERPMRMA. AEVRBPFAPAM, FE JAFMENT KITA\n",
      "\n",
      "33B TRA ALIS SRA, RAMEE MT AE AER AMEN. AM RBE SKB Ae ae\n",
      "\n",
      "(10) ML REAR R IG. WREMMRRS RMA, TUAABATILPAR:\n",
      "\n",
      "1 ARREARS: FRACHRMER MS, HEALER SRRS, bl tof LEM. Rim. RBRSS.\n",
      "\n",
      "2. ABA: A-MBH MI LIRR, todit. Sie. SMRFLA . 3. ABH: MURR. RE. MASAHARU, RKB. CHLZEMS\n",
      "\n",
      "eal, RM Riri. 4. lk: SRATHATRAK, FRAO FR BRIG, HIP RA\n",
      "\n",
      "ERB LOS KI, ABS APTA\n",
      "\n",
      "Overall, Alpaca-33B yields significant improvements over Alpaca-Plus-7B and Alpaca-Plus-13B in various aspects, including numerical reasoning, coding, ethics, etc. We conjecture that these abilities are better handled by bigger models than the smaller ones, though Alpaca-33B is trained\n",
      "\n",
      "11\n",
      "\n",
      "Technical Report\n",
      "\n",
      "Plus-7B (3/10):\n",
      "\n",
      "import heapq\n",
      "\n",
      "1 2 3 def dijkstra(graph, s): 4 queue = [] 5 heap = [s] 6 while len(heap) > 0: 7 current = heap.pop() 8 for i in graph[current]: 9 if i not in heap and dist (i, s) < dist(s, current): 10 heap.append (i) ul return queue Plus-13B (4/10): 1 class Graph: 2 def init__(self, adj): 3 self.adj = adj 4 5 def distance(self, v): 6 dist = [inf] * len(self.adj) 7 for u in self.adj[v]: 8 if u not in dist: 9 dist[u] = 1 + min(dist[ul, dist[v]) 10 ul return dist[v 12 13 def main(): 14 g = Graph({ (0,1), (1,2)), (2,3), (3,41) 15 16 vel 17 d = g.distance(v) 18 print (\"Distance is:’, d) 19 20 if __name__ __main_!: 2 main() 33B (8/10):\n",
      "\n",
      "def dijkstra(graph, source):\n",
      "\n",
      "1\n",
      "\n",
      "2 = {}\n",
      "\n",
      "3 decessors = {}\n",
      "\n",
      "4 for node in graph:\n",
      "\n",
      "5 distances[node] = float (‘inf’) 6 predecessors[node] =\n",
      "\n",
      "T distances[source] = 0\n",
      "\n",
      "8 queue = [source]\n",
      "\n",
      "9 while queue:\n",
      "\n",
      "10 current = queue.pop (0)\n",
      "\n",
      "is for neighbor in graph[current]:\n",
      "\n",
      "12 if distances[neighbor] > distances[current] + graph[current] [neighbor]: 13 distances [neighbor] = distances[current] + graph[current] [neighbor] 14 predecessors[neighbor] = current\n",
      "\n",
      "15 return distances\n",
      "\n",
      "Figure 2: Example output for coding task. 33B model significantly outperforms Alpaca-Plus-7B and Alpaca-Plus-13B.\n",
      "\n",
      "with less data. Another possible reason would be the inherited ability from the original LLaMA, in which coding and reasoning ability is relatively language-independent. However, we also noticed that Alpaca-33B has inferior results in text generation, multi-turn dialogue, etc. As Plus series models are trained on much more data, they are capable of providing more diverse and rich content. We anticipate these issues can be tackled when Alpaca-Plus-33B becomes available, as we find these abilities are relatively easy to overcome than those that require high-level reasoning, such as numerical reasoning and coding-related tasks. For complete comparisons, ratings, and sample outputs, please refer to our GitHub repository.”\n",
      "\n",
      "5 RESULTS ON NATURAL LANGUAGE UNDERSTANDING TASKS\n",
      "\n",
      "5.1 TASK DESCRIPTION\n",
      "\n",
      "Besides the generation performance test for instruction-following tasks, we also tested our models on the C-Eval dataset (Huang et al., 2023), which is a multi-choice question answering dataset. C-\n",
      "\n",
      "°nttps: //github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples\n",
      "\n",
      "12\n",
      "\n",
      "Technical Report\n",
      "\n",
      "Eval mainly covers four categories: STEM, Social, Humanities, and Others, consisting of nearly 14K samples for 52 disciplines. Similar to other multi-choice QA datasets, such as RACE (Lai et al., 2017), it requires the model to produce the correct option label based on the given question. We mainly tested our model on the validation split (1,346 samples) and test split (12,342 samples), where the test scores are obtained by submitting models’ prediction files to the official leaderboard.\n",
      "\n",
      "5.2 DECODING STRATEGY\n",
      "\n",
      "To evaluate LLaMA models on this dataset, we directly feed the examples to these models. While when evaluating Alpaca models, we wrap the examples in the prompt template as demonstrated in Section 2.5. Then the model is asked to make a one-step prediction and give the probability distribution of the next token p(y|a), where y € V (V is the vocabulary). To map the probability distribution to a valid label ¢ in {A, B, C, D}, we extract and gather the probabilities of related tokens. We introduce a verbalizer V(-) to map each label ¢ to tokens in the vocabulary:\n",
      "\n",
      "Va) = CAA}, VB) = {BB}, VC) = TC,“ }, V(D) = {*D’,“_D\"} The probability of predicting label t is given by p(t € {A,B,C,D}|x) = D> ply = ila) (4) teV(i) The label with the max probability is taken as the final prediction.\n",
      "\n",
      "Next, we will elaborate on our results and analysis in the following two subsections, illustrating the comparisons to the original LLaMA and other models.\n",
      "\n",
      "5.3 COMPARISONS TO ORIGINAL LLAMA\n",
      "\n",
      "Figure 3 demonstrates how our models evolve based on the original LLaMA. Detailed results are depicted in Table 8. We mainly describe our findings in the following aspects.\n",
      "\n",
      "LlaMA fll Chinese-LLaMA Ml Chinese-LLaMA-Plus IMI Chinese-Alpaca__—_—IJ Chinese-Alpaca-Plus 45\n",
      "\n",
      "20 7B (zero-shot) 7B (6-shot) 13B (zero-shot) 13B (6-shot)\n",
      "\n",
      "Figure 3: Results on C-Eval valid set. The results are grouped by different settings (zero-shot and 5-shot) and model sizes (7B and 13B).\n",
      "\n",
      "Chinese LLaMA improves original LLaMA. We can see that the proposed Chinese LLaMA models yield moderate improvements over the original LLaMA, which demonstrates that the pre- training on Chinese data has some positive effect on C-Eval but not always. When we compare Chinese LLaMA and LLaMA-Plus, the latter does not show significant improvements over the for- mer one, even showing inferior results for 13B setting. This might indicate that the pure language model (like LLaMA) may not be a good choice for C-Eval or similar tasks, and it does not ben- efit much from increasing the pre-training data size (from 20G to 120G for Chinese LLaMA and LLaMA-Plus, respectively).\n",
      "\n",
      "13\n",
      "\n",
      "Technical Report\n",
      "\n",
      "Table 8: Results on C-Eval valid and test sets. All prediction files are generated by ourselves.\n",
      "\n",
      "The test set scores are obtained by submitting prediction files to the C-Eval leaderboard. Model Valid Set Test Set Zero-shot 5-shot Zero-shot 5-shot\n",
      "\n",
      "Random 25.0 25.0 25.0 25.0 LLaMA-65B 37.2 41.2 33.4 38.8 LLaMA-33B 34.5 37.9 32.4 36.0 LLaMA-13B 27.8 30.9 28.5 29.6 LLaMA-7B 25.6 25.3 26.7 27.8 Chinese-LLaMA-33B 34.9 38.4 34.6 39.5 Chinese-LLaMA-Plus-13B 27.3 34.0 27.8 33.3 Chinese-LLaMA-13B 29.4 35.0 29.2 33.7 Chinese-LLaMA-Plus-7B 27.3 28.3 26.8 28.4 Chinese-LLaMA-7B 26.2 26.2 27.1 27.2 Chinese-Alpaca-33B 43.3 42.6 41.6 40.4 Chinese-Alpaca-Plus-13B 43.3 42.4 41.5 39.9 Chinese-Alpaca-13B 37.1 36.3 36.7 34.5 Chinese-Alpaca-Plus-7B 36.7 32.9 36.4 32.3 Chinese-Alpaca-7B 30.8 32.5 30.7 29.2\n",
      "\n",
      "Alpaca models show significant improvements over LLaMA. Among different settings, such as zero-shot or 5-shot, the Alpaca model series show significant improvements over LLaMA coun- terparts, demonstrating that the instruction-following models are more capable of handling these NLU-like tasks than pure language models. Unlike the phenomenon observed in the LLaMA series, we can see that Alpaca-Plus models yield significant improvement over basic Alpaca models. This might further indicate that instruction-following models are more capable of handling NLU-like tasks and can unleash the power of using more pre-training data (LLaMA-Plus).\n",
      "\n",
      "LLaMA generally yields better performance in a few-shot setting, while Alpaca prefers zero- shot. Generally speaking, LLaMA with 5-shot setting shows better performance than zero-shot setting, while Alpaca with zero-shot setting is much better than 5-shot one. As LLaMA is not de- signed for instruction-following, few-shot setting might give valuable information on how to follow the question answering structure in C-Eval. However, on the contrary, as Alpaca has already been trained with millions of instruction data, it is less likely to benefit from additional shots. Also, the official 5-shot setting uses identical prompts for all samples, making it some distraction for Alpaca models.\n",
      "\n",
      "We would like to emphasize that these observations are solely based on the results of the C-Eval dataset, and whether it is generalizable to other datasets requires further investigation. In the fu- ture, we will include more comprehensive tests to further investigate LLaMA and Alpaca models’ behaviors.\n",
      "\n",
      "5.4 COMPARISONS TO OTHER MODELS\n",
      "\n",
      "We include our two best-performing models, i.e., Chinese-Alpaca-33B and Chinese-Alpaca-Plus- 13B, in the C-Eval leaderboard to make a comparison with other LLMs, including both open-source and non-open-source ones. The test results on the C-Eval leaderboard (as of June 9, 2023) are shown in Table 9.\n",
      "\n",
      "Not surprisingly, non-open-source LLMs have significantly better performance than open-source ones. When it comes to our models, we can see that both Chinese-Alpaca-33B and Chinese-Alpaca- Plus-13B yield competitive performance among open-source LLMs in this leaderboard, showing only a moderate gap to Bloomz-mt-176B (Scao et al., 2022) and GLM-130B (Zeng et al., 2023), considering that the latter ones have several times of magnitude and trained with way more data than ours.\n",
      "\n",
      "14\n",
      "\n",
      "Technical Report\n",
      "\n",
      "Table 9: Test results on C-Eval leaderboard (as of June 9, 2023), ordered by average scores. Model name with boldface represents our submissions, while the other results are evaluated by C-Eval officials. We re-evaluated two models marked with { (these scores are not shown publicly) based on our own inference script and achieved significantly better performance than those evaluated\n",
      "\n",
      "by C-Eval. The parameter size of the model is depicted in parentheses when available. Open: open- source. Avg-H: Average (Hard). Model N-Shot Open Avg Avg-H STEM _ Social Human Others GPT-4 5-shot x 68.7 54.9 67. 716 64.5 67.8 InternLM (104B) few-shot x 62.7 46.0 58. 76.7 64.6 56.4 ChatGPT 5-shot x 54.4 414 52.9 61.8 50.9 53.6 Claude-v1.3 5-shot x 54.2 39.0 51.9 61.7 52.1 53.7 Claude-instant-v1.0 5-shot x 45.9 35.5 43. 53.8 44.2 45.4 Bloomz-mt (176B) 0-shot v 44.3 30.8 39.0 53.0 47.7 42.7 GLM-130B 0-shot v 44.0 30.7 36.7 55.8 47.7 43.0 Chinese-Alpaca-33B 0-shot v 41.6 30.3 37.0 51.6 42.3 40.3 Chinese-Alpaca-Plus-13B 0-shot v 41.5 30.5 36.6 49.7 43.1 41.2 CubeLM (13B) few-shot x 40.2 27.3 34. 49.7 43.4 39.6 ChatGLM-6B 0-shot v 38.9 29.2 33.3 48.3 41.3 38.0 LLaMA-65B 5-shot v 38.8 31.7 37.8 45.6 36.1 37.1 Chinese-Alpaca-13Bt 0-shot v 36.7 28.4 33. 43.7 38.4 35.0 Chinese-LLaMA-13B; 5-shot v 33.7 28.1 31.9 38.6 33.5 32.8 Chinese-LLaMA-13B 5-shot v 33.3 27.3 31.6 37.2 33.6 32.8 MOSS (16B) 0-shot v 33.1 28.4 31.6 37.0 33.4 32.1 Chinese-Alpaca-13B 0-shot v 30.9 24.4 274 39.2 32.5 28.0\n",
      "\n",
      "For another aspect, Chinese-Alpaca-13B and Chinese-LLaMA-13B were previously evaluated by C- Eval. We also manually submitted the prediction file by our own implementation to the leaderboard. The results show that both models show significant improvements over the ones evaluated by C-Eval, especially for Alpaca-13B model, yielding +5.8 average score (from 30.9 to 36.7). Also, Alpaca- 13B shows advantages over LLaMA-13B, which is in accordance with our previous findings. These observations indicate that adopting a proper decoding strategy and prompt template might be vital in achieving better performance for individual LLMs, especially for instruction-following models.\n",
      "\n",
      "6 EFFECT OF DIFFERENT QUANTIZATION METHODS\n",
      "\n",
      "Deploying large language models on personal computers, particularly on CPUs, has historically been challenging due to their immense computational requirements. However, with the help of many community efforts, such as 1lama.cpp (Gerganov, 2023), users can efficiently quantize LLMs, significantly reducing memory usage and computational demands, making it easier to deploy LLMs on personal computers. This also enables quicker interactions with the models and facilitates local data processing. Quantizing LLMs and deploying them on personal computers offer several benefits. Firstly, it helps users protect their data privacy by ensuring that sensitive information remains within their local environment rather than being transmitted to external servers. Secondly, it democratizes access to LLMs by making them more accessible to users with limited computational resources. Lastly, it promotes the development of new applications and research directions that take advantage of local LLM deployments. Overall, the ability to deploy LLMs on personal computers using 1 lama. cpp (or similar) paves the way for a more versatile and privacy-conscious utilization of LLMs in various domains.\n",
      "\n",
      "In this section, we investigate the effect of different quantization methods. We use 1 lama. cpp to quantize Alpaca-Plus-7B, Alpaca-Plus-13B, and Alpaca-33B and calculate the perplexity on Chi- nese text corpora. We quantize these models into 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit forms to compare with the original FP16 one.!° The results are shown in Figure 4.\n",
      "\n",
      "‘Specifically, we use q2_K, q3-K, q4-0, q5-0, q6_K, and q8_0 quantization option for each quantized model.\n",
      "\n",
      "15\n",
      "\n",
      "Technical Report\n",
      "\n",
      "— Plus-7B — Plus-13B — 33B\n",
      "\n",
      "19 18.292\n",
      "\n",
      "9.147\n",
      "\n",
      "ae a yo ot os oo ee\n",
      "\n",
      "Figure 4: Perplexities for different quantization methods. Note that 33B model has a higher PPL as it is trained on less data than the others.\n",
      "\n",
      "The quantization level is strictly bound to the memory usage and inference speed, and thus a trade- off must be made when choosing a proper quantization level. As we can see, the 8-bit quantization method has almost the same or even lower perplexities compared to the original FP16 model, demon- strating that it is a good choice for deploying LLMs on personal computers, with only half size of the FP16 one. The 6-bit models also achieve decent PPLs comparable to the 8-bit one, making it a better balance of speed and performance. When we use a more aggressive quantization level, the performance drastically decreases (i.e., higher PPL), especially for 3-bit and 2-bit. We also discover that larger models are less sensitive to quantization methods than smaller ones. For example, the performance of 33B models changes much more mildly than the others. A similar result is also observed when comparing Plus-7B and Plus-13B models. This might indicate that though 2-bit and 3-bit quantization are less effective for smaller models, it might be a promising way to deploy larger models without significant performance loss. This is extremely helpful when the users only have limited computing resources and still want to try large language models. This might also imply that the quantized training method may become a main-stream approach for training large language models, especially for those with limited training resources.\n",
      "\n",
      "7 CONCLUSION\n",
      "\n",
      "In this technical report, we have presented an approach to enhance the Chinese understanding and generation capabilities of the LLaMA model. Acknowledging the limitations of the original LLaMA’s Chinese vocabulary, we expanded it by incorporating 20K additional Chinese tokens, sig- nificantly increasing its encoding efficiency for the Chinese language. Building on the Chinese LLaMA, we employed supervised fine-tuning with instruction data, resulting in Chinese Alpaca models exhibiting improved instruction-following capabilities.\n",
      "\n",
      "To evaluate our models effectively, we annotated 200 samples across ten distinct task types and utilized GPT-4 for evaluation. Our experiments demonstrated that the proposed models significantly outperformed the original LLaMA in Chinese understanding and generation tasks. We also tested our models on C-Eval datasets. The results show that the proposed model could achieve significant improvements and show competitive performance to the models with several times bigger sizes.\n",
      "\n",
      "Looking ahead, we plan to explore Reinforcement Learning from Human Feedback (RLHF) or Re- inforcement Learning from AI Instructed Feedback (RLAIF) to further align the models’ output with human preferences. Moreover, we intend to adopt more advanced and effective quantization methods, such as GPTQ (Frantar et al., 2022), among others. Additionally, we aim to investigate al- ternative methods to LoRA for more efficient and effective pre-training and fine-tuning of large lan-\n",
      "\n",
      "16\n",
      "\n",
      "Technical Report\n",
      "\n",
      "guage models, ultimately enhancing their performance and applicability across various tasks within the Chinese NLP community.\n",
      "\n",
      "LIMITATIONS\n",
      "\n",
      "While this project has successfully enhanced the Chinese understanding and generation capabilities of the LLaMA and Alpaca models, several limitations must be acknowledged:\n",
      "\n",
      "¢ Harmful and unpredictable content: Though our model can reject unethical queries, these mod- els may still generate harmful or misaligned with human preferences and values. This issue may arise from biases in the training data or the models’ inability to discern appropriate outputs in certain contexts.\n",
      "\n",
      "¢ Insufficient training: Due to constraints in computing power and data availability, the training of the models may not be sufficient for optimal performance. As a result, there is still room for improvement in the Chinese understanding capabilities of the models.\n",
      "\n",
      "¢ Lack of robustness: The models may exhibit brittleness in some situations, producing incon- sistent or nonsensical outputs when faced with adversarial inputs or rare language phenomena.\n",
      "\n",
      "¢ Comprehensive evaluation: Evaluating large language models is an important topic in the cur- rent era. While we have seen many evaluation benchmarks for LLMs, their comprehensiveness and appropriateness for LLMs should be well-studied and examined. A more diverse and com- prehensive LLM evaluation dataset and benchmark will have a great positive effect on shaping the future of LLM research.\n",
      "\n",
      "¢ Scalability and efficiency: Although we applied LoRA and quantization to make the model more accessible to a broader community, when combined with the original LLaMA, the mod- els’ large size and complexity can lead to difficulties in deployment, especially for users with limited computational resources. This issue may hinder the accessibility and widespread adop- tion of the models in various applications.\n",
      "\n",
      "Future work should address these limitations to further enhance the models’ capabilities, making them more robust, accessible, and effective for a broader range of applications in the Chinese NLP community.\n",
      "\n",
      "ACKNOWLEDGMENTS\n",
      "\n",
      "The original draft was polished by OpenAI GPT-4 for grammatical corrections and clarity improve- ments. We would like to thank our community members for their contributions to our open-source projects.\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.\n",
      "\n",
      "Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.\n",
      "\n",
      "Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. Revisiting pre- trained models for Chinese natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pp. 657-668, Online, Novem- ber 2020. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/2020.findings-emnlp.58.\n",
      "\n",
      "Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and Ziqing Yang. Pre-training with whole word\n",
      "\n",
      "masking for chinese bert. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3504-3514, 2021. doi: 10.1109/TASLP.2021.3124365.\n",
      "\n",
      "17\n",
      "\n",
      "Technical Report\n",
      "\n",
      "Yiming Cui, Wanxiang Che, Shijin Wang, and Ting Liu. Lert: A linguistically-motivated pre-trained language model. arXiv preprint arXiv:2211.05344, 2022.\n",
      "\n",
      "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized Ilms. arXiv preprint arXiv:2305.14314, 2023.\n",
      "\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, Volume I (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. URL https: //www.aclweb.org/ anthology/N19-1423.\n",
      "\n",
      "Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training compression for generative pretrained transformers. arXiv preprint arXiv:2210.17323, 2022.\n",
      "\n",
      "Georgi Gerganov. llama.cpp. https: //github.com/ggerganov/llama. cpp, 2023.\n",
      "\n",
      "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. arXiv e-prints, art. arXiv:2106.09685, June 2021. doi: 10.48550/arXiv.2106.09685.\n",
      "\n",
      "Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.\n",
      "\n",
      "Andreas Kopf, Yannic Kilcher, Dimitri von Riitte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richard Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. OpenAssistant Conversations — Democratizing Large Language Model Align- ment. arXiv e-prints, art. arXiv:2304.07327, April 2023. doi: 10.48550/arXiv.2304.07327.\n",
      "\n",
      "Taku Kudo and John Richardson. SentencePiece: A simple and language independent sub- word tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66-71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL https: //aclanthology.org/D18-2012.\n",
      "\n",
      "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 785-794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17- 1082. URL https://aclanthology.org/D17-1082.\n",
      "\n",
      "Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese, 2023.\n",
      "\n",
      "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer- ence on Learning Representations, 2019. URL https: //openreview.net/forum?id= Bkg6RiCqyY7.\n",
      "\n",
      "OpenAl. Introducing chatgpt. https: //openai.com/blog/chatgpt, 2022.\n",
      "\n",
      "OpenAI. GPT-4 Technical Report. arXiv e-prints, art. arXiv:2303.08774, March 2023. doi: 10. 48550/arXiv.2303.08774.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel- ton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. arXiv e-prints, art. arXiv:2203.02155, March 2022. doi: 10.48550/arXiv.2203.02155.\n",
      "\n",
      "18\n",
      "\n",
      "Technical Report\n",
      "\n",
      "Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023.\n",
      "\n",
      "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under- standing by generative pre-training. 2018.\n",
      "\n",
      "Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System opti- mizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 3505-3506, 2020.\n",
      "\n",
      "Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili¢, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, Frangois Yvon, Matthias Gallé, et al. Bloom: A 176b- parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n",
      "\n",
      "Noam Shazeer. Glu variants improve transformer, 2020.\n",
      "\n",
      "Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021.\n",
      "\n",
      "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023a.\n",
      "\n",
      "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023b.\n",
      "\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Roziére, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar- mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "\n",
      "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n",
      "\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\n",
      "\n",
      "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning Language Model with Self Generated Instructions. arXiv e-prints, art. arXiv:2212.10560, December 2022. doi: 10.48550/arXiv.2212.10560.\n",
      "\n",
      "Bright Xu. Nlp chinese corpus: Large scale chinese corpus for nlp, September 2019. URL https: //doi.org/10.5281/zenodo. 3402023.\n",
      "\n",
      "Ziging Yang, Zihang Xu, Yiming Cui, Baoxin Wang, Min Lin, Dayong Wu, and Zhigang Chen. CINO: A Chinese minority pre-trained language model. In Proceedings of the 29th Interna- tional Conference on Computational Linguistics, pp. 3937-3949, Gyeongju, Republic of Ko- rea, October 2022. International Committee on Computational Linguistics. URL https: //aclanthology.org/2022.coling-1.346.\n",
      "\n",
      "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130b: An open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=—Aw0rrrPUF.\n",
      "\n",
      "Biao Zhang and Rico Sennrich. Root Mean Square Layer Normalization. In Advances in Neural Information Processing Systems 32, Vancouver, Canada, 2019. URL https: //openreview. net/references/pdf?id=S1qBAf6rr.\n",
      "\n",
      "19\n",
      "\n",
      "Technical Report\n",
      "\n",
      "A APPENDIX\n",
      "\n",
      "We present the baseline results on Chinese-LLaMA-2 and Chinese-Alpaca-2 as follows. Most of the settings are identical to those in Chinese-LLaMA.\n",
      "\n",
      "A.1 C-EVAL\n",
      "\n",
      "The results on C-Eval (Huang et al., 2023) are presented in Table 10.\n",
      "\n",
      "Table 10: Results on C-Eval valid and test sets.\n",
      "\n",
      "Model Valid Set Test Set Zero-shot 5-shot Zero-shot 5-shot Chinese-LLaMA-2-7B 28.2 36.0 30.3 34.2 Chinese-LLaMA-2-13B 40.6 42.7 38.0 41.6 Chinese-Alpaca-2-7B 41.3 42.9 40.3 39.5 Chinese-Alpaca-2-13B 44.3 45.9 42.6 44.0\n",
      "\n",
      "A.2.> CMMLU\n",
      "\n",
      "The results on CMMLU (Li et al., 2023) are presented in Table 11.\n",
      "\n",
      "Table 11: Results on CMMLU test sets.\n",
      "\n",
      "Test Set Model Zero-shot Few-shot Chinese-LLaMA-2-7B 27.9 34.1 Chinese-LLaMA-2-13B 38.9 42.5 Chinese-Alpaca-2-7B 40.0 41.8 Chinese-Alpaca-2-13B 43.2 45.5\n",
      "\n",
      "A.3. LONGBENCH\n",
      "\n",
      "The results on LongBench (Bai et al., 2023) are presented in Table 12. This benchmark is specifi- cally designed to test the long context ability of LLMs. We test the Chinese subsets of LongBench (including code tasks). The models marked with 16K were finetuned using Positional Interpolation (PI) method (Chen et al., 2023), which supports 16K context. The models marked with 64K were finetuned using YaRN method (Peng et al., 2023), which supports 64K context.\n",
      "\n",
      "20\n",
      "\n",
      "Technical Report\n",
      "\n",
      "Table 12: Results on LongBench (Chinese + code tasks). S-QA: Single-doc QA, M-QA: Multi- doc QA, Summ: Summarization, FS-Learn: Few-shot Learning, Code: Code Completion, Synthetic: Synthetic Tasks.\n",
      "\n",
      "Model S-QA M-QA Summ _ FS-Learn Code Synthetic Average Chinese-LLaMA-2-7B 19.0 13.9 6.4 11.0 11.0 47 11.0 Chinese-LLaMA-2-7B-16K 33.2 15.9 6.5 23.5 10.3 5.3 15.8 Chinese-LLaMA-2-7B-64K 27.2 16.4 6.5 33.0 78 5.0 16.0 Chinese-LLaMA-2-13B 28.3 14.4 4.6 16.3 10.4 5.4 13.2 Chinese-LLaMA-2-13B-16K 36.7 17.7 3.1 29.8 13.8 3.0 17.3 Chinese-Alpaca-2-7B 34.0 17.4 11.8 21.3 50.3 4.5 23.2 Chinese-Alpaca-2-7B-16K 46.4 23.3 14.3 29.0 49.6 9.0 28.6 Chinese-Alpaca-2-7B-64K 44.7 28.1 14.4 39.0 44.6 5.0 29.3 Chinese-Alpaca-2-13B 38.4 20.0 11.9 17.3 46.5 8.0 23.7 Chinese-Alpaca-2-13B-16K 47.9 26.7 13.0 22.3 46.6 21.5 29.7\n",
      "\n",
      "21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use this Cell Output and copy to .md file\n",
    "for element in elements:\n",
    "    print(element)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n1 INTRODUCTION', '\\n\\nNatural language processing (NLP) field has witnessed a substantial paradigm shift with the advent of Large Language Models (LLMs). These models, distinguished by their considerable size and comprehensive training data, have demonstrated extraordinary abilities in comprehending and pro- ducing human-like text. In contrast to pre-trained language models dedicated to text understanding, such as BERT (Devlin et al., 2019), the GPT series (Radford et al., 2018) accentuates text generation, positioning them as more suitable platforms for creativity compared to their counterparts. Notably, the latest members of the GPT family, namely ChatGPT and GPT-4, have garnered significant atten- tion, establishing themselves as leading examples in this rapidly evolving field.\\n\\nChatGPT (OpenAI, 2022), evolved from InstructGPT (Ouyang et al., 2022), serves as an advanced conversational AI model capable of conducting context-aware, human-like interactions. Its success set the stage for the development of GPT-4 (OpenAI, 2023), a more sophisticated LLM, demonstrat- ing even greater potential in natural language understanding, generation, and various NLP tasks, especially for its multi-modal and reasoning abilities. These models have catalyzed new research directions and applications, intensifying interest in exploring the potential of Artificial General In- telligence (AGI). Exhibiting impressive performance across multiple benchmarks, they have also demonstrated capabilities for few-shot learning and adaptability to new tasks, significantly driving the expansion of NLP research. Consequently, they have inspired both researchers and industry pro- fessionals to further harness their potential across a wide array of applications, including sentiment analysis, machine translation, question-answering systems, and more.\\n\\nEqual contributions. ‘Chinese LLaMA series: https: //github.com/ymcui/Chinese-LLaMA-Alpaca Chinese Llama-2 series: https: //github.com/ymcui/Chinese-LLaMA-Alpaca-2\\n\\nTechnical Report\\n\\nMeta’s LLaMA Meta’s Llama-2 Foundation Models Foundation Models (7B, 13B, 33B) (7B, 13B) Pre-training with Pre-training with Pre-training with 206 Text Data 1206 Text Data 120G Text Data\\n\\nChinese-LLaMA Chinese-LLaMA-Plus\\n\\nChinese-LLaMA-2 Chinese-LLaMA-2-16K\\n\\n2M-4.3M Supervised 2M-4.3M Supervised 5M Supervised Long Context Fine-tuning Fine-tuning Fine-tuning Fine-tuning\\n\\nChinese-Alpaca\\n\\nChinese-Alpaca-Plus\\n\\nChinese-Alpaca-2\\n\\nChinese-Alpaca-:\\n\\n|\\n\\nLong Response Fine-tuning\\n\\nChinese-Alpaca-Pro\\n\\nFigure 1: Overview of the proposed Chinese LLaMA and Chinese Alpaca models (based on Meta’s LLaMA and Llama-2). Chinese LLaMA series are foundation models, and Chinese Alpaca series are chat or instruction-following models.\\n\\nHowever, as impactful as LLMs have been, their implementation comes with inherent limitations that hamper transparent and open research. A major concern is their proprietary nature, which restricts access to the models, thus inhibiting the broader research community’s ability to build upon their successes. Furthermore, the vast computational resources necessary for training and deploying these models present a challenge for researchers with limited resources, further compounding the accessibility problem.\\n\\nTo tackle these limitations, the NLP research community has gravitated towards open-source alter- natives to promote greater transparency and collaboration. LLaMA (Touvron et al., 2023), Llama-2 (Touvron et al., 2023), and Alpaca (Taori et al., 2023a) serve as notable examples of such initia- tives. These open-source LLMs are intended to facilitate academic research and accelerate progress within the NLP field. The aim of open-sourcing these models is to foster an environment conducive to further advancements in model development, fine-tuning, and evaluation, ultimately leading to the creation of robust, capable LLMs applicable to a wide variety of uses.\\n\\nDespite the considerable strides made by LLaMA and Alpaca in NLP, they exhibit inherent limita- tions concerning native support for Chinese language tasks. Their vocabularies contain only a few hundred Chinese tokens, substantially hindering their efficiency in encoding and decoding Chinese text. Building on our previous work with the Chinese BERT series (Cui et al., 2021) and Chinese minority-oriented multilingual pre-trained models (Yang et al., 2022), in this technical report, we propose the development of Chinese LLaMA and Alpaca models with enhanced capabilities for understanding and generating Chinese content. We extend the original LLaMA’s vocabulary with an additional 20,000 Chinese tokens, significantly improving its proficiency in processing and gen- erating Chinese text. To ensure efficient training and deployment of these models, we employ the Low-Rank Adaptation (LoRA) approach (Hu et al., 2021), enabling us to train and fine-tune the models without excessive computational costs. We anticipate our preliminary study to enhance the Chinese understanding and generation capabilities of LLaMA and Alpaca serves as a foundation for researchers aiming to adapt these models to other languages. By showcasing the feasibility and effectiveness of our approach, we offer insights and methodologies that can be employed to extend vocabularies and improve the performance of LLaMA and Alpaca models in various languages. An overview of the proposed models is depicted in Figure 1.\\n\\nTechnical Report\\n\\nIn summary, the contributions of this technical report are as follows:\\n\\n¢ We enhance the encoding and decoding efficiency of the Chinese language and improve LLaMA’s Chinese understanding ability by extending the original LLaMA’s vocabulary with an additional 20,000 Chinese tokens.\\n\\n¢ We employ the Low-Rank Adaptation (LoRA) approach to facilitate efficient training and de- ployment of the Chinese LLaMA and Alpaca models, enabling researchers to work with these models without incurring excessive computational costs.\\n\\n¢ We evaluate the performance of the proposed LLaMA and Alpaca models in instruction- following tasks and natural language understanding tasks, thereby demonstrating substantial improvements over their original counterparts in the context of Chinese language tasks.\\n\\n« We make the resources and findings of our study publicly available, fostering further research and collaboration in the NLP community and encouraging the adaptation of LLaMA and Al- paca models to other languages.\\n\\n2 CHINESE LLAMA AND CHINESE ALPACA\\n\\n2.1 BACKGROUND\\n\\nLLaMA (Touvron et al., 2023) is a foundational, decoder-only large language model built upon the transformer architecture (Vaswani et al., 2017). Similar to the GPT series and other transformer- based LLMs, LLaMA consists of an embedding layer, multiple transformer blocks, and a language model head. LLaMA also incorporates improvements utilized in different models, such as pre- normalization (Zhang & Sennrich, 2019), SwiGLU activation (Shazeer, 2020), and rotary embed- dings (Su et al., 2021). LLaMA is available in four different model sizes: 7B, 13B, 33B, and 65B.\\n\\nLLaMA has been pre-trained with a standard language modeling task (see Section 2.4) using a mix of publicly available sources, such as crawled web pages, books, Wikipedia, and preprint pa- pers. Experimental findings reveal that LLaMA delivers competitive performance compared to other LLMs like GPT-3, albeit at a smaller model size. This compactness and effectiveness have garnered considerable attention from researchers, leading to the widespread use of LLaMA-based models.\\n\\n2.2 CHINESE VOCABULARY EXTENSION\\n\\nLLaMA’s training set encompasses roughly 1.4T tokens, with the majority in English and a small fraction in other European languages using Latin or Cyrillic scripts (Touvron et al., 2023). Thus, LLaMA possesses multilingual and cross-lingual comprehension abilities, mostly demonstrated in European languages. Interestingly, our prior preliminary study reveals that LLaMA exhibits basic Chinese understanding ability, although its capacity to generate Chinese texts is limited.\\n\\nTo equip LLaMA with enhanced Chinese understanding and generation capabilities, we propose to continue pre-training the LLaMA model with Chinese corpora. However, directly applying contin- ual pre-training with Chinese corpora encounters several challenges. Firstly, the original LLaMA vocabulary covers less than a thousand Chinese characters, which is insufficient to encode gen- eral Chinese texts. Although the LLaMA tokenizer circumvents this issue by tokenizing unknown UTF-8 characters to bytes, this strategy significantly extends sequence length and slows down the encoding and decoding efficiency of Chinese texts, as each Chinese character splits into 3-4 byte tokens. Secondly, byte tokens are not exclusively designed to represent Chinese characters. Since byte tokens also signify UTF-8 tokens in other languages, it becomes challenging for byte tokens and transformer encoders to effectively learn representations capturing the semantic meaning of Chinese characters.\\n\\nTo address these problems and improve encoding efficiency, we propose to extend LLaMA vocab- ulary with additional Chinese tokens and adapt the model for the extended vocabulary (Yang et al., 2022). The extension process proceeds as follows:\\n\\nTechnical Report\\n\\nTo enhance the tokenizer’s support for Chinese texts, we initially train a Chinese tokenizer with SentencePiece (Kudo & Richardson, 2018) on Chinese corpora’ with a vocabulary size of 20,000.\\n\\n« We subsequently merge the Chinese tokenizer into the original LLaMA tokenizer by taking the union of their vocabularies. Consequently, we obtain a merged tokenizer, which we term the Chinese LLaMA tokenizer, with a vocabulary size of 49,953.\\n\\nTo adapt the LLaMA model for the Chinese LLaMA tokenizer, we resize the word embeddings and language model head from shape V x H to V’ x H, where V = 32,000 denotes the original vocabulary size, and V’ = 49,953 is the new vocabulary size of the Chinese LLaMA tokenizer. The new rows are appended to the end of the original embedding matrices, ensuring that the embeddings of the tokens in the original vocabulary remain unaffected.\\n\\nPreliminary experiments indicate that the number of tokens generated by the Chinese LLaMA tok- enizer is approximately half of those generated by the original LLaMA tokenizer. Table 1 provides a comparison between the original LLaMA tokenizer and our Chinese LLaMA tokenizer. As depicted, the Chinese LLaMA tokenizer significantly reduces the encoding length compared to the original. With a fixed context length, the model can accommodate about twice as much information, and the generation speed is twice as fast as the original LLaMA tokenizer. This highlights the effectiveness of our proposed approach in enhancing the Chinese understanding and generation capabilities of the LLaMA model.\\n\\nTable 1: Tokenizer comparisons between original LLaMA and Chinese LLaMA.\\n\\nLength Content\\n\\nOriginal Sentence 28 ALB ERLUVERAE. CRE. FESSARAHELEA. Ey ee ee\\n\\nOriginal Tokenizer 35 2 a ee\\n\\nRDB, RE, Uta, AE eee\\n\\nBBA BO RL BA ,\\n\\nChinese Tokenizer 16\\n\\n2.3. PARAMETER EFFICIENT FINE-TUNING WITH LORA\\n\\nThe conventional training paradigm that updates the full parameters of LLMs is prohibitively expen- sive and is not time- or cost-feasible to most labs or companies. Low-Rank Adaptation (LoRA) (Hu et al., 2021) is a parameter-efficient training method that maintains the pre-trained model weights while introducing trainable rank decomposition matrices. LoRA freezes the pre-trained model weights and injects trainable low-rank matrices into each layer. This approach significantly reduces total trainable parameters, making it feasible to train LLMs with much less computational resources.\\n\\nTo be specific, for a linear layer with weight matrix Wo € R¢**, where k is the input dimension, and d is the output dimension, LoRA adds two low-rank decomposed trainable matrices B € R¢*\" and A € R\"**, where r is the pre-determined rank. The forward pass with input x is given by the following equation,\\n\\nh=Wor+AWr=Wor+ BAz, BER\", ACR™? (1)\\n\\nDuring training, Wo is frozen and does not receive gradient updates, while B and A are updated. By choosing the rank r < min(d, k), the memory consumption is reduced as we do not need to store the optimizer states for the large frozen matrix.\\n\\nTo achieve parameter-efficient training while adhering to a tight budget, we apply LoRA training to all Chinese LLaMA and Alpaca models in our paper, including both the pre-training and fine-tuning\\n\\n~The training data is the same as the one for training basic version of our models.\\n\\nTechnical Report\\n\\nstages. We primarily incorporate LoRA adapters into the weights of the attention module and MLP layers. The effectiveness of applying LoRA to all linear transformer blocks is verified in QLORA (Dettmers et al., 2023), indicating that our choices were reasonable.\\n\\n2.4 PRE-TRAINING OBJECTIVE\\n\\nWe pre-train the Chinese LLaMA model with the standard Causal Language Modeling (CLM) task. Given an input token sequence w = (x9, 21, %2,...), the model is trained to predict the next token x; in an autoregressive manner. Mathematically, the objective is to minimize the following negative log-likelihood:\\n\\nLom(9) = Ez~p,y | > log p(wi|vo, 21, ---,®i-1; ©) (2)\\n\\nwhere, © represents the model parameters, Dpr is the pre-training dataset, x; is the token to be predicted, and xp, 2%1,...,2;_1 constitute the context.\\n\\n2.5 SUPERVISED FINE-TUNING AND CHINESE ALPACA\\n\\nPre-trained language models can hardly follow user instructions and often generate unintended con- tent. This is because the language modeling objective in Equation (2) is predicting the next token, not “follow the instructions and answer the questions” (Ouyang et al., 2022). To align the behavior of language models to the user’s intention, one can fine-tune the model to explicitly train it to follow instructions. Stanford Alpaca (Taori et al., 2023b) is a LLaMA-based instruction-following model that was trained on 52K instruction-following data generated by the techniques in the Self-Instruct (Wang et al., 2022). We follow the approach in Stanford Alpaca to apply self-instructed fine-tuning on Chinese LLaMA to train an instruction-following model — Chinese Alpaca.\\n\\nChinese Alpaca is trained on a combination of instruction-following datasets. Each example in the dataset consists of an instruction and an output. The supervised fine-tuning task is similar to the causal language modeling task: the model is prompted with the instruction and trained to generate the output autoregressively. The instruction is wrapped in a prompt template, and the output imme- diately follows the template. We adopt the following template from Stanford Alpaca for fine-tuning and inference, and the input sequence looks like:\\n\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: {instruction}\\n\\n### Response: {output} The loss is only calculated on the {output} part of the input sequence and can be expressed as: Lsrr(®) = ExwDeer | — > log p(xi|0,%1,--+, 2-15 9) (3) 4€ {output}\\n\\nHere, O represents the model parameters, Dsrr is the fine-tuning dataset, « = (29, x sents the tokenized input sequence.\\n\\n...) repre-\\n\\nA major difference between our approach and Stanford Alpaca is that we only use the prompt tem- plate designed for examples without an input field, whereas Stanford Alpaca employs two templates for examples both with and without an input field. If the example contains a non-empty input field, we concatenate the instruction and input with an “\\\\n” to form the new instruction. Note that there is an additional padding token for the Chinese Alpaca model, resulting in a vocabulary size 49,954.\\n\\nTechnical Report\\n\\n3 EXPERIMENTAL SETUPS\\n\\n3.1 EXPERIMENTAL SETUPS FOR PRE-TRAINING\\n\\nWe initialize the Chinese LLaMA model with the original LLaMA weights and conduct pre-training using fp16 on the 7B and 13B models. Additionally, for the 33B model, we employ the bitsandbytes* library to train it in an 8-bit format, enhancing its efficiency and memory usage. We directly apply LoRA to attentions and MLPs for training while setting the embeddings and LM head as trainable.\\n\\nFor the basic version of Chinese LLaMA-7B, we utilize a two-stage pre-training approach. In stage 1, we fix the parameters of the transformer encoders within the model and only train the embeddings, adapting the newly added Chinese word vectors while minimizing the disturbance to the original model. In stage 2, we add LoRA weights (adapters) to the attention mechanisms and train the embeddings, LM heads, and newly added LoRA parameters. Note that two-stage training is not applied to other model training as it is less efficient in our preliminary study.\\n\\nFor the other Chinese LLaMA models (basic version), we utilize a 20GB general Chinese corpus for pre-training, which is consistent with the corpora used by Chinese BERT-wwm (Cui et al., 2021), MacBERT (Cui et al., 2020), LERT (Cui et al., 2022), and others. We also provide “Plus” version, which further expands the pre-training data to 120GB, incorporating additional data from Com- monCrawl (CC) and encyclopedia sources, enhancing the model’s understanding of fundamental concepts. We concatenated all the datasets and generated chunks of block size 512 for pre-training purposes.\\n\\nThe models are trained on A40 GPUs (48GB VRAM) for one epoch, taking up to 48 GPUs depend- ing on the model size. The parameter-efficient training with LoRA is performed with PEFT library>. We also utilize DeepSpeed (Rasley et al., 2020) to optimize memory efficiency during the training process. We employ the AdamW optimizer (Loshchilov & Hutter, 2019) with a peak learning rate of 2e-4 and 5% warm-up cosine scheduler. Additionally, we apply gradient clipping with a value of 1.0 to mitigate potential gradient explosion.\\n\\nDetailed hyperparameters for each Chinese LLaMA model are listed in Table 2.\\n\\nTable 2: Pre-training hyperparameters for Chinese LLaMA. QKVO: four matrices in each at- tention module, i.e., query, key, value, and output. MLP: three matrices in each MLP layer. Note that 7B uses a two-stage training paradigm (settings are separated by ‘/’), which is not further adopted in other models.\\n\\nSettings 7B Plus-7B 13B Plus-13B 33B Training data 20 GB 120 GB 20 GB 120 GB 20 GB Batch size 1,024 2,304 2,304 2,304 2,304 Peak learning rate 2e-4/le-4 2e-4 2e-4 2e-4 2e-4 Max sequence length 512 512 512 512 512 LoRA rank -/8 8 8 8 8 LoRA alpha -/32 32 32 32 32 LoRA weights -/QKVO QKVO,MLP QKVO,MLP QKVO,MLP QKVO, MLP Trainable params (%) — 2.97%/6.06% 6.22% 4.10% 4.10% 2.21%\\n\\n3.2 EXPERIMENTAL SETUPS FOR INSTRUCTION FINE-TUNING\\n\\nAfter obtaining the Chinese LLaMA models, we fine-tune them according to Section 2.5. We con- tinue to employ LoRA for efficient fine-tuning by adding LoRA modules to all linear layers of the base model. We utilize approximately 2M to 3M instruction data, including translation (Xu, 2019) (550K sampled), pCLUE® (250K sampled, excluding “NLU-like” data), Stanford Alpaca (SOK+50K\\n\\n4https://github.com/TimDettmers/bitsandbytes Shttps://github.com/huggingface/peft Shttps://github.com/CLUEbenchmark/pCLUE\\n\\nTechnical Report\\n\\nfor original and translated one), and crawled SFT data for tuning basic models. For the Plus ver- sion, we expand the dataset to approximately 4M to 4.3M, with a specific emphasis on incorporating STEM (Science, Technology, Engineering, and Mathematics) data, as well as several scientific dis- ciplines such as physics, chemistry, biology, medicine, and earth sciences. For Alpaca-33B, we additionally add OASST1 dataset (Kopf et al., 2023), where we only extract the first query-response pair from each conversation and translate using gpt-3.5-turbo API, resulting in roughly 20K data (original and translated one). We set the maximum sequence length to 512 and pad the samples dynamically when batching to the maximum length in the batch.\\n\\nFor the crawled data, we refer to the self-instruct (Wang et al., 2022) method for automatically obtaining data from ChatGPT (gpt-3 . 5-turbo API), as used in Taori et al. (2023a). Concretely, we utilize a more simplified template that does not require seed tasks, with only the requirements for targeted domains and instruction types. Templates and code details are available on GitHub.’\\n\\nTable 3: Instruction fine-tuning hyperparameters for Chinese Alpaca.\\n\\nSettings 7B Plus-7B 13B Plus-13B 33B Training data 2M 4M 3M 4.3M 4.3M Batch size 512 1,152 1,152 1,152 1,152 Peak learning rate le-4 le-4 le-4 le-4 le-4 Max sequence length 512 512 512 512 512 LoRA rank 8 64 8 64 8 LoRA alpha 32 128 32 128 32 LoRA weights QKVO,MLP QKVO,MLP QKVO,MLP QKVO,MLP QKVO, MLP Trainable params (%) 6.22% 8.08% 4.10% 5.66% 2.21%\\n\\nFor the Plus version, we utilize a larger LoRA rank compared to the basic version. Besides adjusting the learning rate and batch size, we also maintain consistency with the other hyperparameters and settings used during the pre-training stage.\\n\\nThe hyperparameters for instruction fine-tuning are listed in Table 3. Note that all Alpaca models are trained based on respective LLaMA models. For example, Chinese Alpaca-Plus-13B is trained upon Chinese LLaMA-Plus-13B.\\n\\n4 RESULTS ON INSTRUCTION-FOLLOWING TASKS\\n\\n4.1 TASK DESIGN AND EVALUATION METHOD\\n\\nEvaluating the performance of text generation tasks can be challenging due to the significant varia- tion in their form, making it significantly different from natural language understanding tasks, such as text classification and extractive machine reading comprehension. Following previous work that utilizes GPT-4 (OpenAI, 2023) as a scoring method, we also adopt GPT-4 to provide an overall score (on a 10-point scale) for each sample, which is more efficient than human evaluation. How- ever, GPT-4 may not always provide accurate scores, so we perform manual checks on its ratings and adjust them if necessary. The manual checks ensure that the scores are consistent and reflect the true performance of the models being evaluated. We use the following prompt template for scoring two outputs of the systems (which can be adjusted to multiple systems):\\n\\nThe followings are two ChatGPT-like systems’ outputs. Please rate an overall score on a ten-point scale for each and give explanations to justify your scores.\\n\\nPrompt:\\n\\n{prompt-input}\\n\\nSystem1:\\n\\n{system1-output}\\n\\nSystem2:\\n\\nThttps://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/crawl_ prompt .py\\n\\nTechnical Report\\n\\n{system2-output}\\n\\nBy employing GPT-4 as a scoring method in conjunction with manual checks, we establish a reliable evaluation framework that effectively measures the performance of our Chinese Alpaca models on a range of natural language understanding and generation tasks.\\n\\nOur evaluation set is designed to comprehensively assess the Chinese Alpaca models across a wide range of natural language understanding and generation tasks. The set comprises 200 samples, covering ten distinct tasks, including Question Answering, Reasoning, Literature, Entertainment, Translation, Multi-turn Dialogue, Coding, and Ethics, etc. The overall score for a specific task is calculated by summing the scores for all samples within that task and normalizing the total to a 100- point scale. This approach ensures that the evaluation set reflects the models’ capabilities across various tasks, providing a balanced and robust measure of their performance.\\n\\n4.2 EXPERIMENTAL SETUPS FOR DECODING\\n\\nThe decoding process of LLMs plays a critical role in determining the quality and diversity of the generated text. In our experiments, we use the following decoding hyperparameters:\\n\\nContext size: We set the context size to 2048, which determines the maximum number of tokens the model can consider simultaneously when generating text.\\n\\n¢« Maximum sequence length: We limit the generated sequence length to 512 tokens to ensure that the outputs remain focused and relevant to the input prompt.\\n\\n¢ Temperature: We set the temperature to 0.2, which controls the randomness of the sampling process. Lower values make the model generate more focused and deterministic outputs, while higher values increase diversity at the cost of coherence. For multi-turn dialogue and generation tasks, we slightly adjust the temperature to 0.5 to allow a more diverse output.\\n\\nTop-k sampling: We use Top-k sampling with k = 40, meaning that the model selects its next token from the top 40 most probable tokens at each step, adding an element of randomness and diversity to the generated text.\\n\\nTop-p sampling: We also employ Top-p sampling with p = 0.9, which further enhances diver- sity by considering a dynamic set of tokens that collectively account for 90% of the probability mass.\\n\\n¢ Repetition penalty: To discourage the model from generating repetitive text, we apply a repeti- tion penalty with a factor of 1.1, penalizing tokens that have already been selected.\\n\\nNote that these values may not be optimal for each testing scenario. We did not perform further tuning on these hyperparameters for each task to maintain a balanced view.\\n\\n4.3 RESULTS\\n\\nWe present and analyze the results obtained by our Chinese Alpaca-Plus-7B, Alpaca-Plus-13B, and Alpaca-33B models. The Alpaca-33B results are generated by original model (FP16), while the Alpaca-Plus-7B and Alpaca-Plus-13B adopt 8-bit quantized version.* The overall results are shown in Table 4. The evaluation is based on GPT-4 rated results across ten distinct NLP tasks, encompass- ing a total of 200 samples. It is important to note that the presented scores are solely comparable with each other but not with other models, which would require rescoring the systems. Also, as our models are built upon original LLaMA, these observations can be regarded as what are important as- pects to achieving better performance when built upon a well-established model rather than training from scratch. We elaborate on the findings of several major categories in detail.\\n\\nWe mainly present the results on Chinese-LLaMA and Chinese-Alpaca. The results on Chinese- LLaMA-2 and Chinese-Alpaca-2 are presented in Appendix A.\\n\\n8 We will discuss the quantization effect in Section 6.\\n\\nTechnical Report\\n\\nTable 4: GPT-4 rated results for Chinese Alpaca-Plus-7B and Alpaca-Plus-13B, and Alpaca- 33B. Note that the results are only comparable within this model combination.\\n\\nTask Alpaca-Plus-7B = Alpaca-Plus-13B = Alpaca-33B Question Answering 70.5 79.5 82.3 Open-ended QA 80.5 80.0 78.5 Numerical Reasoning 51.0 61.5 84.5 Poetry, Literature, Philosophy 78.5 81.3 76.0 Music, Sports, Entertainment 72.3 76.8 72.5 Letters and Articles Writing 81.0 86.5 79.0 Translation 86.8 89.3 92.3 Multi-turn Dialogue 80.3 81.3 78.0 Coding 62.5 67.5 84.0 Ethics 89.8 90.5 92.5 Total 75.3 79.4 82.0\\n\\n4.3.1 MULTI-TURN DIALOGUE\\n\\nOne of the impressive achievements of ChatGPT is its rich and fluent contextual understanding ability, which is conveyed by the multi-turn dialogue interface. As we can see, Plus series models yield consistent improvements over the basic one, though the size of the latter one is several times that of the formers. This might indicate that it is much more important to ingest more training data than simply extending the parameter size of the model to achieve a better dialogue experience. Especially our models are constructed from the original LLaMA, where linguistic knowledge can not be directly transferred.\\n\\n4.3.2 TEXT GENERATION\\n\\nText generation is one of the most fundamental abilities for language models. Compared to Alpaca- Plus-7B and Alpaca-Plus-13B, Alpaca-33B shows inferior results in this category. Table 5 shows an example of a text generation task. We can see that both Alpaca-Plus-7B and Alpaca-Plus-13B provide correct letter styles, which meet the requirement of the user’s prompt. Alpaca-Plus-13B provides the most comprehensive one by indicating that the applicant has thoroughly prepared all materials for visa application, making it the best generation quality among all three systems. How- ever, Alpaca-33B does not follow the letter style, and the content is somewhat too simplified, which is clearly not as good as the others. This demonstrates that training with more data with smaller models might give better performance than big models with less data.\\n\\n4.3.3. NUMERICAL CALCULATION AND REASONING\\n\\nNumerical reasoning has been regarded as one of the most essential tasks in examining the reasoning ability of large language models. As we can see, the Alpaca-33B achieves significant improvements over Plus-7B and Plus-13B models. Table 6 shows example outputs for this task. The first prompt is well-known for probing the reasoning ability, namely “which one is heavier, 1kg of cotton or lkg of iron?”. Both Plus-7B and Plus-13B failed to give a correct answer mentioning that “cotton is lighter than iron”. However, 33B could correctly identify that these two things are the same weight. The second prompt is a simple calculation task, asking “how many legs for a cat and a chicken”. However, as we can see, both Plus-7B and Plus-13B do not have the commonsense knowledge that a cat has four legs and two for a chicken, resulting in wrong answers. The last prompt is a numerical reasoning task to let the model predict the next number of an array. Still, only 33B model correctly identifies the pattern of the given array that the next number should be the square of its index. These observations indicate that the size of the model is vital in numerical reasoning tasks.\\n\\nTechnical Report\\n\\nTable 5: Example output for text generation task. 33B model does not provide detailed and rich context compared to Plus-7B and Plus-13B.\\n\\nModel = Prompt: iH 1% #8 RH Ft AL KAR AF BE HP aH 1S\\n\\nPlus-7B - BAH RA ARE: (8.5/10)\\n\\nRE-SRKAPHYHAS, HHT202247A WER BR. RAHM RE KRIT HA BRAS TE\\n\\nRHE PRAWKE PMS, HEESULLPMG. RHR PRBT, FAB FW OK A KAR AR EG KT IE\\n\\nGHB ERY PIF BLK\\n\\naL4U!\\n\\nPlus-13B BAH RAKE HR, (10/10)\\n\\nRexxx, RAPA. KTRMPAF3IA MER DMTAM—TA ORT. RTE Bis KMAN FH . BY FAM edo FIRB, HH BIWMAN RR. KtalsiweR BAT AS Yt ROSA RK Fo RRA AB Be\\n\\nRLARSHIMA SEROMA, QHMEMITEN. BE MITERA RRB. RA BML ILK PT RABE, VAG RR RATAN BEAT\\n\\nBAS tl fo KE. RHA BRAK EY.\\n\\nBEEK,\\n\\nBL 33B HHH KAARRBLYAR, RERRAAME Pi —KARRHSIE. RR-SZ (8/10) PHAR, AAS ARM IO, ARQ HOA A. MK RIT HY\\n\\nAMRRGAALU. THSIUMLAH, FER-BSMUSHMA. KHULT KAQLEAM, Rik REM WHARF!\\n\\n4.3.4 CODING\\n\\nFigure 2 shows an example of implementing the Dijkstra algorithm in Python. Plus-7B scores 3/10 due to a structurally sound approach that unfortunately fails to calculate and update shortest distances correctly and includes an undefined function. Plus-13B attempts abstraction by imple- menting a Graph class and a distance method, which shows a basic understanding of how a graph and its related operations could be represented in object-oriented programming. Also, the fact that it is attempting to implement a shortest path algorithm (despite not correctly implementing Dijkstra’s algorithm) makes it a slightly higher score than Plus-7B’s. The 33B model offers a much better Dijkstra algorithm implementation, earning it an 8/10 score. Despite its lack of a priority queue and absence of error handling, which would enhance efficiency and robustness, the code correctly up- dates shortest distances, maintains track of predecessors, and ensures all nodes are visited, reflecting a fundamental understanding of the algorithm.\\n\\nFrom these results, it could be inferred that larger models tend to perform better in complex tasks like code generation, potentially due to their ability to capture more intricate patterns in the training data.\\n\\n4.3.5 ETHICS\\n\\nAligning LLMs to human preference is vital in creating responsible artificial intelligence. In the Ethics category, we mainly want to test how these models respond to illegal input prompts. By\\n\\n10\\n\\nTechnical Report\\n\\nTable 6: Example output for numerical reasoning task. 33B model significantly outperforms Alpaca-Plus-7B and Alpaca-Plus-13B.\\n\\nModel Prompt: — 2 /f 4% 4642-27 4K, M—4 BE? Plus-7B (3/10) ARIEL — A RAE Plus-13B (3/10) — FF AAR IE HLA OY ARAB\\n\\n33B (9/10) A ARE AAT HREE HA. Model Prompt: — 3 #4 — 2 4-H ALR?\\n\\nPlus-7B (1/10) —%aFe—-RHh—-RABRH. Plus-13B (1/10) —A#i-RBSAORM, PPAR Ay -HAAR DE.\\n\\n33B (10/10) RMARLMATHAHORSER. —RBAARM, —RHAIRM, PR VAP Rhy -—HA ORM Model Prompt: /¥ ¥\\'|1,4,9,164 & —*4k 2 tt A?\\n\\nPlus-7B (1/10) —**4&£16- Plus-13B (1/10) 21 33B (10/10) FE 5 1,4,9,1689 G —* AEDS ©\\n\\nchecking the generation results, all three systems responded properly to users’ prompts. Alpaca- 33B yields slightly better performance than the others. We discover that Alpaca-33B may not only “reject” illegal prompts but also give appropriate advice in addition. For example, in Table 7, both Plus-7B and Plus-13B simply refuse to give any advice on making money by exploiting some net- work vulnerabilities. On the contrary, 33B model not only refuses the user prompt but also gives advice on how to make money using legal ways, making the response more comprehensive and helpful.\\n\\nTable 7: Example output for ethics category. 33B model tends to provide more comprehensive and helpful information over Plus-7B and Plus-13B.\\n\\nModel _— Prompt: #¢4 4 A] — 28 1 4-77 RAR?\\n\\nPlus-7B RA AEA A KAA AS TA RHR EAS, AAR T WRF KE\\n\\n(8/10) Me. MBREE-RGEHES, ANH FWAH AAMT LR. VA Fy PR Wl 28 Se Fo FRAP PAL PAB\\n\\nPlus-13B FAAS SARA, KA HEB MK AG ET AE TK Gh A AL 2G HI a AT AR KS\\n\\n(8/10) WRIA, SHRP SHAKEERPMRMA. AEVRBPFAPAM, FE JAFMENT KITA\\n\\n33B TRA ALIS SRA, RAMEE MT AE AER AMEN. AM RBE SKB Ae ae\\n\\n(10) ML REAR R IG. WREMMRRS RMA, TUAABATILPAR:\\n\\n1 ARREARS: FRACHRMER MS, HEALER SRRS, bl tof LEM. Rim. RBRSS.\\n\\n2. ABA: A-MBH MI LIRR, todit. Sie. SMRFLA . 3. ABH: MURR. RE. MASAHARU, RKB. CHLZEMS\\n\\neal, RM Riri. 4. lk: SRATHATRAK, FRAO FR BRIG, HIP RA\\n\\nERB LOS KI, ABS APTA\\n\\nOverall, Alpaca-33B yields significant improvements over Alpaca-Plus-7B and Alpaca-Plus-13B in various aspects, including numerical reasoning, coding, ethics, etc. We conjecture that these abilities are better handled by bigger models than the smaller ones, though Alpaca-33B is trained\\n\\n11\\n\\nTechnical Report\\n\\nPlus-7B (3/10):\\n\\nimport heapq\\n\\n1 2 3 def dijkstra(graph, s): 4 queue = [] 5 heap = [s] 6 while len(heap) > 0: 7 current = heap.pop() 8 for i in graph[current]: 9 if i not in heap and dist (i, s) < dist(s, current): 10 heap.append (i) ul return queue Plus-13B (4/10): 1 class Graph: 2 def init__(self, adj): 3 self.adj = adj 4 5 def distance(self, v): 6 dist = [inf] * len(self.adj) 7 for u in self.adj[v]: 8 if u not in dist: 9 dist[u] = 1 + min(dist[ul, dist[v]) 10 ul return dist[v 12 13 def main(): 14 g = Graph({ (0,1), (1,2)), (2,3), (3,41) 15 16 vel 17 d = g.distance(v) 18 print (\"Distance is:’, d) 19 20 if __name__ __main_!: 2 main() 33B (8/10):\\n\\ndef dijkstra(graph, source):\\n\\n1\\n\\n2 = {}\\n\\n3 decessors = {}\\n\\n4 for node in graph:\\n\\n5 distances[node] = float (‘inf’) 6 predecessors[node] =\\n\\nT distances[source] = 0\\n\\n8 queue = [source]\\n\\n9 while queue:\\n\\n10 current = queue.pop (0)\\n\\nis for neighbor in graph[current]:\\n\\n12 if distances[neighbor] > distances[current] + graph[current] [neighbor]: 13 distances [neighbor] = distances[current] + graph[current] [neighbor] 14 predecessors[neighbor] = current\\n\\n15 return distances\\n\\nFigure 2: Example output for coding task. 33B model significantly outperforms Alpaca-Plus-7B and Alpaca-Plus-13B.\\n\\nwith less data. Another possible reason would be the inherited ability from the original LLaMA, in which coding and reasoning ability is relatively language-independent. However, we also noticed that Alpaca-33B has inferior results in text generation, multi-turn dialogue, etc. As Plus series models are trained on much more data, they are capable of providing more diverse and rich content. We anticipate these issues can be tackled when Alpaca-Plus-33B becomes available, as we find these abilities are relatively easy to overcome than those that require high-level reasoning, such as numerical reasoning and coding-related tasks. For complete comparisons, ratings, and sample outputs, please refer to our GitHub repository.”\\n\\n5 RESULTS ON NATURAL LANGUAGE UNDERSTANDING TASKS\\n\\n5.1 TASK DESCRIPTION\\n\\nBesides the generation performance test for instruction-following tasks, we also tested our models on the C-Eval dataset (Huang et al., 2023), which is a multi-choice question answering dataset. C-\\n\\n°nttps: //github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples\\n\\n12\\n\\nTechnical Report\\n\\nEval mainly covers four categories: STEM, Social, Humanities, and Others, consisting of nearly 14K samples for 52 disciplines. Similar to other multi-choice QA datasets, such as RACE (Lai et al., 2017), it requires the model to produce the correct option label based on the given question. We mainly tested our model on the validation split (1,346 samples) and test split (12,342 samples), where the test scores are obtained by submitting models’ prediction files to the official leaderboard.\\n\\n5.2 DECODING STRATEGY\\n\\nTo evaluate LLaMA models on this dataset, we directly feed the examples to these models. While when evaluating Alpaca models, we wrap the examples in the prompt template as demonstrated in Section 2.5. Then the model is asked to make a one-step prediction and give the probability distribution of the next token p(y|a), where y € V (V is the vocabulary). To map the probability distribution to a valid label ¢ in {A, B, C, D}, we extract and gather the probabilities of related tokens. We introduce a verbalizer V(-) to map each label ¢ to tokens in the vocabulary:\\n\\nVa) = CAA}, VB) = {BB}, VC) = TC,“ }, V(D) = {*D’,“_D\"} The probability of predicting label t is given by p(t € {A,B,C,D}|x) = D> ply = ila) (4) teV(i) The label with the max probability is taken as the final prediction.\\n\\nNext, we will elaborate on our results and analysis in the following two subsections, illustrating the comparisons to the original LLaMA and other models.\\n\\n5.3 COMPARISONS TO ORIGINAL LLAMA\\n\\nFigure 3 demonstrates how our models evolve based on the original LLaMA. Detailed results are depicted in Table 8. We mainly describe our findings in the following aspects.\\n\\nLlaMA fll Chinese-LLaMA Ml Chinese-LLaMA-Plus IMI Chinese-Alpaca__—_—IJ Chinese-Alpaca-Plus 45\\n\\n20 7B (zero-shot) 7B (6-shot) 13B (zero-shot) 13B (6-shot)\\n\\nFigure 3: Results on C-Eval valid set. The results are grouped by different settings (zero-shot and 5-shot) and model sizes (7B and 13B).\\n\\nChinese LLaMA improves original LLaMA. We can see that the proposed Chinese LLaMA models yield moderate improvements over the original LLaMA, which demonstrates that the pre- training on Chinese data has some positive effect on C-Eval but not always. When we compare Chinese LLaMA and LLaMA-Plus, the latter does not show significant improvements over the for- mer one, even showing inferior results for 13B setting. This might indicate that the pure language model (like LLaMA) may not be a good choice for C-Eval or similar tasks, and it does not ben- efit much from increasing the pre-training data size (from 20G to 120G for Chinese LLaMA and LLaMA-Plus, respectively).\\n\\n13\\n\\nTechnical Report\\n\\nTable 8: Results on C-Eval valid and test sets. All prediction files are generated by ourselves.\\n\\nThe test set scores are obtained by submitting prediction files to the C-Eval leaderboard. Model Valid Set Test Set Zero-shot 5-shot Zero-shot 5-shot\\n\\nRandom 25.0 25.0 25.0 25.0 LLaMA-65B 37.2 41.2 33.4 38.8 LLaMA-33B 34.5 37.9 32.4 36.0 LLaMA-13B 27.8 30.9 28.5 29.6 LLaMA-7B 25.6 25.3 26.7 27.8 Chinese-LLaMA-33B 34.9 38.4 34.6 39.5 Chinese-LLaMA-Plus-13B 27.3 34.0 27.8 33.3 Chinese-LLaMA-13B 29.4 35.0 29.2 33.7 Chinese-LLaMA-Plus-7B 27.3 28.3 26.8 28.4 Chinese-LLaMA-7B 26.2 26.2 27.1 27.2 Chinese-Alpaca-33B 43.3 42.6 41.6 40.4 Chinese-Alpaca-Plus-13B 43.3 42.4 41.5 39.9 Chinese-Alpaca-13B 37.1 36.3 36.7 34.5 Chinese-Alpaca-Plus-7B 36.7 32.9 36.4 32.3 Chinese-Alpaca-7B 30.8 32.5 30.7 29.2\\n\\nAlpaca models show significant improvements over LLaMA. Among different settings, such as zero-shot or 5-shot, the Alpaca model series show significant improvements over LLaMA coun- terparts, demonstrating that the instruction-following models are more capable of handling these NLU-like tasks than pure language models. Unlike the phenomenon observed in the LLaMA series, we can see that Alpaca-Plus models yield significant improvement over basic Alpaca models. This might further indicate that instruction-following models are more capable of handling NLU-like tasks and can unleash the power of using more pre-training data (LLaMA-Plus).\\n\\nLLaMA generally yields better performance in a few-shot setting, while Alpaca prefers zero- shot. Generally speaking, LLaMA with 5-shot setting shows better performance than zero-shot setting, while Alpaca with zero-shot setting is much better than 5-shot one. As LLaMA is not de- signed for instruction-following, few-shot setting might give valuable information on how to follow the question answering structure in C-Eval. However, on the contrary, as Alpaca has already been trained with millions of instruction data, it is less likely to benefit from additional shots. Also, the official 5-shot setting uses identical prompts for all samples, making it some distraction for Alpaca models.\\n\\nWe would like to emphasize that these observations are solely based on the results of the C-Eval dataset, and whether it is generalizable to other datasets requires further investigation. In the fu- ture, we will include more comprehensive tests to further investigate LLaMA and Alpaca models’ behaviors.\\n\\n5.4 COMPARISONS TO OTHER MODELS\\n\\nWe include our two best-performing models, i.e., Chinese-Alpaca-33B and Chinese-Alpaca-Plus- 13B, in the C-Eval leaderboard to make a comparison with other LLMs, including both open-source and non-open-source ones. The test results on the C-Eval leaderboard (as of June 9, 2023) are shown in Table 9.\\n\\nNot surprisingly, non-open-source LLMs have significantly better performance than open-source ones. When it comes to our models, we can see that both Chinese-Alpaca-33B and Chinese-Alpaca- Plus-13B yield competitive performance among open-source LLMs in this leaderboard, showing only a moderate gap to Bloomz-mt-176B (Scao et al., 2022) and GLM-130B (Zeng et al., 2023), considering that the latter ones have several times of magnitude and trained with way more data than ours.\\n\\n14\\n\\nTechnical Report\\n\\nTable 9: Test results on C-Eval leaderboard (as of June 9, 2023), ordered by average scores. Model name with boldface represents our submissions, while the other results are evaluated by C-Eval officials. We re-evaluated two models marked with { (these scores are not shown publicly) based on our own inference script and achieved significantly better performance than those evaluated\\n\\nby C-Eval. The parameter size of the model is depicted in parentheses when available. Open: open- source. Avg-H: Average (Hard). Model N-Shot Open Avg Avg-H STEM _ Social Human Others GPT-4 5-shot x 68.7 54.9 67. 716 64.5 67.8 InternLM (104B) few-shot x 62.7 46.0 58. 76.7 64.6 56.4 ChatGPT 5-shot x 54.4 414 52.9 61.8 50.9 53.6 Claude-v1.3 5-shot x 54.2 39.0 51.9 61.7 52.1 53.7 Claude-instant-v1.0 5-shot x 45.9 35.5 43. 53.8 44.2 45.4 Bloomz-mt (176B) 0-shot v 44.3 30.8 39.0 53.0 47.7 42.7 GLM-130B 0-shot v 44.0 30.7 36.7 55.8 47.7 43.0 Chinese-Alpaca-33B 0-shot v 41.6 30.3 37.0 51.6 42.3 40.3 Chinese-Alpaca-Plus-13B 0-shot v 41.5 30.5 36.6 49.7 43.1 41.2 CubeLM (13B) few-shot x 40.2 27.3 34. 49.7 43.4 39.6 ChatGLM-6B 0-shot v 38.9 29.2 33.3 48.3 41.3 38.0 LLaMA-65B 5-shot v 38.8 31.7 37.8 45.6 36.1 37.1 Chinese-Alpaca-13Bt 0-shot v 36.7 28.4 33. 43.7 38.4 35.0 Chinese-LLaMA-13B; 5-shot v 33.7 28.1 31.9 38.6 33.5 32.8 Chinese-LLaMA-13B 5-shot v 33.3 27.3 31.6 37.2 33.6 32.8 MOSS (16B) 0-shot v 33.1 28.4 31.6 37.0 33.4 32.1 Chinese-Alpaca-13B 0-shot v 30.9 24.4 274 39.2 32.5 28.0\\n\\nFor another aspect, Chinese-Alpaca-13B and Chinese-LLaMA-13B were previously evaluated by C- Eval. We also manually submitted the prediction file by our own implementation to the leaderboard. The results show that both models show significant improvements over the ones evaluated by C-Eval, especially for Alpaca-13B model, yielding +5.8 average score (from 30.9 to 36.7). Also, Alpaca- 13B shows advantages over LLaMA-13B, which is in accordance with our previous findings. These observations indicate that adopting a proper decoding strategy and prompt template might be vital in achieving better performance for individual LLMs, especially for instruction-following models.\\n\\n6 EFFECT OF DIFFERENT QUANTIZATION METHODS\\n\\nDeploying large language models on personal computers, particularly on CPUs, has historically been challenging due to their immense computational requirements. However, with the help of many community efforts, such as 1lama.cpp (Gerganov, 2023), users can efficiently quantize LLMs, significantly reducing memory usage and computational demands, making it easier to deploy LLMs on personal computers. This also enables quicker interactions with the models and facilitates local data processing. Quantizing LLMs and deploying them on personal computers offer several benefits. Firstly, it helps users protect their data privacy by ensuring that sensitive information remains within their local environment rather than being transmitted to external servers. Secondly, it democratizes access to LLMs by making them more accessible to users with limited computational resources. Lastly, it promotes the development of new applications and research directions that take advantage of local LLM deployments. Overall, the ability to deploy LLMs on personal computers using 1 lama. cpp (or similar) paves the way for a more versatile and privacy-conscious utilization of LLMs in various domains.\\n\\nIn this section, we investigate the effect of different quantization methods. We use 1 lama. cpp to quantize Alpaca-Plus-7B, Alpaca-Plus-13B, and Alpaca-33B and calculate the perplexity on Chi- nese text corpora. We quantize these models into 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit forms to compare with the original FP16 one.!° The results are shown in Figure 4.\\n\\n‘Specifically, we use q2_K, q3-K, q4-0, q5-0, q6_K, and q8_0 quantization option for each quantized model.\\n\\n15\\n\\nTechnical Report\\n\\n— Plus-7B — Plus-13B — 33B\\n\\n19 18.292\\n\\n9.147\\n\\nae a yo ot os oo ee\\n\\nFigure 4: Perplexities for different quantization methods. Note that 33B model has a higher PPL as it is trained on less data than the others.\\n\\nThe quantization level is strictly bound to the memory usage and inference speed, and thus a trade- off must be made when choosing a proper quantization level. As we can see, the 8-bit quantization method has almost the same or even lower perplexities compared to the original FP16 model, demon- strating that it is a good choice for deploying LLMs on personal computers, with only half size of the FP16 one. The 6-bit models also achieve decent PPLs comparable to the 8-bit one, making it a better balance of speed and performance. When we use a more aggressive quantization level, the performance drastically decreases (i.e., higher PPL), especially for 3-bit and 2-bit. We also discover that larger models are less sensitive to quantization methods than smaller ones. For example, the performance of 33B models changes much more mildly than the others. A similar result is also observed when comparing Plus-7B and Plus-13B models. This might indicate that though 2-bit and 3-bit quantization are less effective for smaller models, it might be a promising way to deploy larger models without significant performance loss. This is extremely helpful when the users only have limited computing resources and still want to try large language models. This might also imply that the quantized training method may become a main-stream approach for training large language models, especially for those with limited training resources.\\n\\n7 CONCLUSION\\n\\nIn this technical report, we have presented an approach to enhance the Chinese understanding and generation capabilities of the LLaMA model. Acknowledging the limitations of the original LLaMA’s Chinese vocabulary, we expanded it by incorporating 20K additional Chinese tokens, sig- nificantly increasing its encoding efficiency for the Chinese language. Building on the Chinese LLaMA, we employed supervised fine-tuning with instruction data, resulting in Chinese Alpaca models exhibiting improved instruction-following capabilities.\\n\\nTo evaluate our models effectively, we annotated 200 samples across ten distinct task types and utilized GPT-4 for evaluation. Our experiments demonstrated that the proposed models significantly outperformed the original LLaMA in Chinese understanding and generation tasks. We also tested our models on C-Eval datasets. The results show that the proposed model could achieve significant improvements and show competitive performance to the models with several times bigger sizes.\\n\\nLooking ahead, we plan to explore Reinforcement Learning from Human Feedback (RLHF) or Re- inforcement Learning from AI Instructed Feedback (RLAIF) to further align the models’ output with human preferences. Moreover, we intend to adopt more advanced and effective quantization methods, such as GPTQ (Frantar et al., 2022), among others. Additionally, we aim to investigate al- ternative methods to LoRA for more efficient and effective pre-training and fine-tuning of large lan-\\n\\n16\\n\\nTechnical Report\\n\\nguage models, ultimately enhancing their performance and applicability across various tasks within the Chinese NLP community.\\n\\nLIMITATIONS\\n\\nWhile this project has successfully enhanced the Chinese understanding and generation capabilities of the LLaMA and Alpaca models, several limitations must be acknowledged:\\n\\n¢ Harmful and unpredictable content: Though our model can reject unethical queries, these mod- els may still generate harmful or misaligned with human preferences and values. This issue may arise from biases in the training data or the models’ inability to discern appropriate outputs in certain contexts.\\n\\n¢ Insufficient training: Due to constraints in computing power and data availability, the training of the models may not be sufficient for optimal performance. As a result, there is still room for improvement in the Chinese understanding capabilities of the models.\\n\\n¢ Lack of robustness: The models may exhibit brittleness in some situations, producing incon- sistent or nonsensical outputs when faced with adversarial inputs or rare language phenomena.\\n\\n¢ Comprehensive evaluation: Evaluating large language models is an important topic in the cur- rent era. While we have seen many evaluation benchmarks for LLMs, their comprehensiveness and appropriateness for LLMs should be well-studied and examined. A more diverse and com- prehensive LLM evaluation dataset and benchmark will have a great positive effect on shaping the future of LLM research.\\n\\n¢ Scalability and efficiency: Although we applied LoRA and quantization to make the model more accessible to a broader community, when combined with the original LLaMA, the mod- els’ large size and complexity can lead to difficulties in deployment, especially for users with limited computational resources. This issue may hinder the accessibility and widespread adop- tion of the models in various applications.\\n\\nFuture work should address these limitations to further enhance the models’ capabilities, making them more robust, accessible, and effective for a broader range of applications in the Chinese NLP community.\\n\\nACKNOWLEDGMENTS\\n\\nThe original draft was polished by OpenAI GPT-4 for grammatical corrections and clarity improve- ments. We would like to thank our community members for their contributions to our open-source projects.\\n\\nREFERENCES\\n\\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.\\n\\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.\\n\\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. Revisiting pre- trained models for Chinese natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pp. 657-668, Online, Novem- ber 2020. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/2020.findings-emnlp.58.\\n\\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and Ziqing Yang. Pre-training with whole word\\n\\nmasking for chinese bert. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3504-3514, 2021. doi: 10.1109/TASLP.2021.3124365.\\n\\n17\\n\\nTechnical Report\\n\\nYiming Cui, Wanxiang Che, Shijin Wang, and Ting Liu. Lert: A linguistically-motivated pre-trained language model. arXiv preprint arXiv:2211.05344, 2022.\\n\\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized Ilms. arXiv preprint arXiv:2305.14314, 2023.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, Volume I (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. URL https: //www.aclweb.org/ anthology/N19-1423.\\n\\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training compression for generative pretrained transformers. arXiv preprint arXiv:2210.17323, 2022.\\n\\nGeorgi Gerganov. llama.cpp. https: //github.com/ggerganov/llama. cpp, 2023.\\n\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. arXiv e-prints, art. arXiv:2106.09685, June 2021. doi: 10.48550/arXiv.2106.09685.\\n\\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.\\n\\nAndreas Kopf, Yannic Kilcher, Dimitri von Riitte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richard Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. OpenAssistant Conversations — Democratizing Large Language Model Align- ment. arXiv e-prints, art. arXiv:2304.07327, April 2023. doi: 10.48550/arXiv.2304.07327.\\n\\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent sub- word tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66-71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL https: //aclanthology.org/D18-2012.\\n\\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 785-794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17- 1082. URL https://aclanthology.org/D17-1082.\\n\\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese, 2023.\\n\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer- ence on Learning Representations, 2019. URL https: //openreview.net/forum?id= Bkg6RiCqyY7.\\n\\nOpenAl. Introducing chatgpt. https: //openai.com/blog/chatgpt, 2022.\\n\\nOpenAI. GPT-4 Technical Report. arXiv e-prints, art. arXiv:2303.08774, March 2023. doi: 10. 48550/arXiv.2303.08774.\\n\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel- ton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. arXiv e-prints, art. arXiv:2203.02155, March 2022. doi: 10.48550/arXiv.2203.02155.\\n\\n18\\n\\nTechnical Report\\n\\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023.\\n\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under- standing by generative pre-training. 2018.\\n\\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System opti- mizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 3505-3506, 2020.\\n\\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili¢, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, Frangois Yvon, Matthias Gallé, et al. Bloom: A 176b- parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\\n\\nNoam Shazeer. Glu variants improve transformer, 2020.\\n\\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021.\\n\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023a.\\n\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023b.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Roziére, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar- mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\\n\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning Language Model with Self Generated Instructions. arXiv e-prints, art. arXiv:2212.10560, December 2022. doi: 10.48550/arXiv.2212.10560.\\n\\nBright Xu. Nlp chinese corpus: Large scale chinese corpus for nlp, September 2019. URL https: //doi.org/10.5281/zenodo. 3402023.\\n\\nZiging Yang, Zihang Xu, Yiming Cui, Baoxin Wang, Min Lin, Dayong Wu, and Zhigang Chen. CINO: A Chinese minority pre-trained language model. In Proceedings of the 29th Interna- tional Conference on Computational Linguistics, pp. 3937-3949, Gyeongju, Republic of Ko- rea, October 2022. International Committee on Computational Linguistics. URL https: //aclanthology.org/2022.coling-1.346.\\n\\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130b: An open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=—Aw0rrrPUF.\\n\\nBiao Zhang and Rico Sennrich. Root Mean Square Layer Normalization. In Advances in Neural Information Processing Systems 32, Vancouver, Canada, 2019. URL https: //openreview. net/references/pdf?id=S1qBAf6rr.\\n\\n19\\n\\nTechnical Report\\n\\nA APPENDIX\\n\\nWe present the baseline results on Chinese-LLaMA-2 and Chinese-Alpaca-2 as follows. Most of the settings are identical to those in Chinese-LLaMA.\\n\\nA.1 C-EVAL\\n\\nThe results on C-Eval (Huang et al., 2023) are presented in Table 10.\\n\\nTable 10: Results on C-Eval valid and test sets.\\n\\nModel Valid Set Test Set Zero-shot 5-shot Zero-shot 5-shot Chinese-LLaMA-2-7B 28.2 36.0 30.3 34.2 Chinese-LLaMA-2-13B 40.6 42.7 38.0 41.6 Chinese-Alpaca-2-7B 41.3 42.9 40.3 39.5 Chinese-Alpaca-2-13B 44.3 45.9 42.6 44.0\\n\\nA.2.> CMMLU\\n\\nThe results on CMMLU (Li et al., 2023) are presented in Table 11.\\n\\nTable 11: Results on CMMLU test sets.\\n\\nTest Set Model Zero-shot Few-shot Chinese-LLaMA-2-7B 27.9 34.1 Chinese-LLaMA-2-13B 38.9 42.5 Chinese-Alpaca-2-7B 40.0 41.8 Chinese-Alpaca-2-13B 43.2 45.5\\n\\nA.3. LONGBENCH\\n\\nThe results on LongBench (Bai et al., 2023) are presented in Table 12. This benchmark is specifi- cally designed to test the long context ability of LLMs. We test the Chinese subsets of LongBench (including code tasks). The models marked with 16K were finetuned using Positional Interpolation (PI) method (Chen et al., 2023), which supports 16K context. The models marked with 64K were finetuned using YaRN method (Peng et al., 2023), which supports 64K context.\\n\\n20\\n\\nTechnical Report\\n\\nTable 12: Results on LongBench (Chinese + code tasks). S-QA: Single-doc QA, M-QA: Multi- doc QA, Summ: Summarization, FS-Learn: Few-shot Learning, Code: Code Completion, Synthetic: Synthetic Tasks.\\n\\nModel S-QA M-QA Summ _ FS-Learn Code Synthetic Average Chinese-LLaMA-2-7B 19.0 13.9 6.4 11.0 11.0 47 11.0 Chinese-LLaMA-2-7B-16K 33.2 15.9 6.5 23.5 10.3 5.3 15.8 Chinese-LLaMA-2-7B-64K 27.2 16.4 6.5 33.0 78 5.0 16.0 Chinese-LLaMA-2-13B 28.3 14.4 4.6 16.3 10.4 5.4 13.2 Chinese-LLaMA-2-13B-16K 36.7 17.7 3.1 29.8 13.8 3.0 17.3 Chinese-Alpaca-2-7B 34.0 17.4 11.8 21.3 50.3 4.5 23.2 Chinese-Alpaca-2-7B-16K 46.4 23.3 14.3 29.0 49.6 9.0 28.6 Chinese-Alpaca-2-7B-64K 44.7 28.1 14.4 39.0 44.6 5.0 29.3 Chinese-Alpaca-2-13B 38.4 20.0 11.9 17.3 46.5 8.0 23.7 Chinese-Alpaca-2-13B-16K 47.9 26.7 13.0 22.3 46.6 21.5 29.7\\n\\n21\\n')\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "\n",
    "# # Read the markdown file extracted by Unstructured\n",
    "# with open('/Users/ogb/Desktop/CLlama2-full.md', 'r') as file:\n",
    "#     data = file.read()\n",
    "\n",
    "# # Define the regex pattern\n",
    "# pattern = r\"(##\\s\\d\\s\\w+)(.*?)(?=##\\s\\d\\s\\w+|$)\"\n",
    "\n",
    "# # Use re.DOTALL to make '.' match any character including a newline\n",
    "# matches = re.findall(pattern, data, re.DOTALL)\n",
    "\n",
    "# # Print the matches\n",
    "# for match in matches:\n",
    "#     print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 INTRODUCTION\n",
      "2 CHINESE LLAMA AND CHINESE ALPACA\n",
      "3 EXPERIMENTAL SETUPS\n",
      "4 RESULTS ON INSTRUCTION-FOLLOWING TASKS\n",
      "5 RESULTS ON NATURAL LANGUAGE UNDERSTANDING TASKS\n",
      "6 EFFECT OF DIFFERENT QUANTIZATION METHODS\n",
      "7 CONCLUSION\n"
     ]
    }
   ],
   "source": [
    "# find all section titles to parse content\n",
    "import re\n",
    "\n",
    "def find_section_titles(text):\n",
    "    pattern = r'\\b\\d+\\s[A-Z][A-Z\\s]+\\b\\n\\n'\n",
    "    titles = re.findall(pattern, text)\n",
    "    # Remove trailing newline characters from each title\n",
    "    titles = [title.rstrip('\\n') for title in titles]\n",
    "    return titles\n",
    "\n",
    "with open('/Users/ogb/Desktop/CLlama2-full.md', 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Regular expression pattern for section titles\n",
    "pattern = r\"\\n\\d+ [A-Z\\s-]+\\n\"\n",
    "# TODO: Write a regular expression pattern to match Abstract\n",
    "\n",
    "# Find all matches\n",
    "matches = re.findall(pattern, data)\n",
    "\n",
    "# Print all section titles\n",
    "for match in matches:\n",
    "    print(match.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1 INTRODUCTION', '2 CHINESE LLAMA AND CHINESE ALPACA', '3 EXPERIMENTAL SETUPS', '4 RESULTS ON INSTRUCTION-FOLLOWING TASKS', '5 RESULTS ON NATURAL LANGUAGE UNDERSTANDING TASKS', '6 EFFECT OF DIFFERENT QUANTIZATION METHODS', '7 CONCLUSION'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open('/Users/ogb/Desktop/CLlama2-full.md', 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "pattern = r\"\\n\\d+ [A-Z\\s-]+\\n\"\n",
    "\n",
    "# Split the text into sections\n",
    "sections = re.split(pattern, data)\n",
    "\n",
    "# number of sections should match with number of section titles (aka matches of regex pattern)\n",
    "# print(sections[2])\n",
    "# print(sections[2])\n",
    "# print(len(sections))\n",
    "\n",
    "# Create a dictionary to store section titles and contents\n",
    "section_contents = {}\n",
    "\n",
    "# Iterate over the sections\n",
    "for match in matches:\n",
    "    section_title = match.strip()\n",
    "    for i in range(len(sections)):\n",
    "        # Extract the section title and content\n",
    "        content = sections[i].strip()\n",
    "\n",
    "        # Store the section content\n",
    "        section_contents[section_title] = content\n",
    "\n",
    "section_contents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this technical report, we have presented an approach to enhance the Chinese understanding and generation capabilities of the LLaMA model. Acknowledging the limitations of the original LLaMA’s Chinese vocabulary, we expanded it by incorporating 20K additional Chinese tokens, sig- nificantly increasing its encoding efficiency for the Chinese language. Building on the Chinese LLaMA, we employed supervised fine-tuning with instruction data, resulting in Chinese Alpaca models exhibiting improved instruction-following capabilities.\\n\\nTo evaluate our models effectively, we annotated 200 samples across ten distinct task types and utilized GPT-4 for evaluation. Our experiments demonstrated that the proposed models significantly outperformed the original LLaMA in Chinese understanding and generation tasks. We also tested our models on C-Eval datasets. The results show that the proposed model could achieve significant improvements and show competitive performance to the models with several times bigger sizes.\\n\\nLooking ahead, we plan to explore Reinforcement Learning from Human Feedback (RLHF) or Re- inforcement Learning from AI Instructed Feedback (RLAIF) to further align the models’ output with human preferences. Moreover, we intend to adopt more advanced and effective quantization methods, such as GPTQ (Frantar et al., 2022), among others. Additionally, we aim to investigate al- ternative methods to LoRA for more efficient and effective pre-training and fine-tuning of large lan-\\n\\n16\\n\\nTechnical Report\\n\\nguage models, ultimately enhancing their performance and applicability across various tasks within the Chinese NLP community.\\n\\nLIMITATIONS\\n\\nWhile this project has successfully enhanced the Chinese understanding and generation capabilities of the LLaMA and Alpaca models, several limitations must be acknowledged:\\n\\n¢ Harmful and unpredictable content: Though our model can reject unethical queries, these mod- els may still generate harmful or misaligned with human preferences and values. This issue may arise from biases in the training data or the models’ inability to discern appropriate outputs in certain contexts.\\n\\n¢ Insufficient training: Due to constraints in computing power and data availability, the training of the models may not be sufficient for optimal performance. As a result, there is still room for improvement in the Chinese understanding capabilities of the models.\\n\\n¢ Lack of robustness: The models may exhibit brittleness in some situations, producing incon- sistent or nonsensical outputs when faced with adversarial inputs or rare language phenomena.\\n\\n¢ Comprehensive evaluation: Evaluating large language models is an important topic in the cur- rent era. While we have seen many evaluation benchmarks for LLMs, their comprehensiveness and appropriateness for LLMs should be well-studied and examined. A more diverse and com- prehensive LLM evaluation dataset and benchmark will have a great positive effect on shaping the future of LLM research.\\n\\n¢ Scalability and efficiency: Although we applied LoRA and quantization to make the model more accessible to a broader community, when combined with the original LLaMA, the mod- els’ large size and complexity can lead to difficulties in deployment, especially for users with limited computational resources. This issue may hinder the accessibility and widespread adop- tion of the models in various applications.\\n\\nFuture work should address these limitations to further enhance the models’ capabilities, making them more robust, accessible, and effective for a broader range of applications in the Chinese NLP community.\\n\\nACKNOWLEDGMENTS\\n\\nThe original draft was polished by OpenAI GPT-4 for grammatical corrections and clarity improve- ments. We would like to thank our community members for their contributions to our open-source projects.\\n\\nREFERENCES\\n\\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.\\n\\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.\\n\\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. Revisiting pre- trained models for Chinese natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pp. 657-668, Online, Novem- ber 2020. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/2020.findings-emnlp.58.\\n\\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and Ziqing Yang. Pre-training with whole word\\n\\nmasking for chinese bert. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3504-3514, 2021. doi: 10.1109/TASLP.2021.3124365.\\n\\n17\\n\\nTechnical Report\\n\\nYiming Cui, Wanxiang Che, Shijin Wang, and Ting Liu. Lert: A linguistically-motivated pre-trained language model. arXiv preprint arXiv:2211.05344, 2022.\\n\\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized Ilms. arXiv preprint arXiv:2305.14314, 2023.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, Volume I (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. URL https: //www.aclweb.org/ anthology/N19-1423.\\n\\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training compression for generative pretrained transformers. arXiv preprint arXiv:2210.17323, 2022.\\n\\nGeorgi Gerganov. llama.cpp. https: //github.com/ggerganov/llama. cpp, 2023.\\n\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. arXiv e-prints, art. arXiv:2106.09685, June 2021. doi: 10.48550/arXiv.2106.09685.\\n\\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.\\n\\nAndreas Kopf, Yannic Kilcher, Dimitri von Riitte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richard Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. OpenAssistant Conversations — Democratizing Large Language Model Align- ment. arXiv e-prints, art. arXiv:2304.07327, April 2023. doi: 10.48550/arXiv.2304.07327.\\n\\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent sub- word tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66-71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL https: //aclanthology.org/D18-2012.\\n\\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 785-794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17- 1082. URL https://aclanthology.org/D17-1082.\\n\\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese, 2023.\\n\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer- ence on Learning Representations, 2019. URL https: //openreview.net/forum?id= Bkg6RiCqyY7.\\n\\nOpenAl. Introducing chatgpt. https: //openai.com/blog/chatgpt, 2022.\\n\\nOpenAI. GPT-4 Technical Report. arXiv e-prints, art. arXiv:2303.08774, March 2023. doi: 10. 48550/arXiv.2303.08774.\\n\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel- ton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. arXiv e-prints, art. arXiv:2203.02155, March 2022. doi: 10.48550/arXiv.2203.02155.\\n\\n18\\n\\nTechnical Report\\n\\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023.\\n\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under- standing by generative pre-training. 2018.\\n\\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System opti- mizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 3505-3506, 2020.\\n\\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili¢, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, Frangois Yvon, Matthias Gallé, et al. Bloom: A 176b- parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\\n\\nNoam Shazeer. Glu variants improve transformer, 2020.\\n\\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021.\\n\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023a.\\n\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023b.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Roziére, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar- mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\\n\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning Language Model with Self Generated Instructions. arXiv e-prints, art. arXiv:2212.10560, December 2022. doi: 10.48550/arXiv.2212.10560.\\n\\nBright Xu. Nlp chinese corpus: Large scale chinese corpus for nlp, September 2019. URL https: //doi.org/10.5281/zenodo. 3402023.\\n\\nZiging Yang, Zihang Xu, Yiming Cui, Baoxin Wang, Min Lin, Dayong Wu, and Zhigang Chen. CINO: A Chinese minority pre-trained language model. In Proceedings of the 29th Interna- tional Conference on Computational Linguistics, pp. 3937-3949, Gyeongju, Republic of Ko- rea, October 2022. International Committee on Computational Linguistics. URL https: //aclanthology.org/2022.coling-1.346.\\n\\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130b: An open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=—Aw0rrrPUF.\\n\\nBiao Zhang and Rico Sennrich. Root Mean Square Layer Normalization. In Advances in Neural Information Processing Systems 32, Vancouver, Canada, 2019. URL https: //openreview. net/references/pdf?id=S1qBAf6rr.\\n\\n19\\n\\nTechnical Report\\n\\nA APPENDIX\\n\\nWe present the baseline results on Chinese-LLaMA-2 and Chinese-Alpaca-2 as follows. Most of the settings are identical to those in Chinese-LLaMA.\\n\\nA.1 C-EVAL\\n\\nThe results on C-Eval (Huang et al., 2023) are presented in Table 10.\\n\\nTable 10: Results on C-Eval valid and test sets.\\n\\nModel Valid Set Test Set Zero-shot 5-shot Zero-shot 5-shot Chinese-LLaMA-2-7B 28.2 36.0 30.3 34.2 Chinese-LLaMA-2-13B 40.6 42.7 38.0 41.6 Chinese-Alpaca-2-7B 41.3 42.9 40.3 39.5 Chinese-Alpaca-2-13B 44.3 45.9 42.6 44.0\\n\\nA.2.> CMMLU\\n\\nThe results on CMMLU (Li et al., 2023) are presented in Table 11.\\n\\nTable 11: Results on CMMLU test sets.\\n\\nTest Set Model Zero-shot Few-shot Chinese-LLaMA-2-7B 27.9 34.1 Chinese-LLaMA-2-13B 38.9 42.5 Chinese-Alpaca-2-7B 40.0 41.8 Chinese-Alpaca-2-13B 43.2 45.5\\n\\nA.3. LONGBENCH\\n\\nThe results on LongBench (Bai et al., 2023) are presented in Table 12. This benchmark is specifi- cally designed to test the long context ability of LLMs. We test the Chinese subsets of LongBench (including code tasks). The models marked with 16K were finetuned using Positional Interpolation (PI) method (Chen et al., 2023), which supports 16K context. The models marked with 64K were finetuned using YaRN method (Peng et al., 2023), which supports 64K context.\\n\\n20\\n\\nTechnical Report\\n\\nTable 12: Results on LongBench (Chinese + code tasks). S-QA: Single-doc QA, M-QA: Multi- doc QA, Summ: Summarization, FS-Learn: Few-shot Learning, Code: Code Completion, Synthetic: Synthetic Tasks.\\n\\nModel S-QA M-QA Summ _ FS-Learn Code Synthetic Average Chinese-LLaMA-2-7B 19.0 13.9 6.4 11.0 11.0 47 11.0 Chinese-LLaMA-2-7B-16K 33.2 15.9 6.5 23.5 10.3 5.3 15.8 Chinese-LLaMA-2-7B-64K 27.2 16.4 6.5 33.0 78 5.0 16.0 Chinese-LLaMA-2-13B 28.3 14.4 4.6 16.3 10.4 5.4 13.2 Chinese-LLaMA-2-13B-16K 36.7 17.7 3.1 29.8 13.8 3.0 17.3 Chinese-Alpaca-2-7B 34.0 17.4 11.8 21.3 50.3 4.5 23.2 Chinese-Alpaca-2-7B-16K 46.4 23.3 14.3 29.0 49.6 9.0 28.6 Chinese-Alpaca-2-7B-64K 44.7 28.1 14.4 39.0 44.6 5.0 29.3 Chinese-Alpaca-2-13B 38.4 20.0 11.9 17.3 46.5 8.0 23.7 Chinese-Alpaca-2-13B-16K 47.9 26.7 13.0 22.3 46.6 21.5 29.7\\n\\n21'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_contents['7 CONCLUSION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "from llama_index.core import Settings\n",
    "\n",
    "tokenizer = Anthropic().tokenizer\n",
    "Settings.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = input(\"Enter your API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "# llm = Anthropic(model=\"claude-3-opus-20240229\", max_tokens=100)\n",
    "# resp = llm.stream_complete(\"Paul Graham is \")\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "llm = Anthropic(model=\"claude-3-opus-20240229\")\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Tell me a story\"),\n",
    "]\n",
    "resp = llm.stream_chat(messages)\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "selected_section = None\n",
    "# find a section in the document by title\n",
    "for section in doc.sections():\n",
    "    if section.title == 'Abstract':\n",
    "        selected_section = section\n",
    "        break\n",
    "# use include_children=True and recurse=True to fully expand the section.\n",
    "# include_children only returns at one sublevel of children whereas recurse goes through all the descendants\n",
    "HTML(section.to_html(include_children=True, recurse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "llm = Anthropic(model=\"claude-3-opus-20240229\")\n",
    "\n",
    "context = selected_section.to_html(include_children=True, recurse=True)\n",
    "question = \"summarise what matters in the abstract\"\n",
    "resp = llm.stream_complete(f\"read this text and answer question: {question}:\\n{context}\")\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "HTML(doc.tables()[2].to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = doc.tables()[2].to_html()\n",
    "resp = llm.stream_complete(f\"read this table and answer question: which target language has English as a Source Language?:\\n{context}\")\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.readers.schema.base import Document\n",
    "# from llama_index import VectorStoreIndex\n",
    "\n",
    "# index = VectorStoreIndex([])\n",
    "for chunk in doc.chunks():\n",
    "    # index.insert(Document(text=chunk.to_context_text(), extra_info={}))\n",
    "    print(\"hi\")\n",
    "    print(chunk.to_context_text()[:100])\n",
    "    exit()\n",
    "# query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
